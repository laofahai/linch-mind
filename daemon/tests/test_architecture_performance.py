#!/usr/bin/env python3
"""
æ¶æ„æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
éªŒè¯é‡æ„åçš„æ¶æ„æ»¡è¶³æ€§èƒ½è¦æ±‚
"""

import asyncio
import time
import statistics
from pathlib import Path
import tempfile
import sys
import os

# æ·»åŠ daemonåˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core import (
    StructuredExceptionHandler,
    ServiceContainer,
    DatabaseManager,
    handle_exceptions,
    safe_execute
)
from services.ipc import UnifiedIPCServer


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.results = {}
        
    def measure(self, name: str, func, iterations: int = 1000):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            func()
            end = time.perf_counter()
            times.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            
        self.results[name] = {
            'mean': statistics.mean(times),
            'median': statistics.median(times),
            'stdev': statistics.stdev(times) if len(times) > 1 else 0,
            'min': min(times),
            'max': max(times),
            'p95': sorted(times)[int(len(times) * 0.95)],
            'p99': sorted(times)[int(len(times) * 0.99)]
        }
        
    async def measure_async(self, name: str, coro_func, iterations: int = 1000):
        """æµ‹é‡å¼‚æ­¥å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            await coro_func()
            end = time.perf_counter()
            times.append((end - start) * 1000)
            
        self.results[name] = {
            'mean': statistics.mean(times),
            'median': statistics.median(times),
            'stdev': statistics.stdev(times) if len(times) > 1 else 0,
            'min': min(times),
            'max': max(times),
            'p95': sorted(times)[int(len(times) * 0.95)],
            'p99': sorted(times)[int(len(times) * 0.99)]
        }
        
    def print_results(self):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ")
        print("="*80)
        
        for name, metrics in self.results.items():
            print(f"\nğŸ“Š {name}")
            print(f"  å¹³å‡å€¼: {metrics['mean']:.3f}ms")
            print(f"  ä¸­ä½æ•°: {metrics['median']:.3f}ms")
            print(f"  æ ‡å‡†å·®: {metrics['stdev']:.3f}ms")
            print(f"  æœ€å°å€¼: {metrics['min']:.3f}ms")
            print(f"  æœ€å¤§å€¼: {metrics['max']:.3f}ms")
            print(f"  P95: {metrics['p95']:.3f}ms")
            print(f"  P99: {metrics['p99']:.3f}ms")
            
            # æ€§èƒ½åˆ¤å®š
            if 'IPC' in name:
                target = 5.0
                status = "âœ… è¾¾æ ‡" if metrics['p95'] < target else "âŒ æœªè¾¾æ ‡"
                print(f"  ç›®æ ‡: <{target}ms (P95), çŠ¶æ€: {status}")
            elif 'å¼‚å¸¸å¤„ç†' in name:
                target = 0.1
                status = "âœ… è¾¾æ ‡" if metrics['mean'] < target else "âŒ æœªè¾¾æ ‡"
                print(f"  ç›®æ ‡: <{target}ms (å¹³å‡), çŠ¶æ€: {status}")
            elif 'ä¾èµ–æ³¨å…¥' in name:
                target = 1.0
                status = "âœ… è¾¾æ ‡" if metrics['mean'] < target else "âŒ æœªè¾¾æ ‡"
                print(f"  ç›®æ ‡: <{target}ms (å¹³å‡), çŠ¶æ€: {status}")


def test_exception_handling_performance():
    """æµ‹è¯•å¼‚å¸¸å¤„ç†æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    handler = StructuredExceptionHandler()
    
    # æµ‹è¯•å¼‚å¸¸å¤„ç†è£…é¥°å™¨å¼€é”€
    @handle_exceptions("test_operation")
    def normal_function():
        return 1 + 1
    
    benchmark.measure("å¼‚å¸¸å¤„ç†è£…é¥°å™¨å¼€é”€", normal_function)
    
    # æµ‹è¯•å¼‚å¸¸æ•è·å’Œè®°å½•
    @handle_exceptions("test_error")
    def error_function():
        raise ValueError("Test error")
    
    def error_test():
        try:
            error_function()
        except:
            pass
    
    benchmark.measure("å¼‚å¸¸æ•è·å’Œè®°å½•", error_test)
    
    # æµ‹è¯•safe_execute
    def safe_test():
        result = safe_execute(lambda: 1 + 1)
        return result
    
    benchmark.measure("safe_executeå¼€é”€", safe_test)
    
    return benchmark


def test_dependency_injection_performance():
    """æµ‹è¯•ä¾èµ–æ³¨å…¥æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    container = ServiceContainer()
    
    # æ³¨å†Œæµ‹è¯•æœåŠ¡
    class TestService:
        def __init__(self):
            self.value = 42
    
    class DependentService:
        def __init__(self, test_service: TestService):
            self.test_service = test_service
    
    container.register_singleton(TestService)
    container.register_transient(DependentService)
    
    # æµ‹è¯•å•ä¾‹è§£æ
    def resolve_singleton():
        container.get_service(TestService)
    
    benchmark.measure("ä¾èµ–æ³¨å…¥-å•ä¾‹è§£æ", resolve_singleton)
    
    # æµ‹è¯•ç¬æ€è§£æ
    def resolve_transient():
        container.get_service(DependentService)
    
    benchmark.measure("ä¾èµ–æ³¨å…¥-ç¬æ€è§£æ(å«ä¾èµ–)", resolve_transient)
    
    return benchmark


async def test_ipc_performance():
    """æµ‹è¯•IPCé€šä¿¡æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    
    # åˆ›å»ºä¸´æ—¶socketè·¯å¾„
    with tempfile.TemporaryDirectory() as tmpdir:
        socket_path = Path(tmpdir) / "test.sock"
        
        # åˆ›å»ºæœåŠ¡å™¨
        server = UnifiedIPCServer(socket_path=str(socket_path))
        
        # å¯åŠ¨æœåŠ¡å™¨ä»»åŠ¡
        server_task = asyncio.create_task(server.start())
        await asyncio.sleep(0.1)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        
        # æµ‹è¯•è¿æ¥å»ºç«‹
        async def connect_test():
            reader, writer = await asyncio.open_unix_connection(str(socket_path))
            writer.close()
            await writer.wait_closed()
        
        await benchmark.measure_async("IPCè¿æ¥å»ºç«‹", connect_test, iterations=100)
        
        # æµ‹è¯•è¯·æ±‚-å“åº”å¾€è¿”
        async def request_response_test():
            reader, writer = await asyncio.open_unix_connection(str(socket_path))
            
            # å‘é€ç®€å•è¯·æ±‚
            request = b'{"method": "ping", "id": "1", "params": {}}\n'
            writer.write(request)
            await writer.drain()
            
            # è¯»å–å“åº”
            response = await reader.readline()
            
            writer.close()
            await writer.wait_closed()
        
        await benchmark.measure_async("IPCè¯·æ±‚-å“åº”å¾€è¿”", request_response_test, iterations=100)
        
        # åœæ­¢æœåŠ¡å™¨
        await server.stop()
        server_task.cancel()
        
    return benchmark


async def test_database_performance():
    """æµ‹è¯•æ•°æ®åº“ç®¡ç†å™¨æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    
    # ä½¿ç”¨å†…å­˜æ•°æ®åº“è¿›è¡Œæµ‹è¯•
    from core.database_manager import DatabaseConfig
    
    config = DatabaseConfig(
        url="sqlite:///:memory:",
        pool_size=10,
        max_overflow=20
    )
    
    manager = DatabaseManager(config)
    
    # æµ‹è¯•ä¼šè¯è·å–
    def get_session_test():
        with manager.get_session() as session:
            pass
    
    benchmark.measure("æ•°æ®åº“ä¼šè¯è·å–", get_session_test)
    
    # æµ‹è¯•å¼‚æ­¥ä¼šè¯è·å–
    async def get_async_session_test():
        async with manager.get_async_session() as session:
            pass
    
    await benchmark.measure_async("æ•°æ®åº“å¼‚æ­¥ä¼šè¯è·å–", get_async_session_test)
    
    return benchmark


async def main():
    """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ¶æ„æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # æµ‹è¯•å¼‚å¸¸å¤„ç†
    exception_benchmark = test_exception_handling_performance()
    exception_benchmark.print_results()
    
    # æµ‹è¯•ä¾èµ–æ³¨å…¥
    di_benchmark = test_dependency_injection_performance()
    di_benchmark.print_results()
    
    # æµ‹è¯•IPCé€šä¿¡
    try:
        ipc_benchmark = await test_ipc_performance()
        ipc_benchmark.print_results()
    except Exception as e:
        print(f"âš ï¸ IPCæµ‹è¯•è·³è¿‡ (éœ€è¦Unixç¯å¢ƒ): {e}")
    
    # æµ‹è¯•æ•°æ®åº“ç®¡ç†
    try:
        db_benchmark = await test_database_performance()
        db_benchmark.print_results()
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "="*80)
    print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print("="*80)
    
    # æ€»ç»“
    print("\nğŸ“‹ æ€§èƒ½æ€»ç»“:")
    all_benchmarks = [exception_benchmark, di_benchmark]
    
    passed = 0
    failed = 0
    
    for benchmark in all_benchmarks:
        for name, metrics in benchmark.results.items():
            if 'IPC' in name:
                if metrics['p95'] < 5.0:
                    passed += 1
                else:
                    failed += 1
            elif 'å¼‚å¸¸å¤„ç†' in name:
                if metrics['mean'] < 0.1:
                    passed += 1
                else:
                    failed += 1
            elif 'ä¾èµ–æ³¨å…¥' in name:
                if metrics['mean'] < 1.0:
                    passed += 1
                else:
                    failed += 1
    
    print(f"  é€šè¿‡: {passed}")
    print(f"  å¤±è´¥: {failed}")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ï¼")
    else:
        print(f"\nâš ï¸ æœ‰{failed}é¡¹æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œéœ€è¦ä¼˜åŒ–")


if __name__ == "__main__":
    asyncio.run(main())