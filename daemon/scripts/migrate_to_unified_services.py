#!/usr/bin/env python3
"""
ç»Ÿä¸€æœåŠ¡è¿ç§»è„šæœ¬
è‡ªåŠ¨è¯†åˆ«å’Œè¿ç§»é‡å¤çš„æœåŠ¡å®ç°åˆ°ç»Ÿä¸€æœåŠ¡æ¶æ„
"""

import ast
import os
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DuplicateCodeAnalyzer:
    """é‡å¤ä»£ç åˆ†æå™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.duplicate_patterns = {
            'vector_search': [
                'VectorService',
                'vector_search',
                'faiss',
                'search_by_vector',
                'similarity_search'
            ],
            'graph_search': [
                'GraphService', 
                'networkx',
                'find_neighbors',
                'graph_traversal',
                'shortest_path'
            ],
            'cache_systems': [
                'cache',
                'Cache',
                'memory_cache',
                'disk_cache',
                'lru_cache'
            ],
            'thread_pools': [
                'ThreadPoolExecutor',
                'concurrent.futures',
                'executor',
                'submit',
                'thread_pool'
            ]
        }
        
        self.unified_services = {
            'vector_search': 'UnifiedSearchService',
            'graph_search': 'UnifiedSearchService', 
            'cache_systems': 'UnifiedCacheService',
            'thread_pools': 'SharedExecutorService'
        }
    
    def scan_project(self) -> Dict[str, List[str]]:
        """æ‰«æé¡¹ç›®ä¸­çš„é‡å¤å®ç°"""
        results = {category: [] for category in self.duplicate_patterns}
        
        for py_file in self.project_root.rglob("*.py"):
            if any(skip in str(py_file) for skip in ['__pycache__', '.git', 'unified_', 'shared_']):
                continue
                
            try:
                content = py_file.read_text(encoding='utf-8')
                
                for category, patterns in self.duplicate_patterns.items():
                    for pattern in patterns:
                        if pattern in content:
                            results[category].append(str(py_file))
                            break
                            
            except Exception as e:
                logger.warning(f"æ— æ³•åˆ†ææ–‡ä»¶ {py_file}: {e}")
        
        return results
    
    def generate_migration_plan(self) -> Dict[str, List[Dict]]:
        """ç”Ÿæˆè¿ç§»è®¡åˆ’"""
        scan_results = self.scan_project()
        migration_plan = {}
        
        for category, files in scan_results.items():
            if not files:
                continue
                
            unified_service = self.unified_services[category]
            migration_plan[category] = {
                'unified_service': unified_service,
                'affected_files': files,
                'migration_steps': self._get_migration_steps(category),
                'risk_level': self._assess_risk_level(files)
            }
        
        return migration_plan
    
    def _get_migration_steps(self, category: str) -> List[str]:
        """è·å–è¿ç§»æ­¥éª¤"""
        steps = {
            'vector_search': [
                "æ›¿æ¢VectorServiceå¯¼å…¥ä¸ºUnifiedSearchService",
                "æ›´æ–°vector_searchè°ƒç”¨ä¸ºunified search API",
                "ä¿®æ”¹æœç´¢å‚æ•°ä¸ºSearchQueryæ ¼å¼",
                "æ›´æ–°ç»“æœå¤„ç†ä¸ºSearchResultæ ¼å¼"
            ],
            'graph_search': [
                "æ›¿æ¢GraphServiceå¯¼å…¥ä¸ºUnifiedSearchService", 
                "æ›´æ–°graph traversalè°ƒç”¨ä¸ºunified search API",
                "ä¿®æ”¹å›¾æœç´¢å‚æ•°ä¸ºSearchQueryæ ¼å¼",
                "æ›´æ–°ç»“æœå¤„ç†ä¸ºSearchResultæ ¼å¼"
            ],
            'cache_systems': [
                "æ›¿æ¢å„ç§cacheå¯¼å…¥ä¸ºUnifiedCacheService",
                "ç»Ÿä¸€ç¼“å­˜APIè°ƒç”¨æ ¼å¼",
                "è¿ç§»ç¼“å­˜é…ç½®åˆ°CacheType",
                "æ›´æ–°ç¼“å­˜é”®å€¼æ ¼å¼"
            ],
            'thread_pools': [
                "æ›¿æ¢ThreadPoolExecutorå¯¼å…¥ä¸ºSharedExecutorService",
                "æ›´æ–°submitè°ƒç”¨ä¸ºç»Ÿä¸€ä»»åŠ¡æäº¤API",
                "åˆ†ç±»ä»»åŠ¡ç±»å‹(IO/CPU/MLç­‰)",
                "è¿ç§»é”™è¯¯å¤„ç†åˆ°ç»Ÿä¸€æ ¼å¼"
            ]
        }
        return steps.get(category, [])
    
    def _assess_risk_level(self, files: List[str]) -> str:
        """è¯„ä¼°è¿ç§»é£é™©ç­‰çº§"""
        if len(files) >= 10:
            return "HIGH"
        elif len(files) >= 5:
            return "MEDIUM"
        else:
            return "LOW"


class AutoMigrator:
    """è‡ªåŠ¨è¿ç§»å™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.backup_dir = project_root / "backup_before_migration"
        
    def create_backup(self, files: List[str]):
        """åˆ›å»ºè¿ç§»å‰å¤‡ä»½"""
        self.backup_dir.mkdir(exist_ok=True)
        
        for file_path in files:
            src_file = Path(file_path)
            if src_file.exists():
                backup_file = self.backup_dir / src_file.name
                backup_file.write_text(src_file.read_text())
                logger.info(f"å¤‡ä»½æ–‡ä»¶: {src_file} -> {backup_file}")
    
    def migrate_vector_graph_services(self, files: List[str]) -> Dict[str, bool]:
        """è¿ç§»å‘é‡å’Œå›¾æœç´¢æœåŠ¡"""
        results = {}
        
        migration_patterns = [
            # VectorServiceæ›¿æ¢
            (
                r'from\s+services\.storage\.vector_service\s+import\s+VectorService',
                'from services.unified_search_service import UnifiedSearchService, SearchQuery, SearchType'
            ),
            (
                r'from\s+.*\s+import\s+.*VectorService.*',
                'from services.unified_search_service import UnifiedSearchService, SearchQuery, SearchType'
            ),
            # GraphServiceæ›¿æ¢
            (
                r'from\s+services\.storage\.graph_service\s+import\s+GraphService',
                'from services.unified_search_service import UnifiedSearchService, SearchQuery, SearchType'
            ),
            # æœåŠ¡è·å–æ–¹æ³•æ›¿æ¢
            (
                r'get_vector_service\(\)',
                'await get_unified_search_service()'
            ),
            (
                r'get_graph_service\(\)',
                'await get_unified_search_service()'
            ),
            # APIè°ƒç”¨æ›¿æ¢ç¤ºä¾‹ (éœ€è¦æ ¹æ®å…·ä½“ç”¨æ³•è°ƒæ•´)
            (
                r'vector_service\.search\(',
                'search_service.search(SearchQuery(query=..., search_type=SearchType.SEMANTIC, '
            ),
            (
                r'graph_service\.find_neighbors\(',
                'search_service.search(SearchQuery(query="", search_type=SearchType.GRAPH, start_entity_id=..., '
            )
        ]
        
        for file_path in files:
            try:
                file_obj = Path(file_path)
                if not file_obj.exists():
                    continue
                    
                content = file_obj.read_text()
                original_content = content
                
                # åº”ç”¨è¿ç§»æ¨¡å¼
                for pattern, replacement in migration_patterns:
                    content = re.sub(pattern, replacement, content)
                
                # åªæœ‰å†…å®¹å‘ç”Ÿå˜åŒ–æ—¶æ‰å†™å…¥
                if content != original_content:
                    file_obj.write_text(content)
                    logger.info(f"âœ… å·²è¿ç§»æœç´¢æœåŠ¡: {file_path}")
                    results[file_path] = True
                else:
                    results[file_path] = False
                    
            except Exception as e:
                logger.error(f"âŒ è¿ç§»å¤±è´¥ {file_path}: {e}")
                results[file_path] = False
        
        return results
    
    def migrate_cache_systems(self, files: List[str]) -> Dict[str, bool]:
        """è¿ç§»ç¼“å­˜ç³»ç»Ÿ"""
        results = {}
        
        cache_patterns = [
            # æ›¿æ¢å„ç§ç¼“å­˜å¯¼å…¥
            (
                r'from\s+functools\s+import\s+lru_cache',
                'from services.unified_cache_service import get_unified_cache_service, CacheType'
            ),
            (
                r'@lru_cache\(.*?\)',
                '# TODO: è¿ç§»åˆ°UnifiedCacheService - ä½¿ç”¨cache_service.get_or_set()'
            ),
            # å†…å­˜ç¼“å­˜æ›¿æ¢
            (
                r'self\._cache\s*=\s*\{\}',
                '# ä½¿ç”¨UnifiedCacheServiceæ›¿ä»£å†…å­˜ç¼“å­˜'
            ),
            (
                r'cache\.get\(',
                'await cache_service.get('
            ),
            (
                r'cache\.set\(',
                'await cache_service.set('
            )
        ]
        
        for file_path in files:
            try:
                file_obj = Path(file_path)
                if not file_obj.exists():
                    continue
                
                content = file_obj.read_text()
                original_content = content
                
                for pattern, replacement in cache_patterns:
                    content = re.sub(pattern, replacement, content)
                
                if content != original_content:
                    file_obj.write_text(content)
                    logger.info(f"âœ… å·²è¿ç§»ç¼“å­˜ç³»ç»Ÿ: {file_path}")
                    results[file_path] = True
                else:
                    results[file_path] = False
                    
            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜è¿ç§»å¤±è´¥ {file_path}: {e}")
                results[file_path] = False
        
        return results
    
    def migrate_thread_pools(self, files: List[str]) -> Dict[str, bool]:
        """è¿ç§»çº¿ç¨‹æ± """
        results = {}
        
        executor_patterns = [
            # ThreadPoolExecutoræ›¿æ¢
            (
                r'from\s+concurrent\.futures\s+import\s+ThreadPoolExecutor',
                'from services.shared_executor_service import get_shared_executor_service, TaskType'
            ),
            (
                r'ThreadPoolExecutor\(max_workers=(\d+)\)',
                '# ä½¿ç”¨SharedExecutorServiceæ›¿ä»£ThreadPoolExecutor'
            ),
            (
                r'self\._executor\s*=\s*ThreadPoolExecutor\(.*?\)',
                'self._executor = get_shared_executor_service()'
            ),
            (
                r'executor\.submit\(',
                'await executor.submit('
            )
        ]
        
        for file_path in files:
            try:
                file_obj = Path(file_path)
                if not file_obj.exists():
                    continue
                
                content = file_obj.read_text()
                original_content = content
                
                for pattern, replacement in executor_patterns:
                    content = re.sub(pattern, replacement, content)
                
                if content != original_content:
                    file_obj.write_text(content)
                    logger.info(f"âœ… å·²è¿ç§»çº¿ç¨‹æ± : {file_path}")
                    results[file_path] = True
                else:
                    results[file_path] = False
                    
            except Exception as e:
                logger.error(f"âŒ çº¿ç¨‹æ± è¿ç§»å¤±è´¥ {file_path}: {e}")
                results[file_path] = False
        
        return results


def main():
    """ä¸»è¿ç§»æµç¨‹"""
    project_root = Path(__file__).parent.parent
    
    logger.info("ğŸš€ å¼€å§‹ç»Ÿä¸€æœåŠ¡è¿ç§»åˆ†æ")
    
    # åˆ†æé‡å¤ä»£ç 
    analyzer = DuplicateCodeAnalyzer(project_root)
    migration_plan = analyzer.generate_migration_plan()
    
    logger.info("ğŸ“Š è¿ç§»è®¡åˆ’åˆ†æç»“æœ:")
    for category, plan in migration_plan.items():
        logger.info(f"  {category}:")
        logger.info(f"    ç»Ÿä¸€æœåŠ¡: {plan['unified_service']}")
        logger.info(f"    å½±å“æ–‡ä»¶: {len(plan['affected_files'])} ä¸ª")
        logger.info(f"    é£é™©ç­‰çº§: {plan['risk_level']}")
        for step in plan['migration_steps']:
            logger.info(f"      - {step}")
    
    # æ‰§è¡Œè¿ç§»
    migrator = AutoMigrator(project_root)
    
    # åˆ›å»ºå¤‡ä»½
    all_files = []
    for plan in migration_plan.values():
        all_files.extend(plan['affected_files'])
    
    if all_files:
        migrator.create_backup(all_files)
        logger.info(f"âœ… å·²å¤‡ä»½ {len(all_files)} ä¸ªæ–‡ä»¶")
    
    # æ‰§è¡Œåˆ†ç±»è¿ç§»
    total_migrated = 0
    
    if 'vector_search' in migration_plan or 'graph_search' in migration_plan:
        search_files = set()
        if 'vector_search' in migration_plan:
            search_files.update(migration_plan['vector_search']['affected_files'])
        if 'graph_search' in migration_plan:
            search_files.update(migration_plan['graph_search']['affected_files'])
        
        results = migrator.migrate_vector_graph_services(list(search_files))
        migrated_count = sum(results.values())
        total_migrated += migrated_count
        logger.info(f"ğŸ” æœç´¢æœåŠ¡è¿ç§»å®Œæˆ: {migrated_count}/{len(search_files)} ä¸ªæ–‡ä»¶")
    
    if 'cache_systems' in migration_plan:
        cache_files = migration_plan['cache_systems']['affected_files']
        results = migrator.migrate_cache_systems(cache_files)
        migrated_count = sum(results.values())
        total_migrated += migrated_count
        logger.info(f"ğŸ’¾ ç¼“å­˜ç³»ç»Ÿè¿ç§»å®Œæˆ: {migrated_count}/{len(cache_files)} ä¸ªæ–‡ä»¶")
    
    if 'thread_pools' in migration_plan:
        executor_files = migration_plan['thread_pools']['affected_files']
        results = migrator.migrate_thread_pools(executor_files)
        migrated_count = sum(results.values())
        total_migrated += migrated_count
        logger.info(f"ğŸš€ çº¿ç¨‹æ± è¿ç§»å®Œæˆ: {migrated_count}/{len(executor_files)} ä¸ªæ–‡ä»¶")
    
    logger.info(f"âœ… ç»Ÿä¸€æœåŠ¡è¿ç§»å®Œæˆï¼æ€»è®¡è¿ç§» {total_migrated} ä¸ªæ–‡ä»¶")
    logger.info("ğŸ“ è¯·è¿è¡Œæµ‹è¯•éªŒè¯è¿ç§»ç»“æœï¼Œå¿…è¦æ—¶ä»backupç›®å½•æ¢å¤")


if __name__ == "__main__":
    main()