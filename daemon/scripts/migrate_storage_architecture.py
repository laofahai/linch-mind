#!/usr/bin/env python3
"""
å­˜å‚¨æ¶æ„è¿ç§»è„šæœ¬
æ¸…ç†ç°æœ‰åƒåœ¾æ•°æ®å¹¶è¿ç§»åˆ°æ–°çš„æ™ºèƒ½å­˜å‚¨æ¶æ„
"""

import asyncio
import json
import logging
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.service_facade import get_service
from config.intelligent_storage import get_intelligent_storage_config
from services.ai.ollama_service import get_ollama_service
from services.storage.faiss_vector_store import get_faiss_vector_store, VectorDocument
from services.storage.intelligent_event_processor import get_intelligent_event_processor
from services.unified_database_service import UnifiedDatabaseService
from models.database_models import EntityMetadata, ConnectorLog

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class MigrationStats:
    """è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
    total_entities: int = 0
    garbage_entities: int = 0
    valuable_entities: int = 0
    migrated_entities: int = 0
    failed_migrations: int = 0
    storage_before_mb: float = 0.0
    storage_after_mb: float = 0.0
    processing_time_seconds: float = 0.0


class StorageMigration:
    """å­˜å‚¨æ¶æ„è¿ç§»å™¨"""
    
    def __init__(self):
        self.config = get_intelligent_storage_config()
        self.stats = MigrationStats()
        
        # æœåŠ¡ä¾èµ–
        self._db_service = None
        self._ollama_service = None
        self._vector_store = None
        self._intelligent_processor = None
        
        # åƒåœ¾æ•°æ®è¯†åˆ«è§„åˆ™
        self.garbage_patterns = [
            # 10KB+å‰ªè´´æ¿æ—¥å¿—
            lambda props: (
                props.get("event_type") == "clipboard_changed" and
                len(str(props.get("event_data", ""))) > 10000
            ),
            # é‡å¤çš„ä¸´æ—¶æ–‡ä»¶äº‹ä»¶
            lambda props: (
                "temp" in str(props.get("event_data", "")).lower() or
                "tmp" in str(props.get("event_data", "")).lower()
            ),
            # ç©ºå†…å®¹äº‹ä»¶
            lambda props: (
                not props.get("event_data") or
                str(props.get("event_data", "")).strip() == ""
            ),
            # ç³»ç»Ÿæ—¥å¿—å™ªéŸ³
            lambda props: (
                props.get("event_type") in ["system_log", "debug_log"] and
                len(str(props.get("event_data", ""))) > 5000
            ),
            # æ— æ„ä¹‰çš„é‡å¤æ•°æ®
            lambda props: (
                str(props.get("event_data", "")).count("\\n") > 100 or
                str(props.get("event_data", "")).count(" ") > 2000
            ),
        ]

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¿ç§»å™¨"""
        try:
            self._db_service = get_service(UnifiedDatabaseService)
            self._ollama_service = await get_ollama_service()
            self._vector_store = await get_faiss_vector_store()
            self._intelligent_processor = await get_intelligent_event_processor()
            
            logger.info("è¿ç§»å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"è¿ç§»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def run_complete_migration(self, dry_run: bool = False) -> MigrationStats:
        """æ‰§è¡Œå®Œæ•´çš„å­˜å‚¨è¿ç§»"""
        start_time = datetime.utcnow()
        
        try:
            logger.info("ğŸš€ å¼€å§‹å­˜å‚¨æ¶æ„è¿ç§»...")
            
            # 1. åˆ†æç°æœ‰æ•°æ®
            await self._analyze_existing_data()
            
            # 2. è¯†åˆ«å’Œå¤‡ä»½åƒåœ¾æ•°æ®
            garbage_entities = await self._identify_garbage_data()
            
            if not dry_run:
                # 3. å¤‡ä»½åƒåœ¾æ•°æ®
                await self._backup_garbage_data(garbage_entities)
                
                # 4. æ¸…ç†åƒåœ¾æ•°æ®
                await self._cleanup_garbage_data(garbage_entities)
            
            # 5. è¿ç§»æœ‰ä»·å€¼æ•°æ®
            valuable_entities = await self._identify_valuable_data()
            
            if not dry_run:
                await self._migrate_valuable_data(valuable_entities)
            
            # 6. éªŒè¯è¿ç§»ç»“æœ
            await self._verify_migration()
            
            # 7. ç”Ÿæˆè¿ç§»æŠ¥å‘Š
            await self._generate_migration_report(dry_run)
            
            self.stats.processing_time_seconds = (datetime.utcnow() - start_time).total_seconds()
            
            if dry_run:
                logger.info("âœ… è¿ç§»åˆ†æå®Œæˆï¼ˆè¯•è¿è¡Œæ¨¡å¼ï¼‰")
            else:
                logger.info("âœ… å­˜å‚¨æ¶æ„è¿ç§»å®Œæˆ")
            
            return self.stats
            
        except Exception as e:
            logger.error(f"å­˜å‚¨è¿ç§»å¤±è´¥: {e}")
            raise

    # === æ•°æ®åˆ†æ ===

    async def _analyze_existing_data(self):
        """åˆ†æç°æœ‰æ•°æ®"""
        logger.info("ğŸ” åˆ†æç°æœ‰æ•°æ®...")
        
        with self._db_service.get_session() as session:
            # ç»Ÿè®¡æ€»å®ä½“æ•°
            total_entities = session.query(EntityMetadata).count()
            self.stats.total_entities = total_entities
            
            # ç»Ÿè®¡è¿æ¥å™¨äº‹ä»¶
            connector_events = session.query(EntityMetadata).filter(
                EntityMetadata.type == "connector_event"
            ).count()
            
            # ç»Ÿè®¡æ™ºèƒ½äº‹ä»¶
            intelligent_events = session.query(EntityMetadata).filter(
                EntityMetadata.type == "intelligent_event"
            ).count()
            
            logger.info(f"æ•°æ®æ¦‚è§ˆ:")
            logger.info(f"  æ€»å®ä½“æ•°: {total_entities}")
            logger.info(f"  è¿æ¥å™¨äº‹ä»¶: {connector_events}")
            logger.info(f"  æ™ºèƒ½äº‹ä»¶: {intelligent_events}")

    async def _identify_garbage_data(self) -> List[EntityMetadata]:
        """è¯†åˆ«åƒåœ¾æ•°æ®"""
        logger.info("ğŸ—‘ï¸  è¯†åˆ«åƒåœ¾æ•°æ®...")
        
        garbage_entities = []
        
        with self._db_service.get_session() as session:
            # è·å–æ‰€æœ‰è¿æ¥å™¨äº‹ä»¶
            entities = session.query(EntityMetadata).filter(
                EntityMetadata.type == "connector_event"
            ).all()
            
            for entity in entities:
                properties = entity.properties or {}
                
                # åº”ç”¨åƒåœ¾è¯†åˆ«è§„åˆ™
                is_garbage = False
                for pattern in self.garbage_patterns:
                    try:
                        if pattern(properties):
                            is_garbage = True
                            break
                    except Exception as e:
                        logger.debug(f"åƒåœ¾è¯†åˆ«è§„åˆ™å¼‚å¸¸: {e}")
                
                if is_garbage:
                    garbage_entities.append(entity)
        
        self.stats.garbage_entities = len(garbage_entities)
        logger.info(f"è¯†åˆ«åˆ°åƒåœ¾æ•°æ®: {len(garbage_entities)} æ¡")
        
        return garbage_entities

    async def _identify_valuable_data(self) -> List[EntityMetadata]:
        """è¯†åˆ«æœ‰ä»·å€¼æ•°æ®"""
        logger.info("ğŸ’ è¯†åˆ«æœ‰ä»·å€¼æ•°æ®...")
        
        valuable_entities = []
        
        with self._db_service.get_session() as session:
            # è·å–æ‰€æœ‰è¿æ¥å™¨äº‹ä»¶
            entities = session.query(EntityMetadata).filter(
                EntityMetadata.type == "connector_event"
            ).all()
            
            for entity in entities:
                properties = entity.properties or {}
                
                # è·³è¿‡åƒåœ¾æ•°æ®
                is_garbage = False
                for pattern in self.garbage_patterns:
                    try:
                        if pattern(properties):
                            is_garbage = True
                            break
                    except Exception:
                        pass
                
                if not is_garbage:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰ä»·å€¼çš„å†…å®¹
                    event_data = properties.get("event_data", {})
                    content = self._extract_content_from_event_data(event_data)
                    
                    if content and len(content.strip()) > 10:  # è‡³å°‘10ä¸ªå­—ç¬¦
                        valuable_entities.append(entity)
        
        self.stats.valuable_entities = len(valuable_entities)
        logger.info(f"è¯†åˆ«åˆ°æœ‰ä»·å€¼æ•°æ®: {len(valuable_entities)} æ¡")
        
        return valuable_entities

    # === æ•°æ®æ¸…ç† ===

    async def _backup_garbage_data(self, garbage_entities: List[EntityMetadata]):
        """å¤‡ä»½åƒåœ¾æ•°æ®"""
        logger.info("ğŸ’¾ å¤‡ä»½åƒåœ¾æ•°æ®...")
        
        backup_dir = self.config.storage_dir / "backups" / "migration"
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backup_file = backup_dir / f"garbage_data_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        
        backup_data = []
        for entity in garbage_entities:
            backup_data.append({
                "entity_id": entity.entity_id,
                "name": entity.name,
                "type": entity.type,
                "description": entity.description,
                "properties": entity.properties,
                "created_at": entity.created_at.isoformat() if entity.created_at else None,
                "updated_at": entity.updated_at.isoformat() if entity.updated_at else None,
            })
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åƒåœ¾æ•°æ®å·²å¤‡ä»½åˆ°: {backup_file}")

    async def _cleanup_garbage_data(self, garbage_entities: List[EntityMetadata]):
        """æ¸…ç†åƒåœ¾æ•°æ®"""
        logger.info("ğŸ§¹ æ¸…ç†åƒåœ¾æ•°æ®...")
        
        cleaned_count = 0
        batch_size = 100
        
        # åˆ†æ‰¹åˆ é™¤
        for i in range(0, len(garbage_entities), batch_size):
            batch = garbage_entities[i:i + batch_size]
            
            with self._db_service.get_session() as session:
                for entity in batch:
                    try:
                        # åˆ é™¤ç›¸å…³è¿æ¥å™¨æ—¥å¿—
                        session.query(ConnectorLog).filter(
                            ConnectorLog.details.contains(entity.entity_id)
                        ).delete(synchronize_session=False)
                        
                        # åˆ é™¤å®ä½“
                        session.delete(entity)
                        cleaned_count += 1
                        
                    except Exception as e:
                        logger.error(f"åˆ é™¤å®ä½“å¤±è´¥ [{entity.entity_id}]: {e}")
                
                session.commit()
            
            logger.debug(f"å·²æ¸…ç† {cleaned_count}/{len(garbage_entities)} æ¡åƒåœ¾æ•°æ®")
        
        logger.info(f"âœ… åƒåœ¾æ•°æ®æ¸…ç†å®Œæˆ: {cleaned_count} æ¡")

    # === æ•°æ®è¿ç§» ===

    async def _migrate_valuable_data(self, valuable_entities: List[EntityMetadata]):
        """è¿ç§»æœ‰ä»·å€¼æ•°æ®åˆ°æ™ºèƒ½å­˜å‚¨"""
        logger.info("ğŸš€ è¿ç§»æœ‰ä»·å€¼æ•°æ®åˆ°æ™ºèƒ½å­˜å‚¨...")
        
        migrated_count = 0
        failed_count = 0
        batch_size = 50
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(valuable_entities), batch_size):
            batch = valuable_entities[i:i + batch_size]
            
            for entity in batch:
                try:
                    # æå–äº‹ä»¶ä¿¡æ¯
                    properties = entity.properties or {}
                    connector_id = properties.get("connector_id", "unknown")
                    event_type = properties.get("event_type", "unknown")
                    event_data = properties.get("event_data", {})
                    timestamp = properties.get("timestamp", datetime.utcnow().isoformat())
                    metadata = properties.get("metadata", {})
                    
                    # ä½¿ç”¨æ™ºèƒ½å¤„ç†å™¨é‡æ–°å¤„ç†
                    result = await self._intelligent_processor.process_connector_event(
                        connector_id=connector_id,
                        event_type=event_type,
                        event_data=event_data,
                        timestamp=timestamp,
                        metadata=metadata,
                    )
                    
                    if result.accepted:
                        migrated_count += 1
                        logger.debug(f"è¿ç§»æˆåŠŸ: {entity.entity_id}, ä»·å€¼={result.value_score:.3f}")
                        
                        # åˆ é™¤åŸæœ‰è®°å½•
                        with self._db_service.get_session() as session:
                            session.delete(entity)
                            session.commit()
                    else:
                        failed_count += 1
                        logger.debug(f"è¿ç§»æ‹’ç»: {entity.entity_id}, åŸå› ={result.reasoning}")
                        
                except Exception as e:
                    failed_count += 1
                    logger.error(f"è¿ç§»å¤±è´¥ [{entity.entity_id}]: {e}")
            
            logger.info(f"è¿ç§»è¿›åº¦: {migrated_count + failed_count}/{len(valuable_entities)}")
        
        self.stats.migrated_entities = migrated_count
        self.stats.failed_migrations = failed_count
        
        logger.info(f"âœ… æ•°æ®è¿ç§»å®Œæˆ - æˆåŠŸ: {migrated_count}, å¤±è´¥: {failed_count}")

    # === éªŒè¯å’ŒæŠ¥å‘Š ===

    async def _verify_migration(self):
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info("ğŸ” éªŒè¯è¿ç§»ç»“æœ...")
        
        with self._db_service.get_session() as session:
            # æ£€æŸ¥å‰©ä½™çš„è¿æ¥å™¨äº‹ä»¶
            remaining_events = session.query(EntityMetadata).filter(
                EntityMetadata.type == "connector_event"
            ).count()
            
            # æ£€æŸ¥æ–°çš„æ™ºèƒ½äº‹ä»¶
            intelligent_events = session.query(EntityMetadata).filter(
                EntityMetadata.type == "intelligent_event"
            ).count()
            
            logger.info(f"è¿ç§»éªŒè¯ç»“æœ:")
            logger.info(f"  å‰©ä½™è¿æ¥å™¨äº‹ä»¶: {remaining_events}")
            logger.info(f"  æ–°æ™ºèƒ½äº‹ä»¶: {intelligent_events}")
            
            if remaining_events > 0:
                logger.warning(f"ä»æœ‰ {remaining_events} ä¸ªè¿æ¥å™¨äº‹ä»¶æœªå¤„ç†")

    async def _generate_migration_report(self, dry_run: bool = False):
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆè¿ç§»æŠ¥å‘Š...")
        
        report = {
            "migration_summary": {
                "execution_mode": "dry_run" if dry_run else "full_migration",
                "start_time": datetime.utcnow().isoformat(),
                "processing_time_seconds": self.stats.processing_time_seconds,
            },
            "data_analysis": {
                "total_entities": self.stats.total_entities,
                "garbage_entities": self.stats.garbage_entities,
                "valuable_entities": self.stats.valuable_entities,
                "garbage_percentage": (self.stats.garbage_entities / self.stats.total_entities * 100) if self.stats.total_entities > 0 else 0,
            },
            "migration_results": {
                "migrated_entities": self.stats.migrated_entities,
                "failed_migrations": self.stats.failed_migrations,
                "migration_success_rate": (self.stats.migrated_entities / self.stats.valuable_entities * 100) if self.stats.valuable_entities > 0 else 0,
            },
            "storage_optimization": {
                "estimated_storage_reduction_mb": self.stats.storage_before_mb - self.stats.storage_after_mb,
                "compression_ratio": self.stats.storage_before_mb / self.stats.storage_after_mb if self.stats.storage_after_mb > 0 else 0,
            },
            "recommendations": self._generate_recommendations(),
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = self.config.storage_dir / "reports"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        mode_suffix = "dry_run" if dry_run else "migration"
        report_file = report_dir / f"storage_{mode_suffix}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        logger.info("ğŸ“Š è¿ç§»æŠ¥å‘Š:")
        logger.info(f"  æ€»å®ä½“æ•°: {self.stats.total_entities}")
        logger.info(f"  åƒåœ¾æ•°æ®: {self.stats.garbage_entities} ({report['data_analysis']['garbage_percentage']:.1f}%)")
        logger.info(f"  æœ‰ä»·å€¼æ•°æ®: {self.stats.valuable_entities}")
        logger.info(f"  æˆåŠŸè¿ç§»: {self.stats.migrated_entities}")
        logger.info(f"  è¿ç§»æˆåŠŸç‡: {report['migration_results']['migration_success_rate']:.1f}%")
        
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åƒåœ¾æ•°æ®æ¯”ä¾‹è¿‡é«˜
        if self.stats.total_entities > 0:
            garbage_rate = self.stats.garbage_entities / self.stats.total_entities
            if garbage_rate > 0.3:
                recommendations.append("åƒåœ¾æ•°æ®æ¯”ä¾‹è¿‡é«˜ï¼Œå»ºè®®è°ƒæ•´è¿æ¥å™¨é…ç½®ä»¥å‡å°‘æ— ç”¨æ•°æ®é‡‡é›†")
        
        # è¿ç§»å¤±è´¥ç‡è¿‡é«˜
        if self.stats.valuable_entities > 0:
            failure_rate = self.stats.failed_migrations / self.stats.valuable_entities
            if failure_rate > 0.1:
                recommendations.append("è¿ç§»å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥AIæ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥")
        
        # æ•°æ®é‡å»ºè®®
        if self.stats.total_entities < 100:
            recommendations.append("æ•°æ®é‡è¾ƒå°‘ï¼Œå»ºè®®è¿è¡Œä¸€æ®µæ—¶é—´åå†æ¬¡è¿ç§»ä»¥è·å¾—æ›´å¥½çš„AIè¯„ä¼°æ•ˆæœ")
        
        # æ€§èƒ½å»ºè®®
        if self.stats.processing_time_seconds > 300:  # 5åˆ†é’Ÿ
            recommendations.append("è¿ç§»è€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®åœ¨ç³»ç»Ÿç©ºé—²æ—¶æ®µæ‰§è¡Œæˆ–è°ƒæ•´æ‰¹å¤„ç†å¤§å°")
        
        if not recommendations:
            recommendations.append("è¿ç§»è¿‡ç¨‹æ­£å¸¸ï¼Œå»ºè®®å®šæœŸæ‰§è¡Œæ•°æ®æ¸…ç†ç»´æŠ¤")
        
        return recommendations

    # === å·¥å…·æ–¹æ³• ===

    def _extract_content_from_event_data(self, event_data: Any) -> Optional[str]:
        """ä»äº‹ä»¶æ•°æ®ä¸­æå–å†…å®¹"""
        if isinstance(event_data, dict):
            content_fields = ["content", "text", "data", "message", "body", "value"]
            for field in content_fields:
                if field in event_data:
                    content = event_data[field]
                    if isinstance(content, str) and content.strip():
                        return content.strip()
        elif isinstance(event_data, str):
            return event_data.strip()
        
        return None

    # === å…¬å…±æ¥å£ ===

    async def analyze_only(self) -> Dict[str, Any]:
        """ä»…åˆ†ææ•°æ®ï¼Œä¸æ‰§è¡Œè¿ç§»"""
        await self._analyze_existing_data()
        garbage_entities = await self._identify_garbage_data()
        valuable_entities = await self._identify_valuable_data()
        
        return {
            "total_entities": self.stats.total_entities,
            "garbage_entities": self.stats.garbage_entities,
            "valuable_entities": self.stats.valuable_entities,
            "garbage_samples": [
                {
                    "entity_id": entity.entity_id,
                    "name": entity.name,
                    "size_kb": len(str(entity.properties)) / 1024,
                }
                for entity in garbage_entities[:5]  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
            ],
            "valuable_samples": [
                {
                    "entity_id": entity.entity_id,
                    "name": entity.name,
                    "event_type": entity.properties.get("event_type", "unknown"),
                }
                for entity in valuable_entities[:5]  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
            ],
        }


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Linch Mind å­˜å‚¨æ¶æ„è¿ç§»å·¥å…·")
    print("=" * 50)
    
    migration = StorageMigration()
    
    if not await migration.initialize():
        print("âŒ è¿ç§»å·¥å…·åˆå§‹åŒ–å¤±è´¥")
        return 1
    
    # å‘½ä»¤è¡Œå‚æ•°å¤„ç†
    mode = "interactive"
    if len(sys.argv) > 1:
        arg = sys.argv[1].lower()
        if arg in ("analyze", "dry-run", "migrate"):
            mode = arg
    
    try:
        if mode == "analyze":
            # ä»…åˆ†ææ¨¡å¼
            print("ğŸ” åˆ†æç°æœ‰æ•°æ®...")
            analysis = await migration.analyze_only()
            
            print("\nğŸ“Š æ•°æ®åˆ†æç»“æœ:")
            print(f"  æ€»å®ä½“æ•°: {analysis['total_entities']}")
            print(f"  åƒåœ¾æ•°æ®: {analysis['garbage_entities']}")
            print(f"  æœ‰ä»·å€¼æ•°æ®: {analysis['valuable_entities']}")
            print(f"  åƒåœ¾æ¯”ä¾‹: {analysis['garbage_entities']/analysis['total_entities']*100:.1f}%")
            
            if analysis['garbage_samples']:
                print("\nğŸ—‘ï¸  åƒåœ¾æ•°æ®æ ·æœ¬:")
                for sample in analysis['garbage_samples']:
                    print(f"    - {sample['entity_id']}: {sample['size_kb']:.1f}KB")
            
        elif mode == "dry-run":
            # è¯•è¿è¡Œæ¨¡å¼
            print("ğŸ§ª æ‰§è¡Œè¿ç§»è¯•è¿è¡Œ...")
            stats = await migration.run_complete_migration(dry_run=True)
            print(f"\nâœ… è¯•è¿è¡Œå®Œæˆï¼Œå¤„ç†æ—¶é—´: {stats.processing_time_seconds:.1f}ç§’")
            
        elif mode == "migrate":
            # å®Œæ•´è¿ç§»æ¨¡å¼
            print("âš ï¸  å³å°†æ‰§è¡Œå®Œæ•´è¿ç§»ï¼Œè¿™å°†åˆ é™¤åƒåœ¾æ•°æ®ï¼")
            confirm = input("ç¡®è®¤ç»§ç»­? (yes/no): ")
            if confirm.lower() == "yes":
                print("ğŸš€ æ‰§è¡Œå®Œæ•´è¿ç§»...")
                stats = await migration.run_complete_migration(dry_run=False)
                print(f"\nâœ… è¿ç§»å®Œæˆï¼Œå¤„ç†æ—¶é—´: {stats.processing_time_seconds:.1f}ç§’")
            else:
                print("å–æ¶ˆè¿ç§»")
                return 0
                
        else:
            # äº¤äº’æ¨¡å¼
            print("è¯·é€‰æ‹©æ“ä½œæ¨¡å¼:")
            print("1. åˆ†ææ•°æ® (analyze)")
            print("2. è¯•è¿è¡Œ (dry-run)")
            print("3. å®Œæ•´è¿ç§» (migrate)")
            
            choice = input("è¾“å…¥é€‰æ‹© (1-3): ").strip()
            
            if choice == "1":
                analysis = await migration.analyze_only()
                print(f"\nğŸ“Š åˆ†æå®Œæˆ - åƒåœ¾æ•°æ®: {analysis['garbage_entities']}, æœ‰ä»·å€¼æ•°æ®: {analysis['valuable_entities']}")
            elif choice == "2":
                stats = await migration.run_complete_migration(dry_run=True)
                print(f"\nâœ… è¯•è¿è¡Œå®Œæˆ")
            elif choice == "3":
                confirm = input("âš ï¸  ç¡®è®¤æ‰§è¡Œå®Œæ•´è¿ç§»? (yes/no): ")
                if confirm.lower() == "yes":
                    stats = await migration.run_complete_migration(dry_run=False)
                    print(f"\nâœ… è¿ç§»å®Œæˆ")
                else:
                    print("å–æ¶ˆè¿ç§»")
            else:
                print("æ— æ•ˆé€‰æ‹©")
                return 1
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logger.exception("è¿ç§»å¼‚å¸¸è¯¦æƒ…")
        return 1


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¿ç§»")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)