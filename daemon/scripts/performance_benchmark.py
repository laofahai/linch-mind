#!/usr/bin/env python3
"""
æ¶æ„ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•
éªŒè¯ç»Ÿä¸€æœåŠ¡æ¶æ„çš„æ€§èƒ½æ”¹è¿›æ•ˆæœ
"""

import asyncio
import time
import logging
import statistics
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    throughput: float  # operations per second
    memory_usage_mb: float
    error_count: int = 0


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
        
    async def run_all_benchmarks(self) -> Dict[str, BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹æ¶æ„ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        benchmarks = [
            ("unified_search_performance", self.benchmark_unified_search),
            ("unified_cache_performance", self.benchmark_unified_cache),
            ("shared_executor_performance", self.benchmark_shared_executor),
            ("servicefacade_performance", self.benchmark_servicefacade),
            ("memory_usage_comparison", self.benchmark_memory_usage),
        ]
        
        results = {}
        
        for test_name, benchmark_func in benchmarks:
            logger.info(f"ğŸƒ è¿è¡ŒåŸºå‡†æµ‹è¯•: {test_name}")
            try:
                result = await benchmark_func()
                results[test_name] = result
                self.results.append(result)
                logger.info(f"âœ… {test_name} å®Œæˆ: {result.avg_time:.3f}s avg, {result.throughput:.1f} ops/s")
            except Exception as e:
                logger.error(f"âŒ {test_name} å¤±è´¥: {e}")
        
        return results
    
    async def benchmark_unified_search(self) -> BenchmarkResult:
        """æµ‹è¯•ç»Ÿä¸€æœç´¢æœåŠ¡æ€§èƒ½"""
        from services.unified_search_service import get_unified_search_service, SearchQuery, SearchType
        
        search_service = await get_unified_search_service()
        iterations = 100
        times = []
        error_count = 0
        
        # é¢„çƒ­
        try:
            query = SearchQuery(
                query="test",
                search_type=SearchType.SEMANTIC,
                limit=10
            )
            await search_service.search(query)
        except:
            pass  # é¢„çƒ­å¯èƒ½å¤±è´¥ï¼Œå› ä¸ºæ²¡æœ‰æ•°æ®
        
        start_memory = self._get_memory_usage()
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            try:
                # æµ‹è¯•å¤šç§æœç´¢ç±»å‹
                queries = [
                    SearchQuery(query=f"test_{i}", search_type=SearchType.SEMANTIC, limit=5),
                    SearchQuery(query=f"test_{i}", search_type=SearchType.TEXT, limit=5),
                    SearchQuery(query="", search_type=SearchType.GRAPH, start_entity_id="test_id", limit=5),
                ]
                
                for query in queries:
                    await search_service.search(query)
                
            except Exception:
                error_count += 1
            
            end_time = time.perf_counter()
            times.append(end_time - start_time)
        
        end_memory = self._get_memory_usage()
        
        return BenchmarkResult(
            test_name="unified_search_performance",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            throughput=iterations / sum(times),
            memory_usage_mb=end_memory - start_memory,
            error_count=error_count
        )
    
    async def benchmark_unified_cache(self) -> BenchmarkResult:
        """æµ‹è¯•ç»Ÿä¸€ç¼“å­˜æœåŠ¡æ€§èƒ½"""
        from services.unified_cache_service import get_unified_cache_service, CacheType
        
        cache_service = get_unified_cache_service()
        iterations = 1000
        times = []
        error_count = 0
        
        start_memory = self._get_memory_usage()
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            try:
                # æµ‹è¯•ç¼“å­˜æ“ä½œ
                key = f"test_key_{i % 100}"  # å¤ç”¨ä¸€äº›é”®æ¥æµ‹è¯•ç¼“å­˜å‘½ä¸­
                value = f"test_value_{i}"
                
                # è®¾ç½®ç¼“å­˜
                await cache_service.set(key, value, CacheType.MEMORY)
                
                # è·å–ç¼“å­˜
                result = await cache_service.get(key, CacheType.MEMORY)
                
                # æ‰¹é‡æ“ä½œ
                if i % 10 == 0:
                    batch_items = {f"batch_{j}": f"value_{j}" for j in range(5)}
                    await cache_service.mset(batch_items, CacheType.MEMORY)
                    await cache_service.mget(list(batch_items.keys()), CacheType.MEMORY)
                
            except Exception:
                error_count += 1
            
            end_time = time.perf_counter()
            times.append(end_time - start_time)
        
        end_memory = self._get_memory_usage()
        
        return BenchmarkResult(
            test_name="unified_cache_performance",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            throughput=iterations / sum(times),
            memory_usage_mb=end_memory - start_memory,
            error_count=error_count
        )
    
    async def benchmark_shared_executor(self) -> BenchmarkResult:
        """æµ‹è¯•å…±äº«æ‰§è¡Œå™¨æœåŠ¡æ€§èƒ½"""
        from services.shared_executor_service import get_shared_executor_service, TaskType
        
        executor_service = get_shared_executor_service()
        iterations = 100
        times = []
        error_count = 0
        
        def cpu_task(n):
            """CPUå¯†é›†å‹ä»»åŠ¡"""
            return sum(i * i for i in range(n))
        
        def io_task():
            """IOæ¨¡æ‹Ÿä»»åŠ¡"""
            time.sleep(0.001)  # æ¨¡æ‹ŸIOå»¶è¿Ÿ
            return "io_result"
        
        start_memory = self._get_memory_usage()
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            try:
                # æäº¤å¤šç§ç±»å‹çš„ä»»åŠ¡
                tasks = [
                    executor_service.submit(lambda: cpu_task(100), TaskType.CPU),
                    executor_service.submit(io_task, TaskType.IO),
                    executor_service.submit(lambda: f"result_{i}", TaskType.GENERAL),
                ]
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
            except Exception:
                error_count += 1
            
            end_time = time.perf_counter()
            times.append(end_time - start_time)
        
        end_memory = self._get_memory_usage()
        
        return BenchmarkResult(
            test_name="shared_executor_performance",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            throughput=iterations / sum(times),
            memory_usage_mb=end_memory - start_memory,
            error_count=error_count
        )
    
    async def benchmark_servicefacade(self) -> BenchmarkResult:
        """æµ‹è¯•ServiceFacadeæ€§èƒ½"""
        from core.service_facade import get_service_facade
        from services.unified_cache_service import UnifiedCacheService, get_unified_cache_service
        from core.container import get_container
        
        # ç¡®ä¿æœåŠ¡å·²æ³¨å†Œ
        cache_service = get_unified_cache_service()
        container = get_container()
        container.register_instance(UnifiedCacheService, cache_service)
        
        facade = get_service_facade()
        iterations = 1000
        times = []
        error_count = 0
        
        start_memory = self._get_memory_usage()
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            try:
                # æµ‹è¯•æœåŠ¡è·å–ï¼ˆåº”è¯¥æœ‰ç¼“å­˜ï¼‰
                service = facade.get_service(UnifiedCacheService)
                
                # æµ‹è¯•æœåŠ¡çŠ¶æ€æ£€æŸ¥
                available = facade.is_service_available(UnifiedCacheService)
                
                # æµ‹è¯•å®‰å…¨è·å–
                result = facade.get_service_safe(UnifiedCacheService)
                
            except Exception:
                error_count += 1
            
            end_time = time.perf_counter()
            times.append(end_time - start_time)
        
        end_memory = self._get_memory_usage()
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        stats = facade.get_service_stats()
        logger.info(f"ServiceFacadeç»Ÿè®¡: {stats}")
        
        return BenchmarkResult(
            test_name="servicefacade_performance",
            iterations=iterations,
            total_time=sum(times),
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            throughput=iterations / sum(times),
            memory_usage_mb=end_memory - start_memory,
            error_count=error_count
        )
    
    async def benchmark_memory_usage(self) -> BenchmarkResult:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import gc
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        start_memory = self._get_memory_usage()
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»Ÿä¸€æœåŠ¡
        from services.unified_search_service import get_unified_search_service
        from services.unified_cache_service import get_unified_cache_service
        from services.shared_executor_service import get_shared_executor_service
        
        search_service = await get_unified_search_service()
        cache_service = get_unified_cache_service()
        executor_service = get_shared_executor_service()
        
        # æ‰§è¡Œä¸€äº›æ“ä½œ
        for i in range(100):
            await cache_service.set(f"key_{i}", f"value_{i}")
        
        gc.collect()
        end_memory = self._get_memory_usage()
        
        return BenchmarkResult(
            test_name="memory_usage_comparison",
            iterations=1,
            total_time=0.0,
            avg_time=0.0,
            min_time=0.0,
            max_time=0.0,
            throughput=0.0,
            memory_usage_mb=end_memory - start_memory,
            error_count=0
        )
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ† æ¶æ„ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•é¡¹ç›®æ•°: {len(self.results)}")
        report.append("")
        
        # è¯¦ç»†ç»“æœ
        for result in self.results:
            report.append(f"ğŸ“Š {result.test_name}")
            report.append("-" * 60)
            report.append(f"  è¿­ä»£æ¬¡æ•°: {result.iterations}")
            report.append(f"  æ€»æ—¶é—´: {result.total_time:.3f}s")
            report.append(f"  å¹³å‡æ—¶é—´: {result.avg_time:.3f}s")
            report.append(f"  æœ€å°æ—¶é—´: {result.min_time:.3f}s")
            report.append(f"  æœ€å¤§æ—¶é—´: {result.max_time:.3f}s")
            report.append(f"  ååé‡: {result.throughput:.1f} ops/s")
            report.append(f"  å†…å­˜ä½¿ç”¨: {result.memory_usage_mb:.1f} MB")
            if result.error_count > 0:
                report.append(f"  é”™è¯¯æ•°: {result.error_count}")
            report.append("")
        
        # æ€§èƒ½æ€»ç»“
        report.append("ğŸ¯ æ€§èƒ½æ€»ç»“")
        report.append("-" * 60)
        
        total_throughput = sum(r.throughput for r in self.results if r.throughput > 0)
        total_memory = sum(r.memory_usage_mb for r in self.results)
        total_errors = sum(r.error_count for r in self.results)
        
        report.append(f"  æ€»ååé‡: {total_throughput:.1f} ops/s")
        report.append(f"  æ€»å†…å­˜ä½¿ç”¨: {total_memory:.1f} MB")
        report.append(f"  æ€»é”™è¯¯æ•°: {total_errors}")
        
        # æ¶æ„ä¼˜åŒ–æ•ˆæœè¯„ä¼°
        report.append("")
        report.append("ğŸš€ æ¶æ„ä¼˜åŒ–æ•ˆæœè¯„ä¼°")
        report.append("-" * 60)
        
        # æ ¹æ®æ€§èƒ½ç»“æœç»™å‡ºè¯„ä¼°
        if total_throughput > 1000:
            report.append("  âœ… é«˜æ€§èƒ½: ç³»ç»Ÿååé‡ä¼˜å¼‚")
        elif total_throughput > 500:
            report.append("  âœ… è‰¯å¥½æ€§èƒ½: ç³»ç»Ÿååé‡è‰¯å¥½")
        else:
            report.append("  âš ï¸  æ€§èƒ½éœ€è¦ä¼˜åŒ–")
        
        if total_memory < 100:
            report.append("  âœ… å†…å­˜ä½¿ç”¨åˆç†")
        elif total_memory < 200:
            report.append("  âš ï¸  å†…å­˜ä½¿ç”¨è¾ƒé«˜")
        else:
            report.append("  âŒ å†…å­˜ä½¿ç”¨è¿‡é«˜")
        
        if total_errors == 0:
            report.append("  âœ… é›¶é”™è¯¯: ç³»ç»Ÿç¨³å®šæ€§ä¼˜å¼‚")
        else:
            report.append(f"  âš ï¸  å‘ç° {total_errors} ä¸ªé”™è¯¯ï¼Œéœ€è¦æ£€æŸ¥")
        
        return "\n".join(report)


async def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    
    try:
        # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
        results = await benchmark.run_all_benchmarks()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = benchmark.generate_report()
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = Path("performance_benchmark_report.txt")
        report_file.write_text(report, encoding='utf-8')
        logger.info(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # è¿”å›ç®€è¦ç»“æœ
        return {
            "success": True,
            "total_tests": len(results),
            "total_throughput": sum(r.throughput for r in benchmark.results if r.throughput > 0),
            "total_memory_mb": sum(r.memory_usage_mb for r in benchmark.results),
            "total_errors": sum(r.error_count for r in benchmark.results)
        }
        
    except Exception as e:
        logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


if __name__ == "__main__":
    result = asyncio.run(main())
    print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ: {result}")