#!/usr/bin/env python3
"""
Filesystemè¿æ¥å™¨æ€§èƒ½åˆ†æå·¥å…·
åˆ†æé«˜CPUä½¿ç”¨ç‡é—®é¢˜å¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import os
import sys
import time
import json
import psutil
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import Dict, List, Any, Optional
import signal

class PerformanceAnalyzer:
    def __init__(self, connector_process_name: str = "linch-mind-filesystem"):
        self.connector_process_name = connector_process_name
        self.process: Optional[psutil.Process] = None
        self.monitoring = False
        self.metrics = defaultdict(list)
        self.fs_events = deque(maxlen=1000)  # ä¿æŒæœ€è¿‘1000ä¸ªFSäº‹ä»¶
        self.analysis_results = {}
        
    def find_connector_process(self) -> Optional[psutil.Process]:
        """æŸ¥æ‰¾filesystemè¿æ¥å™¨è¿›ç¨‹"""
        for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'cpu_percent', 'memory_info']):
            try:
                if self.connector_process_name in proc.info['name'] or \
                   any(self.connector_process_name in arg for arg in proc.info['cmdline'] or []):
                    return psutil.Process(proc.info['pid'])
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        return None
    
    def collect_system_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»Ÿå’Œè¿›ç¨‹æ€§èƒ½æŒ‡æ ‡"""
        if not self.process:
            return {}
        
        try:
            # è¿›ç¨‹åŸºç¡€æŒ‡æ ‡
            cpu_percent = self.process.cpu_percent()
            memory_info = self.process.memory_info()
            
            # I/Oè®¡æ•°å™¨ï¼ˆå¯èƒ½ä¸æ”¯æŒæ‰€æœ‰å¹³å°ï¼‰
            try:
                io_counters = self.process.io_counters()
                io_read_count = io_counters.read_count
                io_write_count = io_counters.write_count
                io_read_bytes = io_counters.read_bytes
                io_write_bytes = io_counters.write_bytes
            except (AttributeError, psutil.AccessDenied):
                io_read_count = io_write_count = io_read_bytes = io_write_bytes = 0
            
            # ç³»ç»Ÿæ•´ä½“æŒ‡æ ‡
            system_cpu = psutil.cpu_percent(interval=0.1)
            system_memory = psutil.virtual_memory()
            
            # çº¿ç¨‹å’Œæ–‡ä»¶æè¿°ç¬¦
            num_threads = self.process.num_threads()
            try:
                if hasattr(self.process, 'num_fds'):
                    num_fds = self.process.num_fds()
                else:
                    # macOS/Windows fallback
                    num_fds = len(self.process.open_files())
            except (AttributeError, psutil.AccessDenied):
                num_fds = 0
            
            return {
                'timestamp': datetime.now(),
                'process': {
                    'cpu_percent': cpu_percent,
                    'memory_rss': memory_info.rss,
                    'memory_vms': memory_info.vms,
                    'io_read_count': io_read_count,
                    'io_write_count': io_write_count,
                    'io_read_bytes': io_read_bytes,
                    'io_write_bytes': io_write_bytes,
                    'num_threads': num_threads,
                    'num_fds': num_fds,
                },
                'system': {
                    'cpu_percent': system_cpu,
                    'memory_percent': system_memory.percent,
                    'memory_available': system_memory.available,
                }
            }
        except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
            print(f"æ— æ³•æ”¶é›†è¿›ç¨‹æŒ‡æ ‡: {e}")
            return {}
    
    def monitor_fs_events(self):
        """ç›‘æ§æ–‡ä»¶ç³»ç»Ÿäº‹ä»¶ï¼ˆé€šè¿‡ç³»ç»Ÿè°ƒç”¨ç›‘æ§ï¼‰"""
        try:
            if sys.platform == 'darwin':
                # macOS: ä½¿ç”¨fs_usageç›‘æ§
                cmd = ['sudo', 'fs_usage', '-f', 'filesys', '-p', str(self.process.pid)]
            elif sys.platform.startswith('linux'):
                # Linux: ä½¿ç”¨straceç›‘æ§ç³»ç»Ÿè°ƒç”¨
                cmd = ['strace', '-p', str(self.process.pid), '-e', 'trace=file']
            else:
                print("ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ")
                return
            
            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
                                  text=True, preexec_fn=os.setsid)
            
            while self.monitoring:
                line = proc.stdout.readline()
                if line:
                    self.fs_events.append({
                        'timestamp': datetime.now(),
                        'event': line.strip()
                    })
                    
        except Exception as e:
            print(f"ç›‘æ§FSäº‹ä»¶å¤±è´¥: {e}")
    
    def analyze_monitored_directories(self) -> Dict[str, Any]:
        """åˆ†æè¢«ç›‘æ§çš„ç›®å½•ç»Ÿè®¡ä¿¡æ¯"""
        # è¯»å–è¿æ¥å™¨é…ç½®
        config_path = Path(__file__).parent.parent.parent / "connectors/official/filesystem/connector.toml"
        
        if not config_path.exists():
            return {"error": "è¿æ¥å™¨é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}
        
        # ä»é…ç½®è·å–ç›‘æ§ç›®å½•
        watch_dirs = ["~/Desktop", "~/Documents"]  # é»˜è®¤é…ç½®
        
        analysis = {
            'directories': [],
            'total_files': 0,
            'total_size': 0,
            'file_types': defaultdict(int),
            'large_files': [],
            'deep_directories': []
        }
        
        for watch_dir in watch_dirs:
            expanded_dir = Path(watch_dir).expanduser()
            if not expanded_dir.exists():
                continue
            
            dir_analysis = self._analyze_directory(expanded_dir)
            analysis['directories'].append(dir_analysis)
            analysis['total_files'] += dir_analysis['file_count']
            analysis['total_size'] += dir_analysis['total_size']
            
            # åˆå¹¶æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            for ext, count in dir_analysis['file_types'].items():
                analysis['file_types'][ext] += count
        
        return analysis
    
    def _analyze_directory(self, directory: Path, max_depth: int = 5) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªç›®å½•çš„è¯¦ç»†ä¿¡æ¯"""
        analysis = {
            'path': str(directory),
            'file_count': 0,
            'directory_count': 0,
            'total_size': 0,
            'max_depth_reached': 0,
            'file_types': defaultdict(int),
            'large_files': [],  # > 10MB
            'recent_files': [],  # æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
            'problematic_paths': []  # å¯èƒ½å¯¼è‡´é«˜é¢‘äº‹ä»¶çš„è·¯å¾„
        }
        
        try:
            for item in directory.rglob('*'):
                try:
                    # æ£€æŸ¥æ·±åº¦
                    depth = len(item.relative_to(directory).parts)
                    analysis['max_depth_reached'] = max(analysis['max_depth_reached'], depth)
                    
                    if depth > max_depth:
                        continue
                    
                    if item.is_file():
                        analysis['file_count'] += 1
                        size = item.stat().st_size
                        analysis['total_size'] += size
                        
                        # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
                        ext = item.suffix.lower()
                        analysis['file_types'][ext] += 1
                        
                        # å¤§æ–‡ä»¶
                        if size > 10 * 1024 * 1024:  # > 10MB
                            analysis['large_files'].append({
                                'path': str(item),
                                'size': size,
                                'size_mb': size / (1024 * 1024)
                            })
                        
                        # æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
                        mtime = datetime.fromtimestamp(item.stat().st_mtime)
                        if mtime > datetime.now() - timedelta(minutes=30):
                            analysis['recent_files'].append({
                                'path': str(item),
                                'mtime': mtime
                            })
                        
                        # æ£€æŸ¥é—®é¢˜è·¯å¾„
                        if self._is_problematic_path(item):
                            analysis['problematic_paths'].append(str(item))
                    
                    elif item.is_dir():
                        analysis['directory_count'] += 1
                        
                except (OSError, PermissionError):
                    continue
                    
        except Exception as e:
            analysis['error'] = str(e)
        
        return analysis
    
    def _is_problematic_path(self, path: Path) -> bool:
        """è¯†åˆ«å¯èƒ½å¯¼è‡´é«˜é¢‘äº‹ä»¶çš„è·¯å¾„"""
        problematic_patterns = [
            '.DS_Store', 'Thumbs.db', '~$', '.tmp', '.log',
            '__pycache__', 'node_modules', '.git', '.cache',
            'Cache', 'Logs', 'Temp'
        ]
        
        path_str = str(path).lower()
        return any(pattern.lower() in path_str for pattern in problematic_patterns)
    
    def start_monitoring(self, duration: int = 60):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        self.process = self.find_connector_process()
        if not self.process:
            print(f"æœªæ‰¾åˆ°{self.connector_process_name}è¿›ç¨‹")
            return
        
        print(f"å¼€å§‹ç›‘æ§è¿›ç¨‹ PID {self.process.pid} æŒç»­ {duration} ç§’...")
        self.monitoring = True
        
        # å¯åŠ¨FSäº‹ä»¶ç›‘æ§çº¿ç¨‹
        fs_thread = threading.Thread(target=self.monitor_fs_events, daemon=True)
        fs_thread.start()
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        start_time = time.time()
        while time.time() - start_time < duration and self.monitoring:
            metrics = self.collect_system_metrics()
            if metrics:
                self.metrics['timeline'].append(metrics)
            time.sleep(1)
        
        self.monitoring = False
        print("ç›‘æ§å®Œæˆ")
    
    def analyze_performance_issues(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½é—®é¢˜å¹¶ç”ŸæˆæŠ¥å‘Š"""
        if not self.metrics['timeline']:
            return {"error": "æ²¡æœ‰æ”¶é›†åˆ°æ€§èƒ½æ•°æ®"}
        
        # CPUä½¿ç”¨ç‡åˆ†æ
        cpu_values = [m['process']['cpu_percent'] for m in self.metrics['timeline']]
        avg_cpu = sum(cpu_values) / len(cpu_values)
        max_cpu = max(cpu_values)
        
        # å†…å­˜ä½¿ç”¨åˆ†æ
        memory_values = [m['process']['memory_rss'] for m in self.metrics['timeline']]
        avg_memory = sum(memory_values) / len(memory_values)
        max_memory = max(memory_values)
        
        # I/Oåˆ†æ
        if len(self.metrics['timeline']) > 1:
            io_read_diff = (self.metrics['timeline'][-1]['process']['io_read_count'] - 
                           self.metrics['timeline'][0]['process']['io_read_count'])
            io_write_diff = (self.metrics['timeline'][-1]['process']['io_write_count'] - 
                            self.metrics['timeline'][0]['process']['io_write_count'])
        else:
            io_read_diff = io_write_diff = 0
        
        # çº¿ç¨‹å’Œæ–‡ä»¶æè¿°ç¬¦åˆ†æ
        thread_values = [m['process']['num_threads'] for m in self.metrics['timeline']]
        fd_values = [m['process']['num_fds'] for m in self.metrics['timeline']]
        
        analysis = {
            'performance_summary': {
                'avg_cpu_percent': avg_cpu,
                'max_cpu_percent': max_cpu,
                'avg_memory_mb': avg_memory / (1024 * 1024),
                'max_memory_mb': max_memory / (1024 * 1024),
                'io_read_operations': io_read_diff,
                'io_write_operations': io_write_diff,
                'avg_threads': sum(thread_values) / len(thread_values),
                'max_file_descriptors': max(fd_values),
            },
            'issues_identified': [],
            'recommendations': []
        }
        
        # é—®é¢˜è¯†åˆ«
        if avg_cpu > 50:
            analysis['issues_identified'].append({
                'type': 'high_cpu_usage',
                'severity': 'critical' if avg_cpu > 80 else 'high',
                'description': f'å¹³å‡CPUä½¿ç”¨ç‡ {avg_cpu:.1f}% è¿‡é«˜',
                'impact': 'CPUèµ„æºè¿‡åº¦æ¶ˆè€—ï¼Œå½±å“ç³»ç»Ÿæ•´ä½“æ€§èƒ½'
            })
        
        if max_memory > 500 * 1024 * 1024:  # > 500MB
            analysis['issues_identified'].append({
                'type': 'high_memory_usage',
                'severity': 'high',
                'description': f'å†…å­˜ä½¿ç”¨å³°å€¼ {max_memory/(1024*1024):.1f}MB è¿‡é«˜',
                'impact': 'å†…å­˜æ¶ˆè€—è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿå“åº”ç¼“æ…¢'
            })
        
        if io_read_diff > 10000:  # é«˜é¢‘I/O
            analysis['issues_identified'].append({
                'type': 'excessive_io',
                'severity': 'high',
                'description': f'I/Oæ“ä½œè¿‡äºé¢‘ç¹ ({io_read_diff} æ¬¡è¯»å–)',
                'impact': 'é¢‘ç¹çš„ç£ç›˜I/Oæ“ä½œæ¶ˆè€—CPUå’Œç³»ç»Ÿèµ„æº'
            })
        
        if max(fd_values) > 100:
            analysis['issues_identified'].append({
                'type': 'too_many_file_descriptors',
                'severity': 'medium',
                'description': f'æ–‡ä»¶æè¿°ç¬¦ä½¿ç”¨è¿‡å¤š ({max(fd_values)})',
                'impact': 'å¯èƒ½å­˜åœ¨æ–‡ä»¶æè¿°ç¬¦æ³„æ¼æˆ–ç›‘æ§è¿‡å¤šæ–‡ä»¶'
            })
        
        # FSäº‹ä»¶åˆ†æ
        if len(self.fs_events) > 500:  # é«˜é¢‘äº‹ä»¶
            analysis['issues_identified'].append({
                'type': 'excessive_fs_events',
                'severity': 'critical',
                'description': f'æ–‡ä»¶ç³»ç»Ÿäº‹ä»¶è¿‡äºé¢‘ç¹ ({len(self.fs_events)} ä¸ªäº‹ä»¶)',
                'impact': 'æ–‡ä»¶ç³»ç»Ÿäº‹ä»¶å¤„ç†è¿‡è½½ï¼Œå¯¼è‡´CPUä½¿ç”¨ç‡é£™å‡'
            })
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict[str, Any]):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        issues = {issue['type'] for issue in analysis['issues_identified']}
        
        if 'high_cpu_usage' in issues:
            recommendations.append({
                'priority': 'critical',
                'category': 'configuration',
                'title': 'è°ƒæ•´äº‹ä»¶æ‰¹å¤„ç†é…ç½®',
                'description': 'å¢åŠ batch_intervalå’Œdebounce_timeä»¥å‡å°‘äº‹ä»¶å¤„ç†é¢‘ç‡',
                'implementation': 'å°†batch_intervalä»300mså¢åŠ åˆ°1000msï¼Œdebounce_timeä»300mså¢åŠ åˆ°2000ms'
            })
        
        if 'excessive_fs_events' in issues:
            recommendations.append({
                'priority': 'critical',
                'category': 'filtering',
                'title': 'ä¼˜åŒ–è·¯å¾„è¿‡æ»¤ç­–ç•¥',
                'description': 'æ’é™¤æ›´å¤šä¸å¿…è¦çš„æ–‡ä»¶å’Œç›®å½•ä»¥å‡å°‘äº‹ä»¶å™ªéŸ³',
                'implementation': 'æ·»åŠ æ›´å¤šæ’é™¤æ¨¡å¼ï¼Œå¦‚ä¸´æ—¶æ–‡ä»¶ã€ç¼“å­˜ç›®å½•ã€ç³»ç»Ÿæ–‡ä»¶ç­‰'
            })
        
        if 'excessive_io' in issues:
            recommendations.append({
                'priority': 'high',
                'category': 'algorithm',
                'title': 'ä¼˜åŒ–FSEventså¤„ç†é€»è¾‘',
                'description': 'æ”¹è¿›äº‹ä»¶å¤„ç†ç®—æ³•ï¼Œå‡å°‘ä¸å¿…è¦çš„æ–‡ä»¶ç³»ç»Ÿè®¿é—®',
                'implementation': 'å»¶è¿Ÿæ–‡ä»¶å¤§å°æ£€æŸ¥ï¼Œä¼˜åŒ–è·¯å¾„åŒ¹é…ç®—æ³•ï¼Œå®æ–½äº‹ä»¶èšåˆ'
            })
        
        recommendations.append({
            'priority': 'medium',
            'category': 'monitoring',
            'title': 'å®æ–½ç›‘æ§ç›®å½•é¢„åˆ†æ',
            'description': 'åœ¨å¯åŠ¨æ—¶åˆ†æç›‘æ§ç›®å½•ï¼Œè¯†åˆ«å’Œæ’é™¤é«˜é¢‘å˜åŒ–çš„è·¯å¾„',
            'implementation': 'æ·»åŠ ç›®å½•é¢„æ‰«æé€»è¾‘ï¼Œè‡ªåŠ¨æ£€æµ‹å’Œæ’é™¤ç¼“å­˜ã€æ—¥å¿—ç­‰ç›®å½•'
        })
        
        recommendations.append({
            'priority': 'medium',
            'category': 'configuration',
            'title': 'å¯ç”¨æ™ºèƒ½ç›‘æ§æ¨¡å¼',
            'description': 'æ ¹æ®ç›®å½•ç‰¹å¾è‡ªåŠ¨è°ƒæ•´ç›‘æ§ç­–ç•¥',
            'implementation': 'å¯¹å¤§ç›®å½•ä½¿ç”¨æ›´é•¿çš„å»é‡æ—¶é—´ï¼Œå¯¹å°ç›®å½•ä¿æŒå¿«é€Ÿå“åº”'
        })
        
        analysis['recommendations'] = recommendations
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        # åˆ†æç›‘æ§ç›®å½•
        dir_analysis = self.analyze_monitored_directories()
        
        # åˆ†ææ€§èƒ½é—®é¢˜
        perf_analysis = self.analyze_performance_issues()
        
        report = []
        report.append("# Filesystemè¿æ¥å™¨æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ç›‘æ§ç›®å½•æ¦‚å†µ
        report.append("## ç›‘æ§ç›®å½•æ¦‚å†µ")
        if 'error' not in dir_analysis:
            report.append(f"- æ€»æ–‡ä»¶æ•°: {dir_analysis['total_files']:,}")
            report.append(f"- æ€»å¤§å°: {dir_analysis['total_size'] / (1024*1024*1024):.2f} GB")
            report.append("- æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
            for ext, count in sorted(dir_analysis['file_types'].items(), key=lambda x: x[1], reverse=True)[:10]:
                report.append(f"  - {ext or 'æ— æ‰©å±•å'}: {count} ä¸ªæ–‡ä»¶")
        else:
            report.append(f"âŒ åˆ†æå¤±è´¥: {dir_analysis['error']}")
        report.append("")
        
        # æ€§èƒ½é—®é¢˜æ€»ç»“
        if 'error' not in perf_analysis:
            summary = perf_analysis['performance_summary']
            report.append("## æ€§èƒ½æŒ‡æ ‡æ€»ç»“")
            report.append(f"- å¹³å‡CPUä½¿ç”¨ç‡: {summary['avg_cpu_percent']:.1f}%")
            report.append(f"- å³°å€¼CPUä½¿ç”¨ç‡: {summary['max_cpu_percent']:.1f}%")
            report.append(f"- å¹³å‡å†…å­˜ä½¿ç”¨: {summary['avg_memory_mb']:.1f} MB")
            report.append(f"- å³°å€¼å†…å­˜ä½¿ç”¨: {summary['max_memory_mb']:.1f} MB")
            report.append(f"- I/Oè¯»å–æ“ä½œ: {summary['io_read_operations']:,}")
            report.append(f"- å¹³å‡çº¿ç¨‹æ•°: {summary['avg_threads']:.1f}")
            report.append(f"- æœ€å¤§æ–‡ä»¶æè¿°ç¬¦: {summary['max_file_descriptors']}")
            report.append("")
            
            # é—®é¢˜è¯†åˆ«
            if perf_analysis['issues_identified']:
                report.append("## ğŸš¨ è¯†åˆ«çš„æ€§èƒ½é—®é¢˜")
                for issue in perf_analysis['issues_identified']:
                    severity_emoji = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡"}.get(issue['severity'], "âšª")
                    report.append(f"### {severity_emoji} {issue['description']}")
                    report.append(f"**ä¸¥é‡çº§åˆ«**: {issue['severity']}")
                    report.append(f"**å½±å“**: {issue['impact']}")
                    report.append("")
            
            # ä¼˜åŒ–å»ºè®®
            if perf_analysis['recommendations']:
                report.append("## ğŸ’¡ ä¼˜åŒ–å»ºè®®")
                for i, rec in enumerate(perf_analysis['recommendations'], 1):
                    priority_emoji = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(rec['priority'], "âšª")
                    report.append(f"### {priority_emoji} {i}. {rec['title']}")
                    report.append(f"**ä¼˜å…ˆçº§**: {rec['priority']}")
                    report.append(f"**ç±»åˆ«**: {rec['category']}")
                    report.append(f"**æè¿°**: {rec['description']}")
                    report.append(f"**å®æ–½æ–¹æ¡ˆ**: {rec['implementation']}")
                    report.append("")
        else:
            report.append(f"âŒ æ€§èƒ½åˆ†æå¤±è´¥: {perf_analysis['error']}")
        
        # FSäº‹ä»¶ç»Ÿè®¡
        if self.fs_events:
            report.append("## æ–‡ä»¶ç³»ç»Ÿäº‹ä»¶ç»Ÿè®¡")
            report.append(f"- ç›‘æ§æœŸé—´æ•è·äº‹ä»¶: {len(self.fs_events)}")
            recent_events = [e for e in self.fs_events if e['timestamp'] > datetime.now() - timedelta(minutes=5)]
            report.append(f"- æœ€è¿‘5åˆ†é’Ÿäº‹ä»¶: {len(recent_events)}")
            if recent_events:
                report.append("- æœ€è¿‘äº‹ä»¶æ ·ä¾‹:")
                for event in list(recent_events)[-5:]:
                    report.append(f"  - {event['timestamp'].strftime('%H:%M:%S')}: {event['event'][:100]}...")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Filesystemè¿æ¥å™¨æ€§èƒ½åˆ†æå·¥å…·")
    parser.add_argument("--duration", "-d", type=int, default=60, help="ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--process-name", default="linch-mind-filesystem", help="è¿æ¥å™¨è¿›ç¨‹åç§°")
    
    args = parser.parse_args()
    
    analyzer = PerformanceAnalyzer(args.process_name)
    
    # å¤„ç†Ctrl+C
    def signal_handler(signum, frame):
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ç›‘æ§...")
        analyzer.monitoring = False
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        # å¼€å§‹ç›‘æ§
        analyzer.start_monitoring(args.duration)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_report()
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(report)
            
    except KeyboardInterrupt:
        print("\nç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()