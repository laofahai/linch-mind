#!/usr/bin/env python3
"""
ç»Ÿä¸€æŒä¹…åŒ–æœåŠ¡ - æ¶ˆé™¤9ä¸ªé‡å¤save/loadå®ç°
æ•´åˆJSONã€Pickleã€åŠ å¯†å­˜å‚¨ä¸ºç»Ÿä¸€æ¥å£
"""

import asyncio
import json
import logging
import pickle
import shutil
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable
import tempfile

logger = logging.getLogger(__name__)


class StorageFormat(Enum):
    """å­˜å‚¨æ ¼å¼æšä¸¾"""
    JSON = "json"
    PICKLE = "pickle"
    TEXT = "text"
    BINARY = "binary"
    ENCRYPTED = "encrypted"


class CompressionType(Enum):
    """å‹ç¼©ç±»å‹æšä¸¾"""
    NONE = "none"
    GZIP = "gzip"
    BZIP2 = "bzip2"
    LZMA = "lzma"


@dataclass
class StorageMetadata:
    """å­˜å‚¨å…ƒæ•°æ®"""
    key: str
    format: StorageFormat
    compression: CompressionType = CompressionType.NONE
    encrypted: bool = False
    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)
    size_bytes: int = 0
    checksum: Optional[str] = None
    backup_count: int = 0


@dataclass
class PersistenceStats:
    """æŒä¹…åŒ–ç»Ÿè®¡ä¿¡æ¯"""
    total_saves: int = 0
    total_loads: int = 0
    total_backups: int = 0
    failed_operations: int = 0
    data_size_bytes: int = 0
    last_operation: Optional[datetime] = None


class UnifiedPersistenceService:
    """ç»Ÿä¸€æŒä¹…åŒ–æœåŠ¡ - æ¶ˆé™¤9ä¸ªé‡å¤save/loadå®ç°"""
    
    def __init__(
        self,
        base_dir: Optional[Path] = None,
        enable_backup: bool = True,
        max_backups: int = 5,
        enable_compression: bool = False,
        default_format: StorageFormat = StorageFormat.JSON,
        enable_checksum: bool = True
    ):
        # åŸºç¡€é…ç½®
        self.base_dir = Path(base_dir) if base_dir else self._get_default_base_dir()
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        self.enable_backup = enable_backup
        self.max_backups = max_backups
        self.enable_compression = enable_compression
        self.default_format = default_format
        self.enable_checksum = enable_checksum
        
        # ç›®å½•ç»“æ„
        self.data_dir = self.base_dir / "data"
        self.backup_dir = self.base_dir / "backups"
        self.metadata_dir = self.base_dir / "metadata"
        
        for dir_path in [self.data_dir, self.backup_dir, self.metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = PersistenceStats()
        
        # å…ƒæ•°æ®ç¼“å­˜
        self._metadata_cache: Dict[str, StorageMetadata] = {}
        
        # åºåˆ—åŒ–å™¨å’Œå‹ç¼©å™¨
        self._serializers = {
            StorageFormat.JSON: self._serialize_json,
            StorageFormat.PICKLE: self._serialize_pickle,
            StorageFormat.TEXT: self._serialize_text,
            StorageFormat.BINARY: self._serialize_binary,
            StorageFormat.ENCRYPTED: self._serialize_encrypted,
        }
        
        self._deserializers = {
            StorageFormat.JSON: self._deserialize_json,
            StorageFormat.PICKLE: self._deserialize_pickle,
            StorageFormat.TEXT: self._deserialize_text,
            StorageFormat.BINARY: self._deserialize_binary,
            StorageFormat.ENCRYPTED: self._deserialize_encrypted,
        }
        
        self._compressors = {
            CompressionType.NONE: lambda x: x,
            CompressionType.GZIP: self._compress_gzip,
            CompressionType.BZIP2: self._compress_bzip2,
            CompressionType.LZMA: self._compress_lzma,
        }
        
        self._decompressors = {
            CompressionType.NONE: lambda x: x,
            CompressionType.GZIP: self._decompress_gzip,
            CompressionType.BZIP2: self._decompress_bzip2,
            CompressionType.LZMA: self._decompress_lzma,
        }
        
        logger.info(f"ğŸ’¾ UnifiedPersistenceService åˆå§‹åŒ– (ç›®å½•: {self.base_dir})")

    async def save(
        self,
        key: str,
        data: Any,
        format: Optional[StorageFormat] = None,
        compression: Optional[CompressionType] = None,
        encrypted: bool = False,
        create_backup: Optional[bool] = None
    ) -> bool:
        """ç»Ÿä¸€ä¿å­˜æ¥å£ - æ›¿ä»£9ä¸ªé‡å¤saveå®ç°"""
        try:
            if format is None:
                format = self.default_format
            
            if compression is None:
                compression = CompressionType.GZIP if self.enable_compression else CompressionType.NONE
            
            if create_backup is None:
                create_backup = self.enable_backup
            
            # æ ‡å‡†åŒ–é”®
            normalized_key = self._normalize_key(key)
            
            # åˆ›å»ºå¤‡ä»½
            if create_backup:
                await self._create_backup(normalized_key)
            
            # åºåˆ—åŒ–æ•°æ®
            serialized_data = await self._serialize_data(data, format)
            
            # å‹ç¼©æ•°æ®
            compressed_data = await self._compress_data(serialized_data, compression)
            
            # åŠ å¯†æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if encrypted:
                compressed_data = await self._encrypt_data(compressed_data)
            
            # è®¡ç®—æ ¡éªŒå’Œ
            checksum = None
            if self.enable_checksum:
                checksum = self._calculate_checksum(compressed_data)
            
            # åŸå­å†™å…¥
            success = await self._atomic_write(normalized_key, compressed_data)
            
            if success:
                # æ›´æ–°å…ƒæ•°æ®
                metadata = StorageMetadata(
                    key=normalized_key,
                    format=format,
                    compression=compression,
                    encrypted=encrypted,
                    size_bytes=len(compressed_data),
                    checksum=checksum,
                    updated_at=datetime.utcnow()
                )
                
                await self._save_metadata(normalized_key, metadata)
                self._metadata_cache[normalized_key] = metadata
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats.total_saves += 1
                self.stats.data_size_bytes += len(compressed_data)
                self.stats.last_operation = datetime.utcnow()
                
                logger.debug(f"ä¿å­˜æˆåŠŸ [{format.value}]: {key}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥ [{key}]: {e}")
            self.stats.failed_operations += 1
            return False

    async def load(
        self,
        key: str,
        default: Any = None,
        verify_checksum: bool = True
    ) -> Any:
        """ç»Ÿä¸€åŠ è½½æ¥å£ - æ›¿ä»£9ä¸ªé‡å¤loadå®ç°"""
        try:
            normalized_key = self._normalize_key(key)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            data_file = self._get_data_file_path(normalized_key)
            if not data_file.exists():
                logger.debug(f"æ–‡ä»¶ä¸å­˜åœ¨: {key}")
                return default
            
            # åŠ è½½å…ƒæ•°æ®
            metadata = await self._load_metadata(normalized_key)
            if not metadata:
                logger.warning(f"å…ƒæ•°æ®ç¼ºå¤±ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼: {key}")
                metadata = await self._auto_detect_format(normalized_key)
            
            # è¯»å–æ•°æ®
            with open(data_file, 'rb') as f:
                raw_data = f.read()
            
            # æ ¡éªŒæ•°æ®å®Œæ•´æ€§
            if verify_checksum and metadata.checksum and self.enable_checksum:
                current_checksum = self._calculate_checksum(raw_data)
                if current_checksum != metadata.checksum:
                    logger.error(f"æ•°æ®å®Œæ•´æ€§æ ¡éªŒå¤±è´¥: {key}")
                    return default
            
            # è§£å¯†æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if metadata.encrypted:
                raw_data = await self._decrypt_data(raw_data)
            
            # è§£å‹æ•°æ®
            decompressed_data = await self._decompress_data(raw_data, metadata.compression)
            
            # ååºåˆ—åŒ–æ•°æ®
            data = await self._deserialize_data(decompressed_data, metadata.format)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats.total_loads += 1
            self.stats.last_operation = datetime.utcnow()
            
            logger.debug(f"åŠ è½½æˆåŠŸ [{metadata.format.value}]: {key}")
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥ [{key}]: {e}")
            self.stats.failed_operations += 1
            return default

    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        normalized_key = self._normalize_key(key)
        data_file = self._get_data_file_path(normalized_key)
        return data_file.exists()

    async def delete(self, key: str, create_backup: bool = True) -> bool:
        """åˆ é™¤æ•°æ®"""
        try:
            normalized_key = self._normalize_key(key)
            
            if create_backup:
                await self._create_backup(normalized_key)
            
            # åˆ é™¤æ•°æ®æ–‡ä»¶
            data_file = self._get_data_file_path(normalized_key)
            if data_file.exists():
                data_file.unlink()
            
            # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
            metadata_file = self._get_metadata_file_path(normalized_key)
            if metadata_file.exists():
                metadata_file.unlink()
            
            # æ¸…é™¤ç¼“å­˜
            self._metadata_cache.pop(normalized_key, None)
            
            logger.debug(f"åˆ é™¤æˆåŠŸ: {key}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤å¤±è´¥ [{key}]: {e}")
            return False

    async def list_keys(self, pattern: Optional[str] = None) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        try:
            keys = []
            for data_file in self.data_dir.glob("*"):
                if data_file.is_file():
                    key = data_file.stem  # ç§»é™¤æ‰©å±•å
                    
                    if pattern:
                        import re
                        if not re.search(pattern, key):
                            continue
                    
                    keys.append(key)
            
            return sorted(keys)
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºé”®å¤±è´¥: {e}")
            return []

    # === å¤‡ä»½å’Œæ¢å¤ ===
    
    async def create_backup(self, key: str) -> Optional[str]:
        """åˆ›å»ºå¤‡ä»½"""
        return await self._create_backup(self._normalize_key(key))
    
    async def restore_from_backup(self, key: str, backup_timestamp: Optional[str] = None) -> bool:
        """ä»å¤‡ä»½æ¢å¤"""
        try:
            normalized_key = self._normalize_key(key)
            
            # æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶
            if backup_timestamp:
                backup_file = self.backup_dir / f"{normalized_key}_{backup_timestamp}.bak"
            else:
                # ä½¿ç”¨æœ€æ–°å¤‡ä»½
                backup_files = list(self.backup_dir.glob(f"{normalized_key}_*.bak"))
                if not backup_files:
                    logger.error(f"æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶: {key}")
                    return False
                
                backup_file = max(backup_files, key=lambda f: f.stat().st_mtime)
            
            if not backup_file.exists():
                logger.error(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
                return False
            
            # æ¢å¤æ•°æ®æ–‡ä»¶
            data_file = self._get_data_file_path(normalized_key)
            shutil.copy2(backup_file, data_file)
            
            logger.info(f"ä»å¤‡ä»½æ¢å¤æˆåŠŸ: {key}")
            return True
            
        except Exception as e:
            logger.error(f"ä»å¤‡ä»½æ¢å¤å¤±è´¥ [{key}]: {e}")
            return False

    # === æ‰¹é‡æ“ä½œ ===
    
    async def batch_save(self, items: Dict[str, Any], **kwargs) -> Dict[str, bool]:
        """æ‰¹é‡ä¿å­˜"""
        results = {}
        for key, data in items.items():
            results[key] = await self.save(key, data, **kwargs)
        return results
    
    async def batch_load(self, keys: List[str], **kwargs) -> Dict[str, Any]:
        """æ‰¹é‡åŠ è½½"""
        results = {}
        for key in keys:
            results[key] = await self.load(key, **kwargs)
        return results

    # === å†…éƒ¨æ–¹æ³• ===
    
    def _get_default_base_dir(self) -> Path:
        """è·å–é»˜è®¤åŸºç¡€ç›®å½•"""
        try:
            from core.environment_manager import get_environment_manager
            env_manager = get_environment_manager()
            return env_manager.get_data_dir() / "persistence"
        except:
            return Path.home() / ".linch-mind" / "persistence"
    
    def _normalize_key(self, key: str) -> str:
        """æ ‡å‡†åŒ–é”®"""
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä»¥åˆ›å»ºæœ‰æ•ˆçš„æ–‡ä»¶å
        import re
        return re.sub(r'[^\w\-_.]', '_', key)
    
    def _get_data_file_path(self, key: str) -> Path:
        """è·å–æ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.data_dir / f"{key}.dat"
    
    def _get_metadata_file_path(self, key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.metadata_dir / f"{key}.meta"
    
    async def _create_backup(self, key: str) -> Optional[str]:
        """åˆ›å»ºå¤‡ä»½"""
        try:
            data_file = self._get_data_file_path(key)
            if not data_file.exists():
                return None
            
            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"{key}_{timestamp}.bak"
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(data_file, backup_file)
            
            # æ¸…ç†æ—§å¤‡ä»½
            await self._cleanup_old_backups(key)
            
            self.stats.total_backups += 1
            logger.debug(f"åˆ›å»ºå¤‡ä»½: {backup_file}")
            
            return timestamp
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥ [{key}]: {e}")
            return None
    
    async def _cleanup_old_backups(self, key: str):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            backup_files = sorted(
                self.backup_dir.glob(f"{key}_*.bak"),
                key=lambda f: f.stat().st_mtime,
                reverse=True
            )
            
            # ä¿ç•™æœ€æ–°çš„Nä¸ªå¤‡ä»½
            for backup_file in backup_files[self.max_backups:]:
                backup_file.unlink()
                
        except Exception as e:
            logger.error(f"æ¸…ç†å¤‡ä»½å¤±è´¥ [{key}]: {e}")
    
    async def _atomic_write(self, key: str, data: bytes) -> bool:
        """åŸå­å†™å…¥"""
        try:
            data_file = self._get_data_file_path(key)
            
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿åŸå­æ“ä½œ
            with tempfile.NamedTemporaryFile(
                dir=self.data_dir,
                delete=False,
                suffix=".tmp"
            ) as tmp_file:
                tmp_file.write(data)
                tmp_file.flush()
                tmp_path = Path(tmp_file.name)
            
            # åŸå­ç§»åŠ¨
            tmp_path.rename(data_file)
            return True
            
        except Exception as e:
            logger.error(f"åŸå­å†™å…¥å¤±è´¥ [{key}]: {e}")
            return False
    
    async def _save_metadata(self, key: str, metadata: StorageMetadata):
        """ä¿å­˜å…ƒæ•°æ®"""
        try:
            metadata_file = self._get_metadata_file_path(key)
            metadata_dict = {
                "key": metadata.key,
                "format": metadata.format.value,
                "compression": metadata.compression.value,
                "encrypted": metadata.encrypted,
                "created_at": metadata.created_at.isoformat(),
                "updated_at": metadata.updated_at.isoformat(),
                "size_bytes": metadata.size_bytes,
                "checksum": metadata.checksum,
                "backup_count": metadata.backup_count
            }
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_dict, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥ [{key}]: {e}")
    
    async def _load_metadata(self, key: str) -> Optional[StorageMetadata]:
        """åŠ è½½å…ƒæ•°æ®"""
        try:
            if key in self._metadata_cache:
                return self._metadata_cache[key]
            
            metadata_file = self._get_metadata_file_path(key)
            if not metadata_file.exists():
                return None
            
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata_dict = json.load(f)
            
            metadata = StorageMetadata(
                key=metadata_dict["key"],
                format=StorageFormat(metadata_dict["format"]),
                compression=CompressionType(metadata_dict["compression"]),
                encrypted=metadata_dict["encrypted"],
                created_at=datetime.fromisoformat(metadata_dict["created_at"]),
                updated_at=datetime.fromisoformat(metadata_dict["updated_at"]),
                size_bytes=metadata_dict["size_bytes"],
                checksum=metadata_dict.get("checksum"),
                backup_count=metadata_dict.get("backup_count", 0)
            )
            
            self._metadata_cache[key] = metadata
            return metadata
            
        except Exception as e:
            logger.error(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥ [{key}]: {e}")
            return None
    
    async def _auto_detect_format(self, key: str) -> StorageMetadata:
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®æ ¼å¼"""
        try:
            data_file = self._get_data_file_path(key)
            with open(data_file, 'rb') as f:
                first_bytes = f.read(16)
            
            # ç®€å•çš„æ ¼å¼æ£€æµ‹
            if first_bytes.startswith(b'{') or first_bytes.startswith(b'['):
                format = StorageFormat.JSON
            elif first_bytes.startswith(b'\x80\x03') or first_bytes.startswith(b'\x80\x04'):
                format = StorageFormat.PICKLE
            else:
                format = StorageFormat.BINARY
            
            return StorageMetadata(
                key=key,
                format=format,
                compression=CompressionType.NONE,
                encrypted=False
            )
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¤±è´¥ [{key}]: {e}")
            # é»˜è®¤æ ¼å¼
            return StorageMetadata(
                key=key,
                format=self.default_format,
                compression=CompressionType.NONE,
                encrypted=False
            )
    
    # === åºåˆ—åŒ–å™¨ ===
    
    async def _serialize_data(self, data: Any, format: StorageFormat) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._serializers[format], data)
    
    async def _deserialize_data(self, data: bytes, format: StorageFormat) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._deserializers[format], data)
    
    def _serialize_json(self, data: Any) -> bytes:
        """JSONåºåˆ—åŒ–"""
        return json.dumps(data, ensure_ascii=False, default=str).encode('utf-8')
    
    def _deserialize_json(self, data: bytes) -> Any:
        """JSONååºåˆ—åŒ–"""
        return json.loads(data.decode('utf-8'))
    
    def _serialize_pickle(self, data: Any) -> bytes:
        """Pickleåºåˆ—åŒ–"""
        return pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
    
    def _deserialize_pickle(self, data: bytes) -> Any:
        """Pickleååºåˆ—åŒ–"""
        return pickle.loads(data)
    
    def _serialize_text(self, data: Any) -> bytes:
        """æ–‡æœ¬åºåˆ—åŒ–"""
        return str(data).encode('utf-8')
    
    def _deserialize_text(self, data: bytes) -> str:
        """æ–‡æœ¬ååºåˆ—åŒ–"""
        return data.decode('utf-8')
    
    def _serialize_binary(self, data: Any) -> bytes:
        """äºŒè¿›åˆ¶åºåˆ—åŒ–"""
        if isinstance(data, bytes):
            return data
        elif isinstance(data, str):
            return data.encode('utf-8')
        else:
            return pickle.dumps(data)
    
    def _deserialize_binary(self, data: bytes) -> Any:
        """äºŒè¿›åˆ¶ååºåˆ—åŒ–"""
        try:
            return pickle.loads(data)
        except:
            return data
    
    def _serialize_encrypted(self, data: Any) -> bytes:
        """åŠ å¯†åºåˆ—åŒ–"""
        # å…ˆJSONåºåˆ—åŒ–ï¼Œç„¶ååŠ å¯†
        json_data = self._serialize_json(data)
        return self._encrypt_data_sync(json_data)
    
    def _deserialize_encrypted(self, data: bytes) -> Any:
        """åŠ å¯†ååºåˆ—åŒ–"""
        # å…ˆè§£å¯†ï¼Œç„¶åJSONååºåˆ—åŒ–
        decrypted_data = self._decrypt_data_sync(data)
        return self._deserialize_json(decrypted_data)
    
    # === å‹ç¼©å™¨ ===
    
    async def _compress_data(self, data: bytes, compression: CompressionType) -> bytes:
        """å‹ç¼©æ•°æ®"""
        if compression == CompressionType.NONE:
            return data
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._compressors[compression], data)
    
    async def _decompress_data(self, data: bytes, compression: CompressionType) -> bytes:
        """è§£å‹æ•°æ®"""
        if compression == CompressionType.NONE:
            return data
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._decompressors[compression], data)
    
    def _compress_gzip(self, data: bytes) -> bytes:
        """GZIPå‹ç¼©"""
        import gzip
        return gzip.compress(data)
    
    def _decompress_gzip(self, data: bytes) -> bytes:
        """GZIPè§£å‹"""
        import gzip
        return gzip.decompress(data)
    
    def _compress_bzip2(self, data: bytes) -> bytes:
        """BZIP2å‹ç¼©"""
        import bz2
        return bz2.compress(data)
    
    def _decompress_bzip2(self, data: bytes) -> bytes:
        """BZIP2è§£å‹"""
        import bz2
        return bz2.decompress(data)
    
    def _compress_lzma(self, data: bytes) -> bytes:
        """LZMAå‹ç¼©"""
        import lzma
        return lzma.compress(data)
    
    def _decompress_lzma(self, data: bytes) -> bytes:
        """LZMAè§£å‹"""
        import lzma
        return lzma.decompress(data)
    
    # === åŠ å¯†å™¨ ===
    
    async def _encrypt_data(self, data: bytes) -> bytes:
        """å¼‚æ­¥åŠ å¯†æ•°æ®"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._encrypt_data_sync, data)
    
    async def _decrypt_data(self, data: bytes) -> bytes:
        """å¼‚æ­¥è§£å¯†æ•°æ®"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._decrypt_data_sync, data)
    
    def _encrypt_data_sync(self, data: bytes) -> bytes:
        """åŒæ­¥åŠ å¯†æ•°æ®"""
        try:
            from services.security.selective_encrypted_storage import get_selective_encrypted_storage
            storage = get_selective_encrypted_storage()
            return storage.encrypt_data(data)
        except Exception as e:
            logger.warning(f"åŠ å¯†å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®: {e}")
            return data
    
    def _decrypt_data_sync(self, data: bytes) -> bytes:
        """åŒæ­¥è§£å¯†æ•°æ®"""
        try:
            from services.security.selective_encrypted_storage import get_selective_encrypted_storage
            storage = get_selective_encrypted_storage()
            return storage.decrypt_data(data)
        except Exception as e:
            logger.warning(f"è§£å¯†å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®: {e}")
            return data
    
    def _calculate_checksum(self, data: bytes) -> str:
        """è®¡ç®—æ ¡éªŒå’Œ"""
        import hashlib
        return hashlib.sha256(data).hexdigest()
    
    def get_stats(self) -> PersistenceStats:
        """è·å–æŒä¹…åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°ç£ç›˜ä½¿ç”¨ç»Ÿè®¡
        try:
            total_size = 0
            for data_file in self.data_dir.glob("*.dat"):
                total_size += data_file.stat().st_size
            self.stats.data_size_bytes = total_size
        except Exception as e:
            logger.error(f"è®¡ç®—ç£ç›˜ä½¿ç”¨å¤±è´¥: {e}")
        
        return self.stats


# å…¨å±€å®ä¾‹ç®¡ç†
_unified_persistence_service: Optional[UnifiedPersistenceService] = None


def get_unified_persistence_service(
    base_dir: Optional[Path] = None,
    **kwargs
) -> UnifiedPersistenceService:
    """è·å–ç»Ÿä¸€æŒä¹…åŒ–æœåŠ¡å®ä¾‹"""
    global _unified_persistence_service
    
    if _unified_persistence_service is None:
        _unified_persistence_service = UnifiedPersistenceService(
            base_dir=base_dir,
            **kwargs
        )
    
    return _unified_persistence_service


async def cleanup_unified_persistence_service():
    """æ¸…ç†ç»Ÿä¸€æŒä¹…åŒ–æœåŠ¡"""
    global _unified_persistence_service
    
    if _unified_persistence_service:
        try:
            # æ‰§è¡Œä»»ä½•å¿…è¦çš„æ¸…ç†
            logger.info("ğŸ’¾ UnifiedPersistenceService å·²æ¸…ç†")
        except Exception as e:
            logger.error(f"æ¸…ç†UnifiedPersistenceServiceå¤±è´¥: {e}")
        finally:
            _unified_persistence_service = None