#!/usr/bin/env python3
"""
å­˜å‚¨æ€§èƒ½ç›‘æ§ç³»ç»Ÿ - æ™ºèƒ½å­˜å‚¨æ¶æ„çš„å…¨é¢ç›‘æ§
ç›‘æ§å‹ç¼©æ¯”ã€AIå‡†ç¡®æ€§ã€å­˜å‚¨ä½¿ç”¨æƒ…å†µå’Œç³»ç»Ÿæ€§èƒ½
"""

import asyncio
import json
import logging
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import psutil

from core.error_handling import handle_errors, ErrorSeverity, ErrorCategory
from core.service_facade import get_service
from config.intelligent_storage import get_intelligent_storage_config
from services.ai.ollama_service import get_ollama_service
from services.storage.faiss_vector_store import get_faiss_vector_store
from services.storage.intelligent_event_processor import get_intelligent_event_processor
from services.storage.data_lifecycle_manager import get_data_lifecycle_manager
from services.storage.core.database import UnifiedDatabaseService
from models.database_models import EntityMetadata

logger = logging.getLogger(__name__)


@dataclass
class StorageMetrics:
    """å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    timestamp: datetime
    total_documents: int
    total_shards: int
    storage_size_mb: float
    memory_usage_mb: float
    
    # å‹ç¼©æŒ‡æ ‡
    compression_ratio: float
    storage_efficiency: float
    
    # AIæŒ‡æ ‡
    ai_accuracy_rate: float
    content_acceptance_rate: float
    avg_value_score: float
    
    # æ€§èƒ½æŒ‡æ ‡
    avg_processing_time_ms: float
    avg_search_time_ms: float
    avg_embedding_time_ms: float
    
    # åˆ†å±‚å­˜å‚¨æŒ‡æ ‡
    hot_data_percentage: float
    warm_data_percentage: float
    cold_data_percentage: float
    
    # ç³»ç»Ÿå¥åº·æŒ‡æ ‡
    ollama_status: str
    vector_store_status: str
    database_status: str
    
    # é”™è¯¯å’Œè­¦å‘Š
    error_count_24h: int
    warning_count_24h: int


@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    rule_id: str
    metric_name: str
    threshold: float
    operator: str  # gt/lt/eq
    severity: str  # critical/warning/info
    description: str
    enabled: bool = True


@dataclass
class Alert:
    """å‘Šè­¦äº‹ä»¶"""
    alert_id: str
    rule_id: str
    metric_name: str
    current_value: float
    threshold: float
    severity: str
    description: str
    timestamp: datetime
    resolved: bool = False
    resolved_at: Optional[datetime] = None


class StorageMetricsCollector:
    """å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.config = get_intelligent_storage_config()
        
        # æœåŠ¡ä¾èµ–
        self._ollama_service = None
        self._vector_store = None
        self._intelligent_processor = None
        self._lifecycle_manager = None
        self._db_service = None
        
        # æŒ‡æ ‡å†å²
        self._metrics_history: List[StorageMetrics] = []
        self._max_history = 1000  # ä¿ç•™æœ€è¿‘1000ä¸ªæŒ‡æ ‡ç‚¹
        
        # å‘Šè­¦ç³»ç»Ÿ
        self._alert_rules: List[AlertRule] = []
        self._active_alerts: List[Alert] = []
        
        # æ€§èƒ½ç›‘æ§
        self._is_collecting = False
        self._collection_interval = 60  # 60ç§’æ”¶é›†ä¸€æ¬¡
        
        # æ–‡ä»¶è·¯å¾„
        self.metrics_dir = self.config.storage_dir / "metrics"
        self.metrics_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å‘Šè­¦è§„åˆ™
        self._setup_default_alert_rules()

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨"""
        try:
            # è·å–æœåŠ¡ä¾èµ–
            self._ollama_service = await get_ollama_service()
            self._vector_store = await get_faiss_vector_store()
            self._intelligent_processor = await get_intelligent_event_processor()
            self._lifecycle_manager = await get_data_lifecycle_manager()
            self._db_service = get_service(UnifiedDatabaseService)
            
            # åŠ è½½å†å²æŒ‡æ ‡
            await self._load_metrics_history()
            
            # å¯åŠ¨åå°æ”¶é›†
            if self.config.monitoring.enable_metrics:
                await self._start_metrics_collection()
            
            logger.info("å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def close(self):
        """å…³é—­æŒ‡æ ‡æ”¶é›†å™¨"""
        try:
            self._is_collecting = False
            await self._save_metrics_history()
            logger.info("å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­æŒ‡æ ‡æ”¶é›†å™¨å¤±è´¥: {e}")

    # === æŒ‡æ ‡æ”¶é›† ===

    async def collect_current_metrics(self) -> StorageMetrics:
        """æ”¶é›†å½“å‰æŒ‡æ ‡"""
        try:
            # å¹¶è¡Œæ”¶é›†å„é¡¹æŒ‡æ ‡
            tasks = [
                self._collect_basic_metrics(),
                self._collect_compression_metrics(),
                self._collect_ai_metrics(),
                self._collect_performance_metrics(),
                self._collect_tier_metrics(),
                self._collect_health_metrics(),
                self._collect_error_metrics(),
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åˆå¹¶æŒ‡æ ‡
            metrics = StorageMetrics(
                timestamp=datetime.utcnow(),
                total_documents=0,
                total_shards=0,
                storage_size_mb=0.0,
                memory_usage_mb=0.0,
                compression_ratio=1.0,
                storage_efficiency=0.0,
                ai_accuracy_rate=0.0,
                content_acceptance_rate=0.0,
                avg_value_score=0.0,
                avg_processing_time_ms=0.0,
                avg_search_time_ms=0.0,
                avg_embedding_time_ms=0.0,
                hot_data_percentage=0.0,
                warm_data_percentage=0.0,
                cold_data_percentage=0.0,
                ollama_status="unknown",
                vector_store_status="unknown",
                database_status="unknown",
                error_count_24h=0,
                warning_count_24h=0,
            )
            
            # åˆå¹¶ç»“æœ
            for i, result in enumerate(results):
                if not isinstance(result, Exception):
                    if i == 0:  # åŸºç¡€æŒ‡æ ‡
                        metrics.total_documents = result.get("total_documents", 0)
                        metrics.total_shards = result.get("total_shards", 0)
                        metrics.storage_size_mb = result.get("storage_size_mb", 0.0)
                        metrics.memory_usage_mb = result.get("memory_usage_mb", 0.0)
                    elif i == 1:  # å‹ç¼©æŒ‡æ ‡
                        metrics.compression_ratio = result.get("compression_ratio", 1.0)
                        metrics.storage_efficiency = result.get("storage_efficiency", 0.0)
                    elif i == 2:  # AIæŒ‡æ ‡
                        metrics.ai_accuracy_rate = result.get("ai_accuracy_rate", 0.0)
                        metrics.content_acceptance_rate = result.get("content_acceptance_rate", 0.0)
                        metrics.avg_value_score = result.get("avg_value_score", 0.0)
                    elif i == 3:  # æ€§èƒ½æŒ‡æ ‡
                        metrics.avg_processing_time_ms = result.get("avg_processing_time_ms", 0.0)
                        metrics.avg_search_time_ms = result.get("avg_search_time_ms", 0.0)
                        metrics.avg_embedding_time_ms = result.get("avg_embedding_time_ms", 0.0)
                    elif i == 4:  # åˆ†å±‚æŒ‡æ ‡
                        metrics.hot_data_percentage = result.get("hot_data_percentage", 0.0)
                        metrics.warm_data_percentage = result.get("warm_data_percentage", 0.0)
                        metrics.cold_data_percentage = result.get("cold_data_percentage", 0.0)
                    elif i == 5:  # å¥åº·æŒ‡æ ‡
                        metrics.ollama_status = result.get("ollama_status", "unknown")
                        metrics.vector_store_status = result.get("vector_store_status", "unknown")
                        metrics.database_status = result.get("database_status", "unknown")
                    elif i == 6:  # é”™è¯¯æŒ‡æ ‡
                        metrics.error_count_24h = result.get("error_count_24h", 0)
                        metrics.warning_count_24h = result.get("warning_count_24h", 0)
            
            return metrics
            
        except Exception as e:
            logger.error(f"æ”¶é›†æŒ‡æ ‡å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æŒ‡æ ‡
            return StorageMetrics(
                timestamp=datetime.utcnow(),
                total_documents=0, total_shards=0, storage_size_mb=0.0,
                memory_usage_mb=0.0, compression_ratio=1.0, storage_efficiency=0.0,
                ai_accuracy_rate=0.0, content_acceptance_rate=0.0, avg_value_score=0.0,
                avg_processing_time_ms=0.0, avg_search_time_ms=0.0, avg_embedding_time_ms=0.0,
                hot_data_percentage=0.0, warm_data_percentage=0.0, cold_data_percentage=0.0,
                ollama_status="error", vector_store_status="error", database_status="error",
                error_count_24h=1, warning_count_24h=0,
            )

    async def _collect_basic_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†åŸºç¡€æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # å‘é‡å­˜å‚¨æŒ‡æ ‡
            if self._vector_store:
                vector_metrics = await self._vector_store.get_metrics()
                metrics["total_documents"] = vector_metrics.total_documents
                metrics["total_shards"] = vector_metrics.total_shards
                metrics["storage_size_mb"] = vector_metrics.storage_size_mb
            
            # å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            metrics["memory_usage_mb"] = process.memory_info().rss / 1024 / 1024
            
        except Exception as e:
            logger.error(f"æ”¶é›†åŸºç¡€æŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    async def _collect_compression_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†å‹ç¼©æŒ‡æ ‡"""
        metrics = {}
        
        try:
            if self._vector_store:
                vector_metrics = await self._vector_store.get_metrics()
                metrics["compression_ratio"] = vector_metrics.compression_ratio
                
                # è®¡ç®—å­˜å‚¨æ•ˆç‡
                if vector_metrics.total_documents > 0:
                    original_size = vector_metrics.total_documents * 384 * 4  # float32
                    compressed_size = vector_metrics.storage_size_mb * 1024 * 1024
                    metrics["storage_efficiency"] = original_size / compressed_size if compressed_size > 0 else 0
                else:
                    metrics["storage_efficiency"] = 0.0
        
        except Exception as e:
            logger.error(f"æ”¶é›†å‹ç¼©æŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    async def _collect_ai_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†AIæŒ‡æ ‡"""
        metrics = {}
        
        try:
            if self._intelligent_processor:
                processor_metrics = await self._intelligent_processor.get_metrics()
                
                metrics["content_acceptance_rate"] = processor_metrics.acceptance_rate
                metrics["avg_value_score"] = processor_metrics.avg_value_score
                
                # AIå‡†ç¡®æ€§è¯„ä¼°ï¼ˆåŸºäºæ¥å—ç‡å’Œå¹³å‡åˆ†æ•°ï¼‰
                accuracy = (processor_metrics.acceptance_rate + processor_metrics.avg_value_score) / 2
                metrics["ai_accuracy_rate"] = accuracy
        
        except Exception as e:
            logger.error(f"æ”¶é›†AIæŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    async def _collect_performance_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # æ™ºèƒ½å¤„ç†å™¨æ€§èƒ½
            if self._intelligent_processor:
                processor_metrics = await self._intelligent_processor.get_metrics()
                metrics["avg_processing_time_ms"] = processor_metrics.avg_processing_time_ms
            
            # å‘é‡æœç´¢æ€§èƒ½
            if self._vector_store:
                vector_metrics = await self._vector_store.get_metrics()
                metrics["avg_search_time_ms"] = vector_metrics.avg_search_time_ms
            
            # Ollamaæ€§èƒ½
            if self._ollama_service:
                ollama_metrics = await self._ollama_service.get_metrics()
                metrics["avg_embedding_time_ms"] = ollama_metrics.avg_embedding_time_ms
        
        except Exception as e:
            logger.error(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    async def _collect_tier_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†åˆ†å±‚å­˜å‚¨æŒ‡æ ‡"""
        metrics = {}
        
        try:
            if self._vector_store:
                vector_metrics = await self._vector_store.get_metrics()
                total_shards = vector_metrics.total_shards
                
                if total_shards > 0:
                    metrics["hot_data_percentage"] = (vector_metrics.hot_shards / total_shards) * 100
                    metrics["warm_data_percentage"] = (vector_metrics.warm_shards / total_shards) * 100
                    metrics["cold_data_percentage"] = (vector_metrics.cold_shards / total_shards) * 100
                else:
                    metrics["hot_data_percentage"] = 0.0
                    metrics["warm_data_percentage"] = 0.0
                    metrics["cold_data_percentage"] = 0.0
        
        except Exception as e:
            logger.error(f"æ”¶é›†åˆ†å±‚æŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    async def _collect_health_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†å¥åº·çŠ¶æ€æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # OllamaçŠ¶æ€
            if self._ollama_service:
                try:
                    await self._ollama_service.embed_text("health check")
                    metrics["ollama_status"] = "healthy"
                except:
                    metrics["ollama_status"] = "unhealthy"
            else:
                metrics["ollama_status"] = "unavailable"
            
            # å‘é‡å­˜å‚¨çŠ¶æ€
            if self._vector_store:
                try:
                    vector_metrics = await self._vector_store.get_metrics()
                    metrics["vector_store_status"] = "healthy" if vector_metrics.total_shards >= 0 else "unhealthy"
                except:
                    metrics["vector_store_status"] = "unhealthy"
            else:
                metrics["vector_store_status"] = "unavailable"
            
            # æ•°æ®åº“çŠ¶æ€
            if self._db_service:
                try:
                    with self._db_service.get_session() as session:
                        session.execute("SELECT 1")
                    metrics["database_status"] = "healthy"
                except:
                    metrics["database_status"] = "unhealthy"
            else:
                metrics["database_status"] = "unavailable"
        
        except Exception as e:
            logger.error(f"æ”¶é›†å¥åº·æŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    async def _collect_error_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†é”™è¯¯æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # ä»æ—¥å¿—æ–‡ä»¶ä¸­ç»Ÿè®¡é”™è¯¯
            log_dir = self.config.storage_dir / "logs"
            if log_dir.exists():
                cutoff_time = datetime.utcnow() - timedelta(hours=24)
                
                error_count = 0
                warning_count = 0
                
                for log_file in log_dir.glob("*.log"):
                    try:
                        if log_file.stat().st_mtime > cutoff_time.timestamp():
                            with open(log_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                error_count += content.count("ERROR")
                                warning_count += content.count("WARNING")
                    except:
                        pass
                
                metrics["error_count_24h"] = error_count
                metrics["warning_count_24h"] = warning_count
            else:
                metrics["error_count_24h"] = 0
                metrics["warning_count_24h"] = 0
        
        except Exception as e:
            logger.error(f"æ”¶é›†é”™è¯¯æŒ‡æ ‡å¤±è´¥: {e}")
        
        return metrics

    # === åå°æ”¶é›† ===

    async def _start_metrics_collection(self):
        """å¯åŠ¨åå°æŒ‡æ ‡æ”¶é›†"""
        self._is_collecting = True
        asyncio.create_task(self._metrics_collection_loop())
        logger.info("åå°æŒ‡æ ‡æ”¶é›†å·²å¯åŠ¨")

    async def _metrics_collection_loop(self):
        """æŒ‡æ ‡æ”¶é›†å¾ªç¯"""
        while self._is_collecting:
            try:
                # æ”¶é›†æŒ‡æ ‡
                metrics = await self.collect_current_metrics()
                
                # æ·»åŠ åˆ°å†å²è®°å½•
                self._metrics_history.append(metrics)
                
                # é™åˆ¶å†å²è®°å½•å¤§å°
                if len(self._metrics_history) > self._max_history:
                    self._metrics_history = self._metrics_history[-self._max_history:]
                
                # æ£€æŸ¥å‘Šè­¦
                await self._check_alerts(metrics)
                
                # å®šæœŸä¿å­˜
                if len(self._metrics_history) % 10 == 0:
                    await self._save_metrics_history()
                
                # ç­‰å¾…ä¸‹æ¬¡æ”¶é›†
                await asyncio.sleep(self._collection_interval)
                
            except Exception as e:
                logger.error(f"æŒ‡æ ‡æ”¶é›†å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(60)  # å¼‚å¸¸æ—¶ç­‰å¾…1åˆ†é’Ÿ

    # === å‘Šè­¦ç³»ç»Ÿ ===

    def _setup_default_alert_rules(self):
        """è®¾ç½®é»˜è®¤å‘Šè­¦è§„åˆ™"""
        self._alert_rules = [
            # å‹ç¼©æ¯”å‘Šè­¦
            AlertRule(
                rule_id="compression_ratio_low",
                metric_name="compression_ratio",
                threshold=self.config.monitoring.compression_ratio_threshold,
                operator="lt",
                severity="warning",
                description="å­˜å‚¨å‹ç¼©æ¯”è¿‡ä½",
            ),
            
            # AIå‡†ç¡®æ€§å‘Šè­¦
            AlertRule(
                rule_id="ai_accuracy_low",
                metric_name="ai_accuracy_rate",
                threshold=self.config.monitoring.ai_accuracy_threshold,
                operator="lt",
                severity="critical",
                description="AIè¯„ä¼°å‡†ç¡®æ€§è¿‡ä½",
            ),
            
            # å†…å­˜ä½¿ç”¨å‘Šè­¦
            AlertRule(
                rule_id="memory_usage_high",
                metric_name="memory_usage_mb",
                threshold=1024.0,  # 1GB
                operator="gt",
                severity="warning",
                description="å†…å­˜ä½¿ç”¨è¿‡é«˜",
            ),
            
            # é”™è¯¯ç‡å‘Šè­¦
            AlertRule(
                rule_id="error_rate_high",
                metric_name="error_count_24h",
                threshold=50.0,
                operator="gt",
                severity="critical",
                description="24å°æ—¶é”™è¯¯æ•°è¿‡å¤š",
            ),
            
            # æœåŠ¡å¥åº·å‘Šè­¦
            AlertRule(
                rule_id="ollama_unhealthy",
                metric_name="ollama_status",
                threshold=0.0,  # ç”¨äºå­—ç¬¦ä¸²æ¯”è¾ƒ
                operator="eq",
                severity="critical",
                description="OllamaæœåŠ¡ä¸å¥åº·",
            ),
        ]

    async def _check_alerts(self, metrics: StorageMetrics):
        """æ£€æŸ¥å‘Šè­¦"""
        if not self.config.monitoring.performance_alerts:
            return
        
        current_time = datetime.utcnow()
        
        for rule in self._alert_rules:
            if not rule.enabled:
                continue
            
            try:
                # è·å–æŒ‡æ ‡å€¼
                metric_value = getattr(metrics, rule.metric_name, None)
                if metric_value is None:
                    continue
                
                # å­—ç¬¦ä¸²ç±»å‹ç‰¹æ®Šå¤„ç†
                if isinstance(metric_value, str):
                    if rule.operator == "eq" and metric_value == "unhealthy":
                        await self._trigger_alert(rule, metric_value, current_time)
                    continue
                
                # æ•°å€¼ç±»å‹æ¯”è¾ƒ
                triggered = False
                if rule.operator == "gt" and metric_value > rule.threshold:
                    triggered = True
                elif rule.operator == "lt" and metric_value < rule.threshold:
                    triggered = True
                elif rule.operator == "eq" and metric_value == rule.threshold:
                    triggered = True
                
                if triggered:
                    await self._trigger_alert(rule, metric_value, current_time)
                else:
                    await self._resolve_alert(rule.rule_id, current_time)
                    
            except Exception as e:
                logger.error(f"æ£€æŸ¥å‘Šè­¦è§„åˆ™å¤±è´¥ [{rule.rule_id}]: {e}")

    async def _trigger_alert(self, rule: AlertRule, current_value: Any, timestamp: datetime):
        """è§¦å‘å‘Šè­¦"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ´»è·ƒå‘Šè­¦
        existing_alert = next((a for a in self._active_alerts 
                             if a.rule_id == rule.rule_id and not a.resolved), None)
        
        if existing_alert:
            # æ›´æ–°ç°æœ‰å‘Šè­¦
            existing_alert.current_value = current_value
            existing_alert.timestamp = timestamp
        else:
            # åˆ›å»ºæ–°å‘Šè­¦
            alert = Alert(
                alert_id=f"{rule.rule_id}_{int(timestamp.timestamp())}",
                rule_id=rule.rule_id,
                metric_name=rule.metric_name,
                current_value=current_value,
                threshold=rule.threshold,
                severity=rule.severity,
                description=rule.description,
                timestamp=timestamp,
            )
            
            self._active_alerts.append(alert)
            logger.warning(f"ğŸš¨ è§¦å‘å‘Šè­¦: {rule.description} - å½“å‰å€¼: {current_value}, é˜ˆå€¼: {rule.threshold}")

    async def _resolve_alert(self, rule_id: str, timestamp: datetime):
        """è§£å†³å‘Šè­¦"""
        for alert in self._active_alerts:
            if alert.rule_id == rule_id and not alert.resolved:
                alert.resolved = True
                alert.resolved_at = timestamp
                logger.info(f"âœ… å‘Šè­¦å·²è§£å†³: {alert.description}")

    # === æ•°æ®æŒä¹…åŒ– ===

    async def _save_metrics_history(self):
        """ä¿å­˜æŒ‡æ ‡å†å²"""
        try:
            # æŒ‰æ—¥æœŸåˆ†ç»„ä¿å­˜
            today = datetime.utcnow().strftime("%Y-%m-%d")
            metrics_file = self.metrics_dir / f"metrics_{today}.json"
            
            # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            metrics_data = []
            for metric in self._metrics_history:
                metric_dict = asdict(metric)
                metric_dict["timestamp"] = metric.timestamp.isoformat()
                metrics_data.append(metric_dict)
            
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"ä¿å­˜æŒ‡æ ‡å†å²å¤±è´¥: {e}")

    async def _load_metrics_history(self):
        """åŠ è½½æŒ‡æ ‡å†å²"""
        try:
            # åŠ è½½æœ€è¿‘7å¤©çš„æŒ‡æ ‡
            for i in range(7):
                date = (datetime.utcnow() - timedelta(days=i)).strftime("%Y-%m-%d")
                metrics_file = self.metrics_dir / f"metrics_{date}.json"
                
                if metrics_file.exists():
                    with open(metrics_file, 'r', encoding='utf-8') as f:
                        metrics_data = json.load(f)
                    
                    for metric_dict in metrics_data:
                        metric_dict["timestamp"] = datetime.fromisoformat(metric_dict["timestamp"])
                        metrics = StorageMetrics(**metric_dict)
                        self._metrics_history.append(metrics)
            
            # æŒ‰æ—¶é—´æ’åº
            self._metrics_history.sort(key=lambda x: x.timestamp)
            
            # é™åˆ¶å¤§å°
            if len(self._metrics_history) > self._max_history:
                self._metrics_history = self._metrics_history[-self._max_history:]
                
        except Exception as e:
            logger.error(f"åŠ è½½æŒ‡æ ‡å†å²å¤±è´¥: {e}")

    # === å…¬å…±æ¥å£ ===

    async def get_current_metrics(self) -> StorageMetrics:
        """è·å–å½“å‰æŒ‡æ ‡"""
        return await self.collect_current_metrics()

    async def get_metrics_history(self, hours: int = 24) -> List[StorageMetrics]:
        """è·å–æŒ‡æ ‡å†å²"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        return [m for m in self._metrics_history if m.timestamp >= cutoff_time]

    async def get_active_alerts(self) -> List[Alert]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        return [a for a in self._active_alerts if not a.resolved]

    async def get_metrics_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        history = await self.get_metrics_history(hours)
        
        if not history:
            return {"error": "æ— å†å²æ•°æ®"}
        
        # è®¡ç®—ç»Ÿè®¡å€¼
        latest = history[-1]
        
        # è¶‹åŠ¿åˆ†æ
        if len(history) > 1:
            first = history[0]
            document_growth = latest.total_documents - first.total_documents
            storage_growth = latest.storage_size_mb - first.storage_size_mb
        else:
            document_growth = 0
            storage_growth = 0.0
        
        return {
            "current": asdict(latest),
            "trends": {
                "document_growth": document_growth,
                "storage_growth_mb": storage_growth,
                "avg_compression_ratio": sum(m.compression_ratio for m in history) / len(history),
                "avg_ai_accuracy": sum(m.ai_accuracy_rate for m in history) / len(history),
            },
            "health": {
                "ollama_uptime_percentage": len([m for m in history if m.ollama_status == "healthy"]) / len(history) * 100,
                "error_trend": "increasing" if latest.error_count_24h > 10 else "stable",
            },
            "active_alerts": len(await self.get_active_alerts()),
        }


# === æœåŠ¡å•ä¾‹ç®¡ç† ===

_storage_metrics: Optional[StorageMetricsCollector] = None

async def get_storage_metrics_collector() -> StorageMetricsCollector:
    """è·å–å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _storage_metrics
    if _storage_metrics is None:
        _storage_metrics = StorageMetricsCollector()
        if not await _storage_metrics.initialize():
            raise RuntimeError("å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨åˆå§‹åŒ–å¤±è´¥")
    return _storage_metrics

async def cleanup_storage_metrics():
    """æ¸…ç†å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨"""
    global _storage_metrics
    if _storage_metrics:
        await _storage_metrics.close()
        _storage_metrics = None