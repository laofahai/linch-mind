"""
é€šç”¨æœç´¢è·¯ç”± - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹çš„ç»Ÿä¸€æœç´¢API

é‡æ„è¯´æ˜ (2025-08-16):
- æ›¿ä»£åŸæœ‰çš„æ–‡ä»¶ç³»ç»Ÿç‰¹å®šæœç´¢æ¥å£
- æ”¯æŒæ–‡ä»¶ã€URLã€é‚®ç®±ã€ç”µè¯ç­‰æ‰€æœ‰å†…å®¹ç±»å‹
- ä¿æŒEverythingçº§åˆ«æœç´¢æ€§èƒ½
- æä¾›åˆ†å±‚æœç´¢ç­–ç•¥ (hot/warm/cold)
"""

import asyncio
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..ipc_protocol import IPCRequest, IPCResponse
from ..ipc_router import IPCRouter
from .generic_event_storage import get_generic_event_storage

logger = logging.getLogger(__name__)


def create_universal_search_router() -> IPCRouter:
    """åˆ›å»ºé€šç”¨æœç´¢è·¯ç”±"""
    router = IPCRouter(prefix="/search")

    @router.post("/universal")
    async def universal_search(request: IPCRequest) -> IPCResponse:
        """
        é€šç”¨æœç´¢API - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹

        è¯·æ±‚æ ¼å¼ï¼š
        {
            "query": "æœç´¢å…³é”®è¯",
            "limit": 100,
            "content_types": ["file_path", "url", "email"],  // å¯é€‰
            "connector_ids": ["connector_1", "connector_2"],   // å¯é€‰ï¼Œä»»æ„è¿æ¥å™¨ID
            "index_tiers": ["hot", "warm"],                   // å¯é€‰ï¼Œæ€§èƒ½ä¼˜åŒ–
            "include_metadata": false                         // å¯é€‰ï¼Œæ˜¯å¦åŒ…å«è¯¦ç»†å…ƒæ•°æ®
        }

        å“åº”æ ¼å¼ï¼š
        {
            "results": [
                {
                    "id": "connector_id:content_type:hash",
                    "connector_id": "connector_id",
                    "content_type": "file_path",
                    "primary_key": "/Users/john/Documents/report.pdf",
                    "display_name": "report.pdf",
                    "searchable_text": "report.pdf Documents john",
                    "score": 8.5,
                    "structured_data": {...},
                    "metadata": {...},
                    "last_modified": "2025-08-15T19:00:00Z"
                }
            ],
            "total_results": 25,
            "search_time_ms": 4.2,
            "performance_tier": "hot"
        }
        """
        try:
            data = request.data or {}
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if "query" not in data:
                return IPCResponse.error_response(
                    "INVALID_REQUEST", "Missing required field: query"
                )
            
            query = data["query"]
            if not query or not query.strip():
                return IPCResponse.error_response(
                    "INVALID_REQUEST", "Query cannot be empty"
                )
            
            # æå–æœç´¢å‚æ•°
            limit = data.get("limit", 100)
            content_types = data.get("content_types", [])
            connector_ids = data.get("connector_ids", [])
            include_metadata = data.get("include_metadata", False)
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šæ ¹æ®æŸ¥è¯¢å¤æ‚åº¦æ™ºèƒ½é€‰æ‹©ç´¢å¼•å±‚çº§
            index_tiers = data.get("index_tiers")
            if not index_tiers:
                # æ™ºèƒ½å±‚çº§é€‰æ‹©
                if len(query.strip().split()) == 1 and len(query) < 50:
                    # ç®€å•æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨çƒ­å±‚
                    index_tiers = ["hot", "warm"]
                else:
                    # å¤æ‚æŸ¥è¯¢ï¼Œä½¿ç”¨æ‰€æœ‰å±‚çº§
                    index_tiers = ["hot", "warm", "cold"]
            
            start_time = datetime.now()
            
            logger.info(f"ğŸ” æ‰§è¡Œé€šç”¨æœç´¢: query='{query}', types={content_types}, connectors={connector_ids}")
            
            # è·å–å­˜å‚¨æœåŠ¡å¹¶æ‰§è¡Œæœç´¢
            storage = get_generic_event_storage()
            search_results = storage.search_universal_index(
                query=query,
                limit=limit,
                content_types=content_types,
                connector_ids=connector_ids,
                include_metadata=include_metadata
            )
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            search_time_ms = (datetime.now() - start_time).total_seconds() * 1000
            
            # ç¡®å®šæ€§èƒ½å±‚çº§
            performance_tier = "hot" if search_time_ms < 10 else ("warm" if search_time_ms < 100 else "cold")
            
            logger.info(f"ğŸ¯ æœç´¢å®Œæˆ: {len(search_results)} ç»“æœ, è€—æ—¶ {search_time_ms:.1f}ms, å±‚çº§ {performance_tier}")
            
            return IPCResponse.success_response({
                "results": search_results,
                "total_results": len(search_results),
                "search_time_ms": round(search_time_ms, 2),
                "performance_tier": performance_tier,
                "query_info": {
                    "original_query": query,
                    "content_types_filtered": content_types,
                    "connector_ids_filtered": connector_ids,
                    "index_tiers_used": index_tiers
                }
            })
            
        except Exception as e:
            logger.error(f"âŒ é€šç”¨æœç´¢å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "SEARCH_ERROR", f"Search failed: {str(e)}"
            )

    @router.post("/by_connector")
    async def search_by_connector(request: IPCRequest) -> IPCResponse:
        """
        æŒ‰è¿æ¥å™¨æœç´¢ - å¿«æ·æ¥å£

        è¯·æ±‚æ ¼å¼ï¼š
        {
            "connector_id": "example_connector",
            "query": "report",
            "limit": 50
        }
        """
        try:
            data = request.data or {}
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["connector_id", "query"]
            for field in required_fields:
                if field not in data:
                    return IPCResponse.error_response(
                        "INVALID_REQUEST", f"Missing required field: {field}"
                    )
            
            connector_id = data["connector_id"]
            query = data["query"]
            limit = data.get("limit", 50)
            
            # è°ƒç”¨é€šç”¨æœç´¢ï¼Œé™å®šè¿æ¥å™¨
            storage = get_generic_event_storage()
            search_results = storage.search_universal_index(
                query=query,
                limit=limit,
                connector_ids=[connector_id]
            )
            
            return IPCResponse.success_response({
                "results": search_results,
                "connector_id": connector_id,
                "total_results": len(search_results)
            })
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰è¿æ¥å™¨æœç´¢å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "SEARCH_ERROR", f"Connector search failed: {str(e)}"
            )

    @router.post("/by_content_type")
    async def search_by_content_type(request: IPCRequest) -> IPCResponse:
        """
        æŒ‰å†…å®¹ç±»å‹æœç´¢ - å¿«æ·æ¥å£

        è¯·æ±‚æ ¼å¼ï¼š
        {
            "content_type": "file_path",  // file_path | url | email | phone | text
            "query": "report",
            "limit": 50
        }
        """
        try:
            data = request.data or {}
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["content_type", "query"]
            for field in required_fields:
                if field not in data:
                    return IPCResponse.error_response(
                        "INVALID_REQUEST", f"Missing required field: {field}"
                    )
            
            content_type = data["content_type"]
            query = data["query"]
            limit = data.get("limit", 50)
            
            # éªŒè¯å†…å®¹ç±»å‹
            valid_types = ["file_path", "url", "email", "phone", "text", "contact", "document", "other"]
            if content_type not in valid_types:
                return IPCResponse.error_response(
                    "INVALID_REQUEST", f"Invalid content_type. Must be one of: {valid_types}"
                )
            
            # è°ƒç”¨é€šç”¨æœç´¢ï¼Œé™å®šå†…å®¹ç±»å‹
            storage = get_generic_event_storage()
            search_results = storage.search_universal_index(
                query=query,
                limit=limit,
                content_types=[content_type]
            )
            
            return IPCResponse.success_response({
                "results": search_results,
                "content_type": content_type,
                "total_results": len(search_results)
            })
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰å†…å®¹ç±»å‹æœç´¢å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "SEARCH_ERROR", f"Content type search failed: {str(e)}"
            )

    @router.get("/stats")
    async def get_search_stats(request: IPCRequest) -> IPCResponse:
        """
        è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯

        å“åº”æ ¼å¼ï¼š
        {
            "total_entries": 150000,
            "entries_by_tier": {
                "hot": 50000,
                "warm": 75000,
                "cold": 25000
            },
            "entries_by_type": {
                "file_path": 45000,
                "url": 30000,
                "email": 15000,
                "text": 60000
            },
            "entries_by_connector": {
                "connector_1": 45000,
                "connector_2": 30000,
                "connector_3": 75000
            },
            "search_requests": 2500,
            "avg_search_time_ms": 8.5,
            "database_size": 104857600
        }
        """
        try:
            storage = get_generic_event_storage()
            stats = storage.get_universal_index_stats()
            
            return IPCResponse.success_response(stats)
            
        except Exception as e:
            logger.error(f"âŒ è·å–æœç´¢ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "STATS_ERROR", f"Failed to get stats: {str(e)}"
            )

    @router.post("/clear")
    async def clear_search_index(request: IPCRequest) -> IPCResponse:
        """
        æ¸…ç†æœç´¢ç´¢å¼• - ç®¡ç†æ¥å£

        è¯·æ±‚æ ¼å¼ï¼š
        {
            "connector_id": "example_connector"  // å¯é€‰ï¼Œæ¸…ç†ç‰¹å®šè¿æ¥å™¨çš„ç´¢å¼•
        }
        """
        try:
            data = request.data or {}
            connector_id = data.get("connector_id")
            
            # è¿™é‡Œåº”è¯¥æ·»åŠ æƒé™éªŒè¯ï¼Œç¡®ä¿åªæœ‰ç®¡ç†å‘˜å¯ä»¥æ¸…ç†
            # TODO: å®ç°æƒé™éªŒè¯
            
            from services.storage.universal_index_service import get_universal_index_service
            index_service = get_universal_index_service()
            
            success = index_service.clear_index(connector_id)
            
            if success:
                message = f"å·²æ¸…ç†ç´¢å¼•: {connector_id}" if connector_id else "å·²æ¸…ç†å…¨éƒ¨ç´¢å¼•"
                logger.info(f"ğŸ§¹ {message}")
                return IPCResponse.success_response({
                    "message": message,
                    "connector_id": connector_id
                })
            else:
                return IPCResponse.error_response(
                    "CLEAR_ERROR", "Failed to clear index"
                )
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æœç´¢ç´¢å¼•å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "CLEAR_ERROR", f"Failed to clear index: {str(e)}"
            )

    @router.get("/performance")
    async def get_performance_metrics(request: IPCRequest) -> IPCResponse:
        """
        è·å–æœç´¢æ€§èƒ½æŒ‡æ ‡ - ç›‘æ§æ¥å£

        å“åº”æ ¼å¼ï¼š
        {
            "search_performance": {
                "hot_tier_avg_ms": 3.2,
                "warm_tier_avg_ms": 25.8,
                "cold_tier_avg_ms": 180.5
            },
            "index_performance": {
                "indexing_rate_per_sec": 1500,
                "total_indexed_today": 25000
            },
            "system_health": {
                "database_health": "healthy",
                "memory_usage_mb": 85.6,
                "disk_usage_mb": 120.5
            }
        }
        """
        try:
            # è·å–åŸºç¡€ç»Ÿè®¡
            storage = get_generic_event_storage()
            stats = storage.get_universal_index_stats()
            
            # æ„å»ºæ€§èƒ½æŒ‡æ ‡
            performance_metrics = {
                "search_performance": {
                    "avg_search_time_ms": stats.get("avg_search_time_ms", 0),
                    "total_search_requests": stats.get("search_requests", 0),
                    "search_requests_per_hour": 0  # TODO: å®ç°å°æ—¶çº§ç»Ÿè®¡
                },
                "index_performance": {
                    "total_entries": stats.get("total_entries", 0),
                    "last_update": stats.get("last_update"),
                    "indexing_status": "active" if stats.get("total_entries", 0) > 0 else "idle"
                },
                "storage_breakdown": {
                    "entries_by_tier": stats.get("entries_by_tier", {}),
                    "entries_by_type": stats.get("entries_by_type", {}),
                    "entries_by_connector": stats.get("entries_by_connector", {})
                },
                "system_health": {
                    "database_size_mb": round(stats.get("database_size", 0) / (1024 * 1024), 2),
                    "status": "healthy" if stats.get("total_entries", 0) > 0 else "initializing"
                }
            }
            
            return IPCResponse.success_response(performance_metrics)
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "METRICS_ERROR", f"Failed to get performance metrics: {str(e)}"
            )

    return router