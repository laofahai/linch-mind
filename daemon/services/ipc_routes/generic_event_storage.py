"""
é€šç”¨äº‹ä»¶å­˜å‚¨æ¥å£ - ä¸è¿æ¥å™¨ç±»å‹å®Œå…¨æ— å…³
æä¾›è¿æ¥å™¨æ— å…³çš„äº‹ä»¶å­˜å‚¨å’Œå¤„ç†æœºåˆ¶ï¼Œé›†æˆå†…å®¹åˆ†æåŠŸèƒ½å’Œå¿«é€Ÿç´¢å¼•
"""

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from core.service_facade import get_service
from models.database_models import ConnectorLog, EntityMetadata
from services.unified_database_service import UnifiedDatabaseService
from services.fast_index_storage_service import FastIndexStorageService
from services.storage.intelligent_event_processor import get_intelligent_event_processor

logger = logging.getLogger(__name__)


class GenericEventStorage:
    """
    é€šç”¨äº‹ä»¶å­˜å‚¨ - ä¸è¿æ¥å™¨ç±»å‹å®Œå…¨æ— å…³

    æ‰€æœ‰è¿æ¥å™¨ä½¿ç”¨ç›¸åŒçš„å­˜å‚¨æ¥å£ï¼Œä¸å…³å¿ƒå…·ä½“çš„äº‹ä»¶å†…å®¹æˆ–æ ¼å¼
    """

    def __init__(self):
        self._db_service = None
        self._fast_index_service = None
        self._intelligent_processor = None

    @property
    def db_service(self):
        """æ‡’åŠ è½½æ•°æ®åº“æœåŠ¡"""
        if self._db_service is None:
            try:
                self._db_service = get_service(UnifiedDatabaseService)
            except Exception as e:
                logger.warning(f"Database service not available: {str(e)}")
                return None
        return self._db_service
    
    @property
    def fast_index_service(self):
        """æ‡’åŠ è½½å¿«é€Ÿç´¢å¼•æœåŠ¡"""
        if self._fast_index_service is None:
            try:
                self._fast_index_service = FastIndexStorageService()
                self._fast_index_service.initialize()
            except Exception as e:
                logger.warning(f"Fast index service not available: {str(e)}")
                return None
        return self._fast_index_service
    
    @property
    def intelligent_processor(self):
        """æ‡’åŠ è½½æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨"""
        if self._intelligent_processor is None:
            try:
                # æ³¨æ„ï¼šè¿™é‡Œæ˜¯å¼‚æ­¥è·å–ï¼Œéœ€è¦åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­åˆå§‹åŒ–
                # åœ¨å®é™…è°ƒç”¨æ—¶ä¼šé€šè¿‡ await åˆå§‹åŒ–
                return None
            except Exception as e:
                logger.warning(f"Intelligent processor not available: {str(e)}")
                return None
        return self._intelligent_processor
    
    async def _ensure_intelligent_processor(self):
        """ç¡®ä¿æ™ºèƒ½å¤„ç†å™¨å·²åˆå§‹åŒ–"""
        if self._intelligent_processor is None:
            try:
                self._intelligent_processor = await get_intelligent_event_processor()
                logger.info("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨å·²åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self._intelligent_processor = None
        return self._intelligent_processor

    async def store_generic_event(
        self,
        connector_id: str,
        event_type: str,
        event_data: Dict[str, Any],
        timestamp: str,
        metadata: Optional[Dict[str, Any]],
    ) -> bool:
        """
        å­˜å‚¨é€šç”¨è¿æ¥å™¨äº‹ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨æ™ºèƒ½AIå¤„ç†

        Args:
            connector_id: è¿æ¥å™¨IDï¼ˆä»»æ„å­—ç¬¦ä¸²ï¼‰
            event_type: äº‹ä»¶ç±»å‹ï¼ˆä»»æ„å­—ç¬¦ä¸²ï¼‰
            event_data: äº‹ä»¶æ•°æ®ï¼ˆä»»æ„JSONå¯¹è±¡ï¼‰
            timestamp: æ—¶é—´æˆ³
            metadata: å…ƒæ•°æ®ï¼ˆä»»æ„JSONå¯¹è±¡ï¼Œå¯é€‰ï¼‰

        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        try:
            # å¤„ç†å¿«é€Ÿç´¢å¼•äº‹ä»¶ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œä¸ç»è¿‡AIï¼‰
            if event_type == "file_indexed" and event_data.get("source") == "fast_indexer":
                await self._handle_fast_index_event(event_data)
                return True

            # å°è¯•ä½¿ç”¨æ™ºèƒ½å¤„ç†å™¨ï¼ˆAIé©±åŠ¨ï¼‰
            processor = await self._ensure_intelligent_processor()
            if processor:
                try:
                    result = await processor.process_connector_event(
                        connector_id, event_type, event_data, timestamp, metadata
                    )
                    
                    if result.accepted:
                        logger.info(f"ğŸš€ ä¼˜åŒ–å¤„ç†æˆåŠŸ: {connector_id}/{event_type}, ä»·å€¼={result.value_score:.3f}, è€—æ—¶={result.processing_time_ms:.1f}ms")
                        return True
                    else:
                        logger.debug(f"ğŸ—‘ï¸  ä¼˜åŒ–è¿‡æ»¤æ‹’ç»: {connector_id}/{event_type}, åŸå› ={result.reasoning}")
                        return True  # æ‹’ç»ä¹Ÿæ˜¯æˆåŠŸçš„å¤„ç†ç»“æœ
                        
                except Exception as e:
                    logger.warning(f"æ™ºèƒ½å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼: {e}")
                    # ç»§ç»­æ‰§è¡Œä¼ ç»Ÿå¤„ç†æ–¹å¼
            else:
                logger.debug("æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")

            # å›é€€åˆ°ä¼ ç»Ÿå¤„ç†æ–¹å¼
            return await self._store_generic_event_traditional(
                connector_id, event_type, event_data, timestamp, metadata
            )

        except Exception as e:
            logger.error(f"Failed to store generic event: {str(e)}")
            return False

    async def _store_generic_event_traditional(
        self,
        connector_id: str,
        event_type: str,
        event_data: Dict[str, Any],
        timestamp: str,
        metadata: Optional[Dict[str, Any]],
    ) -> bool:
        """
        ä¼ ç»Ÿäº‹ä»¶å­˜å‚¨æ–¹å¼ï¼ˆå›é€€æœºåˆ¶ï¼‰
        """
        try:
            if self.db_service is None:
                logger.error("Database service is not available")
                return False

            # ç”Ÿæˆé€šç”¨å®ä½“IDï¼ˆä¸ä¾èµ–äº‹ä»¶å†…å®¹ï¼‰
            entity_id = f"{connector_id}_{hash(str(event_data) + str(timestamp)) % 1000000}"

            # å°è¯•è¿›è¡Œä¼ ç»Ÿå†…å®¹åˆ†æ
            content_analysis = await self._analyze_event_content(event_data, event_type)

            # æ„å»ºé€šç”¨çš„å­˜å‚¨ç»“æ„ï¼Œç¡®ä¿metadataä¸ä¸ºNone
            entity_properties = {
                "connector_id": connector_id,
                "event_type": event_type,
                "timestamp": timestamp,
                "event_data": event_data,  # åŸå§‹äº‹ä»¶æ•°æ®ï¼Œä¸åšä»»ä½•è§£æ
                "metadata": metadata or {},  # ç¡®ä¿ä¸ä¸ºNone
                "processed_at": datetime.utcnow().isoformat(),
                "content_analysis": content_analysis,  # æ·»åŠ å†…å®¹åˆ†æç»“æœ
                "processing_mode": "traditional",  # æ ‡è®°å¤„ç†æ¨¡å¼
            }

            with self.db_service.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒäº‹ä»¶
                existing = (
                    session.query(EntityMetadata).filter_by(entity_id=entity_id).first()
                )

                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    existing.properties = entity_properties
                    existing.updated_at = datetime.utcnow()
                    existing.access_count += 1
                    existing.last_accessed = datetime.utcnow()
                    logger.debug(f"Updated existing event: {entity_id}")
                else:
                    # åˆ›å»ºæ–°è®°å½• - ä½¿ç”¨é€šç”¨å‘½å
                    entity_record = EntityMetadata(
                        entity_id=entity_id,
                        name=f"{connector_id}_{event_type}_{datetime.now().strftime('%H%M%S')}",
                        type="connector_event",  # é€šç”¨ç±»å‹
                        description=f"Event from connector {connector_id}",
                        properties=entity_properties,
                        access_count=1,
                    )
                    session.add(entity_record)
                    logger.debug(f"Created new event record: {entity_id}")

                # è®°å½•è¿æ¥å™¨æ—¥å¿—
                log_entry = ConnectorLog(
                    connector_id=connector_id,
                    level="INFO",
                    message=f"Event processed (traditional): {event_type}",
                    details={
                        "event_type": event_type,
                        "event_size": len(str(event_data)),
                        "metadata_keys": list((metadata or {}).keys()),
                        "timestamp": timestamp,
                        "processing_mode": "traditional",
                    },
                )
                session.add(log_entry)

                session.commit()

            logger.info(f"âœ… Stored generic event (traditional) from {connector_id}: {event_type}")
            return True

        except Exception as e:
            logger.error(f"Failed to store generic event (traditional): {str(e)}")
            return False

    async def _analyze_event_content(
        self, event_data: Dict[str, Any], event_type: str
    ) -> Optional[Dict[str, Any]]:
        """
        åˆ†æäº‹ä»¶å†…å®¹

        Args:
            event_data: äº‹ä»¶æ•°æ®
            event_type: äº‹ä»¶ç±»å‹

        Returns:
            å†…å®¹åˆ†æç»“æœ
        """
        try:
            # å°è¯•å¯¼å…¥å†…å®¹åˆ†ææœåŠ¡
            from core.service_facade import get_content_analysis_service

            analysis_service = get_content_analysis_service()

            # å°è¯•ä»äº‹ä»¶æ•°æ®ä¸­æå–æ–‡æœ¬å†…å®¹
            content = self._extract_content_from_event(event_data)
            if not content:
                return None

            # ç¡®å®šå†…å®¹ç±»å‹
            content_type = self._determine_content_type(event_data, event_type)

            # æ‰§è¡Œå†…å®¹åˆ†æ
            analysis_result = analysis_service.analyze_content(content, content_type)

            logger.debug(f"å†…å®¹åˆ†æå®Œæˆ: {len(content)} å­—ç¬¦, ç±»å‹: {content_type}")
            return analysis_result

        except ImportError:
            logger.warning("å†…å®¹åˆ†ææœåŠ¡ä¸å¯ç”¨")
            return None
        except Exception as e:
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return None

    def _extract_content_from_event(self, event_data: Dict[str, Any]) -> Optional[str]:
        """ä»äº‹ä»¶æ•°æ®ä¸­æå–æ–‡æœ¬å†…å®¹"""
        # å¸¸è§çš„å†…å®¹å­—æ®µå
        content_fields = ["content", "text", "data", "message", "body", "value"]

        for field in content_fields:
            if field in event_data:
                content = event_data[field]
                if isinstance(content, str) and content.strip():
                    return content.strip()

        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å†…å®¹å­—æ®µï¼Œå°è¯•è½¬æ¢æ•´ä¸ªäº‹ä»¶æ•°æ®ä¸ºå­—ç¬¦ä¸²
        if isinstance(event_data, dict) and len(event_data) == 1:
            # å•å­—æ®µäº‹ä»¶æ•°æ®
            value = list(event_data.values())[0]
            if isinstance(value, str) and value.strip():
                return value.strip()

        return None

    def _determine_content_type(
        self, event_data: Dict[str, Any], event_type: str
    ) -> str:
        """ç¡®å®šå†…å®¹ç±»å‹"""
        # åŸºäºäº‹ä»¶ç±»å‹æ¨æ–­
        if event_type in ["url_changed", "url_copied", "link_event"]:
            return "url"
        elif event_type in ["file_changed", "file_copied", "file_event"]:
            return "file_path"
        elif event_type in ["clipboard_changed", "content_changed"]:
            # å‰ªè´´æ¿äº‹ä»¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æå†…å®¹
            content = self._extract_content_from_event(event_data)
            if content:
                # ç®€å•å¯å‘å¼åˆ¤æ–­
                if content.startswith(("http://", "https://")):
                    return "url"
                elif "/" in content or "\\" in content:
                    return "file_path"
            return "text"

        # åŸºäºäº‹ä»¶æ•°æ®æ¨æ–­
        content_type_field = event_data.get("content_type", event_data.get("type"))
        if content_type_field:
            return str(content_type_field)

        return "text"
    
    async def _handle_fast_index_event(self, event_data: Dict[str, Any]) -> bool:
        """
        å¤„ç†å¿«é€Ÿç´¢å¼•äº‹ä»¶
        
        Args:
            event_data: å¿«é€Ÿç´¢å¼•äº‹ä»¶æ•°æ®
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.fast_index_service is None:
                logger.warning("å¿«é€Ÿç´¢å¼•æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å¿«é€Ÿç´¢å¼•å¤„ç†")
                return False
            
            # æå–å¿«é€Ÿç´¢å¼•æ‰€éœ€çš„å­—æ®µ
            required_fields = ['path', 'name']
            if not all(field in event_data for field in required_fields):
                logger.warning(f"å¿«é€Ÿç´¢å¼•äº‹ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {required_fields}")
                return False
            
            # æ„å»ºå¿«é€Ÿç´¢å¼•æ¡ç›®
            index_entry = {
                'path': event_data.get('path'),
                'name': event_data.get('name'),
                'size': event_data.get('size', 0),
                'is_directory': event_data.get('is_directory', False),
                'extension': event_data.get('extension', ''),
                'last_modified': event_data.get('last_modified'),
                'priority': event_data.get('priority', 2)
            }
            
            # æ‰¹é‡å­˜å‚¨ï¼ˆè¿™é‡Œæ˜¯å•ä¸ªæ¡ç›®ï¼Œä½†ä½¿ç”¨æ‰¹é‡æ¥å£ä»¥ä¿æŒä¸€è‡´æ€§ï¼‰
            success = await self.fast_index_service.store_fast_index_batch([index_entry])
            
            if success:
                logger.debug(f"âœ… å¿«é€Ÿç´¢å¼•æ¡ç›®å·²å­˜å‚¨: {event_data.get('path')}")
            else:
                logger.warning(f"âŒ å¿«é€Ÿç´¢å¼•æ¡ç›®å­˜å‚¨å¤±è´¥: {event_data.get('path')}")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¿«é€Ÿç´¢å¼•äº‹ä»¶å¤±è´¥: {e}")
            return False
    
    def search_fast_index(
        self,
        query: str,
        limit: int = 100,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        å¿«é€Ÿç´¢å¼•æœç´¢æ¥å£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: ç»“æœé™åˆ¶
            **kwargs: å…¶ä»–æœç´¢å‚æ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            if self.fast_index_service is None:
                logger.warning("å¿«é€Ÿç´¢å¼•æœåŠ¡ä¸å¯ç”¨")
                return []
            
            return self.fast_index_service.search_files(query, limit, **kwargs)
            
        except Exception as e:
            logger.error(f"âŒ å¿«é€Ÿç´¢å¼•æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_fast_index_stats(self) -> Dict[str, Any]:
        """è·å–å¿«é€Ÿç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.fast_index_service is None:
                return {}
            
            return self.fast_index_service.get_stats()
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¿«é€Ÿç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    # === æ™ºèƒ½æœç´¢æ¥å£ ===

    async def search_intelligent_content(
        self,
        query: str,
        k: int = 10,
        min_value_score: float = 0.0,
        content_types: Optional[List[str]] = None,
        connector_ids: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½å†…å®¹æœç´¢æ¥å£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›ç»“æœæ•°é‡
            min_value_score: æœ€å°ä»·å€¼è¯„åˆ†
            content_types: å†…å®¹ç±»å‹è¿‡æ»¤
            connector_ids: è¿æ¥å™¨IDè¿‡æ»¤
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                results = await processor.search_intelligent_content(
                    query=query,
                    k=k,
                    min_value_score=min_value_score,
                    content_types=content_types,
                    connector_ids=connector_ids,
                )
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                search_results = []
                for result in results:
                    search_results.append({
                        "id": result.id,
                        "summary": result.summary,
                        "score": result.score,
                        "value_score": result.value_score,
                        "content_type": result.content_type,
                        "timestamp": result.timestamp.isoformat() if result.timestamp else None,
                        "metadata": result.metadata,
                        "entity_id": result.entity_id,
                    })
                
                logger.info(f"æ™ºèƒ½æœç´¢å®Œæˆ: {len(search_results)} ç»“æœ")
                return search_results
            else:
                logger.warning("æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ™ºèƒ½æœç´¢")
                return []
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢å¤±è´¥: {e}")
            return []

    async def get_content_statistics(self) -> Dict[str, Any]:
        """
        è·å–å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                return await processor.get_content_statistics()
            else:
                return {"error": "æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨"}
                
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}

    async def get_high_value_content(
        self,
        min_score: float = 0.8,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """
        è·å–é«˜ä»·å€¼å†…å®¹
        
        Args:
            min_score: æœ€å°ä»·å€¼è¯„åˆ†
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            é«˜ä»·å€¼å†…å®¹åˆ—è¡¨
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                return await processor.get_content_by_value_range(
                    min_score=min_score,
                    max_score=1.0,
                    limit=limit
                )
            else:
                return []
                
        except Exception as e:
            logger.error(f"è·å–é«˜ä»·å€¼å†…å®¹å¤±è´¥: {e}")
            return []

    async def cleanup_low_value_content(self, min_age_days: int = 30) -> int:
        """
        æ¸…ç†ä½ä»·å€¼å†…å®¹
        
        Args:
            min_age_days: æœ€å°å¹´é¾„ï¼ˆå¤©ï¼‰
            
        Returns:
            æ¸…ç†çš„å†…å®¹æ•°é‡
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                cleaned_count = await processor.cleanup_low_value_content(min_age_days)
                logger.info(f"æ¸…ç†ä½ä»·å€¼å†…å®¹: {cleaned_count} é¡¹")
                return cleaned_count
            else:
                return 0
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä½ä»·å€¼å†…å®¹å¤±è´¥: {e}")
            return 0

    async def get_processing_metrics(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†æ€§èƒ½æŒ‡æ ‡
        
        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                metrics = await processor.get_metrics()
                return {
                    "total_events": metrics.total_events,
                    "accepted_events": metrics.accepted_events,
                    "rejected_events": metrics.rejected_events,
                    "acceptance_rate": metrics.acceptance_rate,
                    "avg_processing_time_ms": metrics.avg_processing_time_ms,
                    "avg_value_score": metrics.avg_value_score,
                    "entities_extracted": metrics.entities_extracted,
                    "storage_efficiency": metrics.storage_efficiency,
                    "last_updated": metrics.last_updated.isoformat(),
                }
            else:
                return {"error": "æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨"}
                
        except Exception as e:
            logger.error(f"è·å–å¤„ç†æŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}


# å•ä¾‹æ¨¡å¼
_generic_storage = None


def get_generic_event_storage() -> GenericEventStorage:
    """è·å–é€šç”¨äº‹ä»¶å­˜å‚¨å®ä¾‹"""
    global _generic_storage
    if _generic_storage is None:
        _generic_storage = GenericEventStorage()
    return _generic_storage
