"""
é€šç”¨äº‹ä»¶å­˜å‚¨æ¥å£ - çœŸæ­£çš„è¿æ¥å™¨æ— å…³æ¶æ„

é‡æ„è¯´æ˜ (2025-08-16):
- ç§»é™¤äº†FastIndexStorageServiceçš„æ–‡ä»¶ç³»ç»Ÿç‰¹å®šé€»è¾‘
- ä½¿ç”¨UniversalIndexServiceæ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹çš„å¿«é€Ÿæœç´¢
- ä¿æŒEverythingçº§åˆ«æœç´¢æ€§èƒ½ï¼Œä½†æ‰©å±•åˆ°ä»»æ„è¿æ¥å™¨
- é›†æˆæ™ºèƒ½AIå¤„ç†å’Œè¯­ä¹‰ç†è§£åŠŸèƒ½
"""

import json
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from core.service_facade import get_service
from models.database_models import ConnectorLog, EntityMetadata
from services.unified_database_service import UnifiedDatabaseService
from services.storage.universal_index_service import get_universal_index_service
from services.storage.intelligent_event_processor import get_intelligent_event_processor

logger = logging.getLogger(__name__)


class GenericEventStorage:
    """
    é€šç”¨äº‹ä»¶å­˜å‚¨ - çœŸæ­£çš„è¿æ¥å™¨æ— å…³æ¶æ„

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. é€šç”¨ç´¢å¼•ï¼šæ”¯æŒæ‰€æœ‰è¿æ¥å™¨çš„å¿«é€Ÿæœç´¢ (æ–‡ä»¶ã€URLã€é‚®ç®±ç­‰)
    2. AIå¤„ç†ï¼šæ™ºèƒ½å†…å®¹åˆ†æå’Œè¯­ä¹‰ç†è§£
    3. ä¼ ç»Ÿå­˜å‚¨ï¼šå…œåº•çš„æ•°æ®æŒä¹…åŒ–æœºåˆ¶
    
    æ‰€æœ‰è¿æ¥å™¨ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¥å£ï¼Œæ— ä»»ä½•ç‰¹å®šé€»è¾‘
    """

    def __init__(self):
        self._db_service = None
        self._universal_index_service = None
        self._intelligent_processor = None

    @property
    def db_service(self):
        """æ‡’åŠ è½½æ•°æ®åº“æœåŠ¡"""
        if self._db_service is None:
            try:
                self._db_service = get_service(UnifiedDatabaseService)
            except Exception as e:
                logger.warning(f"Database service not available: {str(e)}")
                return None
        return self._db_service
    
    @property
    def universal_index_service(self):
        """æ‡’åŠ è½½é€šç”¨ç´¢å¼•æœåŠ¡"""
        if self._universal_index_service is None:
            try:
                self._universal_index_service = get_universal_index_service()
            except Exception as e:
                logger.warning(f"Universal index service not available: {str(e)}")
                return None
        return self._universal_index_service
    
    @property
    def intelligent_processor(self):
        """æ‡’åŠ è½½æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨"""
        if self._intelligent_processor is None:
            try:
                # æ³¨æ„ï¼šè¿™é‡Œæ˜¯å¼‚æ­¥è·å–ï¼Œéœ€è¦åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­åˆå§‹åŒ–
                # åœ¨å®é™…è°ƒç”¨æ—¶ä¼šé€šè¿‡ await åˆå§‹åŒ–
                return None
            except Exception as e:
                logger.warning(f"Intelligent processor not available: {str(e)}")
                return None
        return self._intelligent_processor
    
    def _should_use_intelligent_processing(
        self, 
        connector_id: str, 
        event_type: str, 
        event_data: Dict[str, Any]
    ) -> bool:
        """
        åˆ¤æ–­äº‹ä»¶æ˜¯å¦åº”è¯¥ä½¿ç”¨æ™ºèƒ½å¤„ç†å™¨
        
        åŸåˆ™ï¼š
        1. åŸºäºäº‹ä»¶å†…å®¹ç‰¹å¾ï¼Œè€Œéäº‹ä»¶ç±»å‹æšä¸¾
        2. æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ…å«å€¼å¾—AIåˆ†æçš„æ–‡æœ¬å†…å®¹
        3. ä½¿ç”¨å¯å‘å¼è§„åˆ™ï¼Œé¿å…å¼ºè€¦åˆè¿æ¥å™¨
        """
        
        # ç­–ç•¥1: æ£€æŸ¥äº‹ä»¶æ•°æ®ç»“æ„ç‰¹å¾
        if self._is_structured_metadata_event(event_data):
            # ç»“æ„åŒ–å…ƒæ•°æ®äº‹ä»¶ï¼ˆå¦‚æ–‡ä»¶ç´¢å¼•ï¼‰é€šå¸¸ä¸éœ€è¦AIåˆ†æ
            return self._has_rich_text_content(event_data)
        
        # ç­–ç•¥2: æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹
        if self._has_meaningful_text_content(event_data):
            return True
        
        # ç­–ç•¥3: åŸºäºäº‹ä»¶ç±»å‹çš„å¯å‘å¼è§„åˆ™
        return self._apply_heuristic_rules(connector_id, event_type, event_data)
    
    def _is_structured_metadata_event(self, event_data: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºç»“æ„åŒ–å…ƒæ•°æ®äº‹ä»¶
        
        ç‰¹å¾ï¼š
        - åŒ…å«è·¯å¾„ã€å¤§å°ã€æ—¶é—´æˆ³ç­‰ç»“æ„åŒ–å­—æ®µ
        - ä¸»è¦ç”¨äºç´¢å¼•å’Œæœç´¢ï¼Œè€Œéå†…å®¹åˆ†æ
        """
        # å…ƒæ•°æ®å­—æ®µæ¨¡å¼
        metadata_field_patterns = {
            "path", "size", "modified_time", "created_time", 
            "extension", "is_directory", "url", "email", 
            "phone", "contact_id", "file_id", "item_id"
        }
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿event_dataä¸ä¸ºNoneä¸”ä¸ºå­—å…¸ç±»å‹
        if event_data is None or not isinstance(event_data, dict):
            return False
        
        # å¦‚æœäº‹ä»¶æ•°æ®ä¸»è¦åŒ…å«å…ƒæ•°æ®å­—æ®µï¼Œåˆ™è®¤ä¸ºæ˜¯å…ƒæ•°æ®äº‹ä»¶
        data_fields = set(event_data.keys())
        metadata_fields = data_fields.intersection(metadata_field_patterns)
        
        # å¦‚æœå…ƒæ•°æ®å­—æ®µå æ¯”è¶…è¿‡60%ï¼Œè®¤ä¸ºæ˜¯å…ƒæ•°æ®äº‹ä»¶
        if len(data_fields) > 0:
            metadata_ratio = len(metadata_fields) / len(data_fields)
            return metadata_ratio > 0.6
        
        return False
    
    def _apply_heuristic_rules(
        self, 
        connector_id: str, 
        event_type: str, 
        event_data: Dict[str, Any]
    ) -> bool:
        """
        åº”ç”¨å¯å‘å¼è§„åˆ™
        
        åŸºäºäº‹ä»¶ç±»å‹åç§°çš„è¯­ä¹‰åˆ¤æ–­ï¼Œé¿å…ç¡¬ç¼–ç è¿æ¥å™¨ç±»å‹
        """
        # æ˜ç¡®çš„ç´¢å¼•ç›¸å…³äº‹ä»¶ç±»å‹
        index_keywords = {"indexed", "scan", "progress", "metadata", "catalog"}
        event_type_lower = event_type.lower()
        
        for keyword in index_keywords:
            if keyword in event_type_lower:
                return False  # ç´¢å¼•å‹äº‹ä»¶ï¼Œä¸éœ€è¦AIå¤„ç†
        
        # æ˜ç¡®çš„å†…å®¹ç›¸å…³äº‹ä»¶ç±»å‹
        content_keywords = {"content", "text", "message", "document", "note"}
        
        for keyword in content_keywords:
            if keyword in event_type_lower:
                return True  # å†…å®¹å‹äº‹ä»¶ï¼Œéœ€è¦AIå¤„ç†
        
        # é»˜è®¤ï¼šå¦‚æœæ— æ³•åˆ¤æ–­ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹
        return self._has_minimal_text_content(event_data)
    
    def _has_minimal_text_content(self, event_data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æœ€åŸºæœ¬çš„æ–‡æœ¬å†…å®¹ï¼ˆé™ä½é˜ˆå€¼ï¼‰"""
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿event_dataä¸ä¸ºNoneä¸”ä¸ºå­—å…¸ç±»å‹
        if event_data is None or not isinstance(event_data, dict):
            return False
            
        content_fields = ["content", "text", "data", "message", "body", "value"]
        
        for field in content_fields:
            if field in event_data:
                content = event_data[field]
                if isinstance(content, str) and len(content.strip()) > 5:  # é™ä½é˜ˆå€¼
                    return True
        
        return False
    
    def _has_rich_text_content(self, event_data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç´¢å¼•å‹äº‹ä»¶æ˜¯å¦åŒ…å«ä¸°å¯Œçš„æ–‡æœ¬å†…å®¹å€¼å¾—AIåˆ†æ"""
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿event_dataä¸ä¸ºNoneä¸”ä¸ºå­—å…¸ç±»å‹
        if event_data is None or not isinstance(event_data, dict):
            return False
            
        content_fields = ["content", "text", "description", "summary", "body"]
        
        for field in content_fields:
            if field in event_data:
                content = event_data[field]
                if isinstance(content, str) and len(content.strip()) > 50:  # è¾ƒé«˜çš„æ–‡æœ¬é•¿åº¦é˜ˆå€¼
                    return True
        return False
    
    def _has_meaningful_text_content(self, event_data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹"""
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿event_dataä¸ä¸ºNoneä¸”ä¸ºå­—å…¸ç±»å‹
        if event_data is None or not isinstance(event_data, dict):
            return False
            
        content_fields = ["content", "text", "data", "message", "body", "value"]
        
        for field in content_fields:
            if field in event_data:
                content = event_data[field]
                if isinstance(content, str) and len(content.strip()) > 10:
                    return True
        
        # æ£€æŸ¥å•å­—æ®µæ–‡æœ¬äº‹ä»¶
        if isinstance(event_data, dict) and len(event_data) == 1:
            value = list(event_data.values())[0]
            if isinstance(value, str) and len(value.strip()) > 10:
                return True
        
        return False
    
    async def _ensure_intelligent_processor(self):
        """ç¡®ä¿æ™ºèƒ½å¤„ç†å™¨å·²åˆå§‹åŒ–"""
        if self._intelligent_processor is None:
            try:
                self._intelligent_processor = await get_intelligent_event_processor()
                logger.info("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨å·²åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self._intelligent_processor = None
        return self._intelligent_processor

    async def store_generic_event(
        self,
        connector_id: str,
        event_type: str,
        event_data: Dict[str, Any],
        timestamp: str,
        metadata: Optional[Dict[str, Any]],
    ) -> bool:
        """
        å­˜å‚¨é€šç”¨è¿æ¥å™¨äº‹ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨æ™ºèƒ½AIå¤„ç†

        Args:
            connector_id: è¿æ¥å™¨IDï¼ˆä»»æ„å­—ç¬¦ä¸²ï¼‰
            event_type: äº‹ä»¶ç±»å‹ï¼ˆä»»æ„å­—ç¬¦ä¸²ï¼‰
            event_data: äº‹ä»¶æ•°æ®ï¼ˆä»»æ„JSONå¯¹è±¡ï¼‰
            timestamp: æ—¶é—´æˆ³
            metadata: å…ƒæ•°æ®ï¼ˆä»»æ„JSONå¯¹è±¡ï¼Œå¯é€‰ï¼‰

        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä½¿ç”¨é€šç”¨ç´¢å¼•å¤„ç†æ‰€æœ‰è¿æ¥å™¨äº‹ä»¶ï¼ˆä¿æŒå¿«é€Ÿæœç´¢èƒ½åŠ›ï¼‰
            await self._handle_universal_index_event(
                connector_id, event_type, event_data, timestamp, metadata
            )

            # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œæ™ºèƒ½å¤„ç†ï¼ˆåŸºäºäº‹ä»¶ç±»å‹å’Œå†…å®¹ï¼‰
            if self._should_use_intelligent_processing(connector_id, event_type, event_data):
                # å°è¯•ä½¿ç”¨æ™ºèƒ½å¤„ç†å™¨ï¼ˆAIé©±åŠ¨ï¼‰
                processor = await self._ensure_intelligent_processor()
                if processor:
                    try:
                        result = await processor.process_connector_event(
                            connector_id, event_type, event_data, timestamp, metadata
                        )
                        
                        if result.accepted:
                            logger.info(f"ğŸš€ ä¼˜åŒ–å¤„ç†æˆåŠŸ: {connector_id}/{event_type}, ä»·å€¼={result.value_score:.3f}, è€—æ—¶={result.processing_time_ms:.1f}ms")
                            return True
                        else:
                            logger.debug(f"ğŸ—‘ï¸  ä¼˜åŒ–è¿‡æ»¤æ‹’ç»: {connector_id}/{event_type}, åŸå› ={result.reasoning}")
                            return True  # æ‹’ç»ä¹Ÿæ˜¯æˆåŠŸçš„å¤„ç†ç»“æœ
                            
                    except Exception as e:
                        logger.warning(f"æ™ºèƒ½å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼: {e}")
                        # ç»§ç»­æ‰§è¡Œä¼ ç»Ÿå¤„ç†æ–¹å¼
                else:
                    logger.debug("æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
            else:
                logger.debug(f"äº‹ä»¶ç±»å‹ä¸é€‚åˆæ™ºèƒ½å¤„ç†: {connector_id}/{event_type}, ç›´æ¥ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")

            # å›é€€åˆ°ä¼ ç»Ÿå¤„ç†æ–¹å¼
            return await self._store_generic_event_traditional(
                connector_id, event_type, event_data, timestamp, metadata
            )

        except Exception as e:
            logger.error(f"Failed to store generic event: {str(e)}")
            return False

    async def _store_generic_event_traditional(
        self,
        connector_id: str,
        event_type: str,
        event_data: Dict[str, Any],
        timestamp: str,
        metadata: Optional[Dict[str, Any]],
    ) -> bool:
        """
        ä¼ ç»Ÿäº‹ä»¶å­˜å‚¨æ–¹å¼ï¼ˆå›é€€æœºåˆ¶ï¼‰
        """
        try:
            if self.db_service is None:
                logger.error("Database service is not available")
                return False

            # ğŸ›¡ï¸ å¢å¼ºçš„å®ä½“IDç”Ÿæˆ - é˜²æ­¢ç©ºå€¼å¯¼è‡´çš„å“ˆå¸Œå†²çª
            import hashlib
            
            # ç¡®ä¿å…³é”®å­—æ®µä¸ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼é˜²æ­¢å“ˆå¸Œå†²çª
            safe_connector_id = connector_id or "unknown_connector"
            safe_event_type = event_type or "unknown_event"
            safe_event_data = event_data if event_data is not None else {"empty": True}
            
            # æ·»åŠ æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§ï¼ˆå¯¹äºç›¸åŒå†…å®¹çš„äº‹ä»¶ï¼‰
            content_key = f"{safe_connector_id}:{safe_event_type}:{json.dumps(safe_event_data, sort_keys=True)}:{timestamp}"
            content_hash = hashlib.sha256(content_key.encode()).hexdigest()[:16]
            entity_id = f"{safe_connector_id}_{safe_event_type}_{content_hash}"

            # å°è¯•è¿›è¡Œä¼ ç»Ÿå†…å®¹åˆ†æ
            content_analysis = await self._analyze_event_content(event_data, event_type)

            # æ„å»ºé€šç”¨çš„å­˜å‚¨ç»“æ„ï¼Œç¡®ä¿metadataä¸ä¸ºNone
            entity_properties = {
                "connector_id": connector_id,
                "event_type": event_type,
                "timestamp": timestamp,
                "event_data": event_data,  # åŸå§‹äº‹ä»¶æ•°æ®ï¼Œä¸åšä»»ä½•è§£æ
                "metadata": metadata or {},  # ç¡®ä¿ä¸ä¸ºNone
                "processed_at": datetime.utcnow().isoformat(),
                "content_analysis": content_analysis,  # æ·»åŠ å†…å®¹åˆ†æç»“æœ
                "processing_mode": "traditional",  # æ ‡è®°å¤„ç†æ¨¡å¼
            }

            with self.db_service.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒäº‹ä»¶
                existing = (
                    session.query(EntityMetadata).filter_by(entity_id=entity_id).first()
                )

                if existing:
                    # å‘ç°é‡å¤äº‹ä»¶ï¼Œåªæ›´æ–°è®¿é—®è®¡æ•°å’Œæ—¶é—´æˆ³
                    existing.access_count += 1
                    existing.last_accessed = datetime.utcnow()
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ—¶é—´æˆ³ï¼ˆå…è®¸å°èŒƒå›´æ—¶é—´å·®å¼‚ï¼‰
                    existing_timestamp = existing.properties.get('timestamp', '')
                    if timestamp != existing_timestamp:
                        # ä¿ç•™æœ€æ–°çš„æ—¶é—´æˆ³
                        existing.properties['timestamp'] = timestamp
                        existing.properties['last_seen'] = datetime.utcnow().isoformat()
                        existing.updated_at = datetime.utcnow()
                    
                    logger.debug(f"Duplicate event detected, updated access count: {entity_id}")
                else:
                    # åˆ›å»ºæ–°è®°å½• - ä½¿ç”¨é€šç”¨å‘½å
                    entity_record = EntityMetadata(
                        entity_id=entity_id,
                        name=f"{connector_id}_{event_type}_{datetime.now().strftime('%H%M%S')}",
                        type="connector_event",  # é€šç”¨ç±»å‹
                        description=f"Event from connector {connector_id}",
                        properties=entity_properties,
                        access_count=1,
                    )
                    session.add(entity_record)
                    logger.debug(f"Created new event record: {entity_id}")

                # è®°å½•è¿æ¥å™¨æ—¥å¿—
                log_entry = ConnectorLog(
                    connector_id=connector_id,
                    level="INFO",
                    message=f"Event processed (traditional): {event_type}",
                    details={
                        "event_type": event_type,
                        "event_size": len(str(event_data)),
                        "metadata_keys": list(metadata.keys()) if metadata is not None and isinstance(metadata, dict) else [],
                        "timestamp": timestamp,
                        "processing_mode": "traditional",
                    },
                )
                session.add(log_entry)

                session.commit()

            logger.info(f"âœ… Stored generic event (traditional) from {connector_id}: {event_type}")
            return True

        except Exception as e:
            logger.error(f"Failed to store generic event (traditional): {str(e)}")
            return False

    async def _analyze_event_content(
        self, event_data: Dict[str, Any], event_type: str
    ) -> Optional[Dict[str, Any]]:
        """
        åˆ†æäº‹ä»¶å†…å®¹

        Args:
            event_data: äº‹ä»¶æ•°æ®
            event_type: äº‹ä»¶ç±»å‹

        Returns:
            å†…å®¹åˆ†æç»“æœ
        """
        try:
            # ç®€åŒ–å†…å®¹åˆ†æï¼Œé¿å…å¾ªç¯å¯¼å…¥
            content = self._extract_content_from_event(event_data)
            if not content:
                return None

            # ç¡®å®šå†…å®¹ç±»å‹
            content_type = self._determine_content_type(event_data, event_type)

            # åŸºç¡€å†…å®¹åˆ†æï¼ˆä¸ä¾èµ–å¤–éƒ¨æœåŠ¡ï¼‰
            analysis_result = {
                "content_length": len(content),
                "content_type": content_type,
                "word_count": len(content.split()) if content else 0,
                "analyzed_at": datetime.utcnow().isoformat(),
                "analysis_method": "basic_local"
            }

            logger.debug(f"åŸºç¡€å†…å®¹åˆ†æå®Œæˆ: {len(content)} å­—ç¬¦, ç±»å‹: {content_type}")
            return analysis_result

        except Exception as e:
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return None

    def _extract_content_from_event(self, event_data: Dict[str, Any]) -> Optional[str]:
        """ä»äº‹ä»¶æ•°æ®ä¸­æå–æ–‡æœ¬å†…å®¹"""
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿event_dataä¸ä¸ºNoneä¸”ä¸ºå­—å…¸ç±»å‹
        if event_data is None or not isinstance(event_data, dict):
            return None
            
        # å¸¸è§çš„å†…å®¹å­—æ®µå
        content_fields = ["content", "text", "data", "message", "body", "value"]

        for field in content_fields:
            if field in event_data:
                content = event_data[field]
                if isinstance(content, str) and content.strip():
                    return content.strip()

        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å†…å®¹å­—æ®µï¼Œå°è¯•è½¬æ¢æ•´ä¸ªäº‹ä»¶æ•°æ®ä¸ºå­—ç¬¦ä¸²
        if isinstance(event_data, dict) and len(event_data) == 1:
            # å•å­—æ®µäº‹ä»¶æ•°æ®
            value = list(event_data.values())[0]
            if isinstance(value, str) and value.strip():
                return value.strip()

        return None

    def _determine_content_type(
        self, event_data: Dict[str, Any], event_type: str
    ) -> str:
        """ç¡®å®šå†…å®¹ç±»å‹"""
        # åŸºäºäº‹ä»¶ç±»å‹æ¨æ–­
        if event_type in ["url_changed", "url_copied", "link_event"]:
            return "url"
        elif event_type in ["file_changed", "file_copied", "file_event"]:
            return "file_path"
        elif event_type in ["clipboard_changed", "content_changed"]:
            # å‰ªè´´æ¿äº‹ä»¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æå†…å®¹
            content = self._extract_content_from_event(event_data)
            if content:
                # ç®€å•å¯å‘å¼åˆ¤æ–­
                if content.startswith(("http://", "https://")):
                    return "url"
                elif "/" in content or "\\" in content:
                    return "file_path"
            return "text"

        # åŸºäºäº‹ä»¶æ•°æ®æ¨æ–­ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        if event_data is not None and isinstance(event_data, dict):
            content_type_field = event_data.get("content_type", event_data.get("type"))
            if content_type_field:
                return str(content_type_field)

        return "text"
    
    async def _handle_universal_index_event(
        self,
        connector_id: str,
        event_type: str,
        event_data: Dict[str, Any],
        timestamp: str,
        metadata: Optional[Dict[str, Any]]
    ) -> bool:
        """
        å¤„ç†é€šç”¨ç´¢å¼•äº‹ä»¶ - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹
        
        Args:
            connector_id: è¿æ¥å™¨ID
            event_type: äº‹ä»¶ç±»å‹
            event_data: äº‹ä»¶æ•°æ®
            timestamp: æ—¶é—´æˆ³
            metadata: å…ƒæ•°æ®
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.universal_index_service is None:
                logger.warning("é€šç”¨ç´¢å¼•æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡ç´¢å¼•å¤„ç†")
                return False
            
            # æ„å»ºé€šç”¨ç´¢å¼•æ¡ç›® - å®‰å…¨å¤„ç†Noneå€¼
            index_entry = {
                'connector_id': connector_id,
                'event_type': event_type,
                'event_data': event_data if event_data is not None and isinstance(event_data, dict) else {},
                'timestamp': timestamp,
                'metadata': metadata if metadata is not None and isinstance(metadata, dict) else {}
            }
            
            # æ‰¹é‡å­˜å‚¨ï¼ˆè¿™é‡Œæ˜¯å•ä¸ªæ¡ç›®ï¼Œä½†ä½¿ç”¨æ‰¹é‡æ¥å£ä»¥ä¿æŒä¸€è‡´æ€§ï¼‰
            success = await self.universal_index_service.index_content_batch([index_entry])
            
            if success:
                logger.debug(f"âœ… é€šç”¨ç´¢å¼•æ¡ç›®å·²å­˜å‚¨: {connector_id}/{event_type}")
            else:
                logger.warning(f"âŒ é€šç”¨ç´¢å¼•æ¡ç›®å­˜å‚¨å¤±è´¥: {connector_id}/{event_type}")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†é€šç”¨ç´¢å¼•äº‹ä»¶å¤±è´¥: {e}")
            return False
    
    def search_universal_index(
        self,
        query: str,
        limit: int = 100,
        content_types: List[str] = None,
        connector_ids: List[str] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        é€šç”¨ç´¢å¼•æœç´¢æ¥å£ - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: ç»“æœé™åˆ¶
            content_types: å†…å®¹ç±»å‹è¿‡æ»¤ (å¦‚ ["file_path", "url"])
            connector_ids: è¿æ¥å™¨è¿‡æ»¤ (å¦‚ ["filesystem", "clipboard"])
            **kwargs: å…¶ä»–æœç´¢å‚æ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            if self.universal_index_service is None:
                logger.warning("é€šç”¨ç´¢å¼•æœåŠ¡ä¸å¯ç”¨")
                return []
            
            # å¯¼å…¥SearchQueryå’ŒContentType
            from services.storage.universal_index_service import SearchQuery, ContentType
            
            # è½¬æ¢å†…å®¹ç±»å‹
            content_type_enums = []
            if content_types:
                for ct in content_types:
                    try:
                        content_type_enums.append(ContentType(ct))
                    except ValueError:
                        logger.warning(f"æœªçŸ¥å†…å®¹ç±»å‹: {ct}")
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = SearchQuery(
                text=query,
                content_types=content_type_enums,
                connector_ids=connector_ids or [],
                limit=limit,
                **kwargs
            )
            
            # æ‰§è¡Œæœç´¢
            search_results = self.universal_index_service.search(search_query)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            results = []
            for result in search_results:
                results.append({
                    'id': result.entry.id,
                    'connector_id': result.entry.connector_id,
                    'content_type': result.entry.content_type.value,
                    'primary_key': result.entry.primary_key,
                    'display_name': result.entry.display_name,
                    'searchable_text': result.entry.searchable_text,
                    'score': result.score,
                    'structured_data': result.entry.structured_data,
                    'metadata': result.entry.metadata,
                    'last_modified': result.entry.last_modified.isoformat() if result.entry.last_modified else None
                })
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ é€šç”¨ç´¢å¼•æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_universal_index_stats(self) -> Dict[str, Any]:
        """è·å–é€šç”¨ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.universal_index_service is None:
                return {}
            
            return self.universal_index_service.get_stats()
            
        except Exception as e:
            logger.error(f"âŒ è·å–é€šç”¨ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    # === æ™ºèƒ½æœç´¢æ¥å£ ===

    async def search_intelligent_content(
        self,
        query: str,
        k: int = 10,
        min_value_score: float = 0.0,
        content_types: Optional[List[str]] = None,
        connector_ids: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½å†…å®¹æœç´¢æ¥å£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›ç»“æœæ•°é‡
            min_value_score: æœ€å°ä»·å€¼è¯„åˆ†
            content_types: å†…å®¹ç±»å‹è¿‡æ»¤
            connector_ids: è¿æ¥å™¨IDè¿‡æ»¤
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                results = await processor.search_intelligent_content(
                    query=query,
                    k=k,
                    min_value_score=min_value_score,
                    content_types=content_types,
                    connector_ids=connector_ids,
                )
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                search_results = []
                for result in results:
                    search_results.append({
                        "id": result.id,
                        "summary": result.summary,
                        "score": result.score,
                        "value_score": result.value_score,
                        "content_type": result.content_type,
                        "timestamp": result.timestamp.isoformat() if result.timestamp else None,
                        "metadata": result.metadata,
                        "entity_id": result.entity_id,
                    })
                
                logger.info(f"æ™ºèƒ½æœç´¢å®Œæˆ: {len(search_results)} ç»“æœ")
                return search_results
            else:
                logger.warning("æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ™ºèƒ½æœç´¢")
                return []
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢å¤±è´¥: {e}")
            return []

    async def get_content_statistics(self) -> Dict[str, Any]:
        """
        è·å–å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                return await processor.get_content_statistics()
            else:
                return {"error": "æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨"}
                
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}

    async def get_high_value_content(
        self,
        min_score: float = 0.8,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """
        è·å–é«˜ä»·å€¼å†…å®¹
        
        Args:
            min_score: æœ€å°ä»·å€¼è¯„åˆ†
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            é«˜ä»·å€¼å†…å®¹åˆ—è¡¨
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                return await processor.get_content_by_value_range(
                    min_score=min_score,
                    max_score=1.0,
                    limit=limit
                )
            else:
                return []
                
        except Exception as e:
            logger.error(f"è·å–é«˜ä»·å€¼å†…å®¹å¤±è´¥: {e}")
            return []

    async def cleanup_low_value_content(self, min_age_days: int = 30) -> int:
        """
        æ¸…ç†ä½ä»·å€¼å†…å®¹
        
        Args:
            min_age_days: æœ€å°å¹´é¾„ï¼ˆå¤©ï¼‰
            
        Returns:
            æ¸…ç†çš„å†…å®¹æ•°é‡
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                cleaned_count = await processor.cleanup_low_value_content(min_age_days)
                logger.info(f"æ¸…ç†ä½ä»·å€¼å†…å®¹: {cleaned_count} é¡¹")
                return cleaned_count
            else:
                return 0
                
        except Exception as e:
            logger.error(f"æ¸…ç†ä½ä»·å€¼å†…å®¹å¤±è´¥: {e}")
            return 0

    async def get_processing_metrics(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†æ€§èƒ½æŒ‡æ ‡
        
        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        try:
            processor = await self._ensure_intelligent_processor()
            if processor:
                metrics = await processor.get_metrics()
                return {
                    "total_events": metrics.total_events,
                    "accepted_events": metrics.accepted_events,
                    "rejected_events": metrics.rejected_events,
                    "acceptance_rate": metrics.acceptance_rate,
                    "avg_processing_time_ms": metrics.avg_processing_time_ms,
                    "avg_value_score": metrics.avg_value_score,
                    "entities_extracted": metrics.entities_extracted,
                    "storage_efficiency": metrics.storage_efficiency,
                    "last_updated": metrics.last_updated.isoformat(),
                }
            else:
                return {"error": "æ™ºèƒ½å¤„ç†å™¨ä¸å¯ç”¨"}
                
        except Exception as e:
            logger.error(f"è·å–å¤„ç†æŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}


# å•ä¾‹æ¨¡å¼
_generic_storage = None


def get_generic_event_storage() -> GenericEventStorage:
    """è·å–é€šç”¨äº‹ä»¶å­˜å‚¨å®ä¾‹"""
    global _generic_storage
    if _generic_storage is None:
        _generic_storage = GenericEventStorage()
    return _generic_storage
