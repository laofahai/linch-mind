"""
æ˜Ÿç©ºå®‡å®™æ•°æ®è·¯ç”± - ä¸ºUIæ˜Ÿç©ºå¯è§†åŒ–æä¾›çœŸå®æ•°æ®æº

æä¾›Entityã€Graphã€Vectoræ•°æ®çš„ç»Ÿä¸€IPCæ¥å£ï¼Œ
æ”¯æŒæ˜Ÿç©ºå®‡å®™ç»„ä»¶æ‰€éœ€çš„æ‰€æœ‰æ•°æ®ç±»å‹ã€‚

ä½œè€…: Linch Mind å¼€å‘å›¢é˜Ÿ
åˆ›å»º: 2025-08-20
ç‰ˆæœ¬: v1.0 (æ¶æ„ç»Ÿä¸€ç‰ˆ)
"""

import asyncio
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..core.protocol import IPCRequest, IPCResponse
from ..core.router import IPCRouter
from core.service_facade import get_service
from services.storage.core.database import UnifiedDatabaseService
# from services.cached_networkx_service import CachedNetworkXService  # ä¸´æ—¶æ³¨é‡Š
from services.storage.vector_service import VectorService
from services.storage.universal_index_service import UniversalIndexService

logger = logging.getLogger(__name__)


def create_starry_universe_data_router() -> IPCRouter:
    """åˆ›å»ºæ˜Ÿç©ºå®‡å®™æ•°æ®è·¯ç”±"""
    router = IPCRouter(prefix="/starry_universe")

    # ================================
    # Entity æ•°æ®æ¥å£
    # ================================

    @router.get("/entities")
    async def get_entities(request: IPCRequest) -> IPCResponse:
        """
        è·å–å®ä½“æ•°æ® - æ™ºæ…§æ˜Ÿåº§å¯è§†åŒ–

        æŸ¥è¯¢å‚æ•°ï¼š
        {
            "limit": 100,
            "entity_types": ["file", "person", "concept"],  // å¯é€‰
            "include_relationships": true,                  // å¯é€‰
            "importance_threshold": 0.1                     // å¯é€‰ï¼Œé‡è¦æ€§è¿‡æ»¤
        }

        å“åº”æ ¼å¼ï¼š
        {
            "entities": [
                {
                    "entity_id": "entity_123",
                    "name": "é¡¹ç›®æ–‡æ¡£.md",
                    "type": "file",
                    "description": "é‡è¦é¡¹ç›®æ–‡æ¡£",
                    "properties": {...},
                    "tags": ["project", "important"],
                    "access_count": 15,
                    "last_accessed": "2025-08-20T10:00:00Z",
                    "created_at": "2025-08-01T09:00:00Z",
                    "updated_at": "2025-08-20T10:00:00Z",
                    "importance_score": 0.85
                }
            ],
            "relationships": [
                {
                    "source_entity_id": "entity_123",
                    "target_entity_id": "entity_456", 
                    "relationship_type": "related_to",
                    "strength": 8,
                    "description": "ç›¸å…³æ–‡æ¡£",
                    "properties": {...}
                }
            ],
            "total_count": 250,
            "stats": {
                "by_type": {"file": 120, "person": 80, "concept": 50},
                "total_relationships": 380
            }
        }
        """
        try:
            data = request.data or {}
            
            # å‚æ•°è§£æ
            limit = data.get("limit", 100)
            entity_types = data.get("entity_types", [])
            include_relationships = data.get("include_relationships", True)
            importance_threshold = data.get("importance_threshold", 0.0)

            logger.info(f"ğŸŒŸ è·å–å®ä½“æ•°æ®: limit={limit}, types={entity_types}")

            # è·å–æ•°æ®åº“æœåŠ¡
            db_service = get_service(UnifiedDatabaseService)
            
            # æŸ¥è¯¢å®ä½“æ•°æ®
            entities_data = await _fetch_entities_with_importance(
                db_service, limit, entity_types, importance_threshold
            )
            
            relationships_data = []
            if include_relationships:
                relationships_data = await _fetch_entity_relationships(
                    db_service, [e["entity_id"] for e in entities_data]
                )

            # ç»Ÿè®¡ä¿¡æ¯
            stats = await _calculate_entity_stats(db_service)

            logger.info(f"âœ¨ å®ä½“æ•°æ®è·å–å®Œæˆ: {len(entities_data)} entities, {len(relationships_data)} relationships")
            
            # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªå®ä½“çš„å­—æ®µç»“æ„
            if entities_data:
                logger.info(f"ğŸ” å®ä½“æ•°æ®å­—æ®µè°ƒè¯•: {list(entities_data[0].keys())}")
                logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªå®ä½“ç¤ºä¾‹: {entities_data[0]}")

            return IPCResponse.success_response({
                "entities": entities_data,
                "relationships": relationships_data,
                "total_count": len(entities_data),
                "stats": stats,
                "query_info": {
                    "limit": limit,
                    "entity_types": entity_types,
                    "include_relationships": include_relationships,
                    "importance_threshold": importance_threshold
                }
            })

        except Exception as e:
            logger.error(f"âŒ è·å–å®ä½“æ•°æ®å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "ENTITY_FETCH_ERROR", f"Failed to fetch entities: {str(e)}"
            )

    @router.get("/entities/{entity_id}")
    async def get_entity_details(request: IPCRequest) -> IPCResponse:
        """
        è·å–å•ä¸ªå®ä½“è¯¦æƒ…

        è·¯å¾„å‚æ•°ï¼š
        - entity_id: å®ä½“ID

        å“åº”æ ¼å¼ï¼š
        {
            "entity": {...},  // å®Œæ•´å®ä½“ä¿¡æ¯
            "relationships": [...],  // ç›¸å…³å…³ç³»
            "related_entities": [...],  // ç›¸å…³å®ä½“
            "activity_timeline": [...],  // æ´»åŠ¨æ—¶é—´çº¿
            "importance_breakdown": {...}  // é‡è¦æ€§åˆ†æ
        }
        """
        try:
            entity_id = request.path_params.get("entity_id")
            if not entity_id:
                return IPCResponse.error_response("INVALID_REQUEST", "Missing entity_id")

            db_service = get_service(UnifiedDatabaseService)
            
            # è·å–å®ä½“è¯¦æƒ…
            entity = await _fetch_single_entity(db_service, entity_id)
            if not entity:
                return IPCResponse.error_response("NOT_FOUND", f"Entity {entity_id} not found")

            # è·å–ç›¸å…³æ•°æ®
            relationships = await _fetch_entity_relationships(db_service, [entity_id])
            related_entities = await _fetch_related_entities(db_service, entity_id)
            
            return IPCResponse.success_response({
                "entity": entity,
                "relationships": relationships,
                "related_entities": related_entities,
                "entity_id": entity_id
            })

        except Exception as e:
            logger.error(f"âŒ è·å–å®ä½“è¯¦æƒ…å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "ENTITY_DETAIL_ERROR", f"Failed to fetch entity details: {str(e)}"
            )

    # ================================
    # Graph æ•°æ®æ¥å£
    # ================================

    @router.get("/graph")
    async def get_graph_data(request: IPCRequest) -> IPCResponse:
        """
        è·å–çŸ¥è¯†å›¾è°±æ•°æ® - çŸ¥è¯†å®‡å®™å¯è§†åŒ–

        æŸ¥è¯¢å‚æ•°ï¼š
        {
            "max_nodes": 500,
            "max_edges": 1000,
            "include_centrality": true,  // åŒ…å«ä¸­å¿ƒæ€§è®¡ç®—
            "include_clusters": true,    // åŒ…å«èšç±»ä¿¡æ¯
            "node_types": ["file", "person"],  // èŠ‚ç‚¹ç±»å‹è¿‡æ»¤
            "min_edge_weight": 0.3      // æœ€å°è¾¹æƒé‡
        }

        å“åº”æ ¼å¼ï¼š
        {
            "nodes": [
                {
                    "id": "entity_123",
                    "label": "é¡¹ç›®æ–‡æ¡£.md", 
                    "node_type": "file",
                    "attributes": {...},
                    "weight": 1.0,
                    "importance": 0.85,
                    "centrality": 0.42
                }
            ],
            "edges": [
                {
                    "source": "entity_123",
                    "target": "entity_456",
                    "edge_type": "related_to",
                    "weight": 0.75,
                    "attributes": {...}
                }
            ],
            "clusters": {
                "cluster_1": ["entity_123", "entity_456"],
                "cluster_2": ["entity_789"]
            },
            "graph_metrics": {
                "node_count": 250,
                "edge_count": 380,
                "density": 0.012,
                "average_degree": 3.04,
                "is_connected": true
            },
            "centrality_metrics": {
                "betweenness": {...},
                "closeness": {...},
                "eigenvector": {...}
            }
        }
        """
        try:
            data = request.data or {}
            
            # å‚æ•°è§£æ
            max_nodes = data.get("max_nodes", 500)
            max_edges = data.get("max_edges", 1000)
            include_centrality = data.get("include_centrality", True)
            include_clusters = data.get("include_clusters", True)
            node_types = data.get("node_types", [])
            min_edge_weight = data.get("min_edge_weight", 0.0)

            logger.info(f"ğŸŒŒ è·å–å›¾è°±æ•°æ®: max_nodes={max_nodes}, include_centrality={include_centrality}")

            # è·å–NetworkXæœåŠ¡ (ä¸´æ—¶è·³è¿‡)
            # graph_service = get_service(CachedNetworkXService)
            
            # ç¡®ä¿å›¾è°±å·²åŠ è½½ (ä¸´æ—¶è·³è¿‡)
            # await graph_service.load_optimized_graph()
            
            # è·å–å›¾æ•°æ®
            graph_data = await _extract_graph_visualization_data(
                None, max_nodes, max_edges, node_types, min_edge_weight
            )
            
            # è®¡ç®—ä¸­å¿ƒæ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰
            centrality_metrics = {}
            if include_centrality:
                centrality_metrics = await _calculate_centrality_metrics(None)  # ä¸´æ—¶ä¼ None
            
            # è·å–èšç±»ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            clusters = {}
            if include_clusters:
                clusters = await _get_graph_clusters(None)  # ä¸´æ—¶ä¼ None

            logger.info(f"ğŸ¯ å›¾è°±æ•°æ®è·å–å®Œæˆ: {len(graph_data['nodes'])} nodes, {len(graph_data['edges'])} edges")

            return IPCResponse.success_response({
                **graph_data,
                "clusters": clusters,
                "centrality_metrics": centrality_metrics,
                "query_info": {
                    "max_nodes": max_nodes,
                    "max_edges": max_edges,
                    "include_centrality": include_centrality,
                    "include_clusters": include_clusters,
                    "node_types": node_types,
                    "min_edge_weight": min_edge_weight
                }
            })

        except Exception as e:
            logger.error(f"âŒ è·å–å›¾è°±æ•°æ®å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "GRAPH_FETCH_ERROR", f"Failed to fetch graph data: {str(e)}"
            )

    # ================================
    # Vector æ•°æ®æ¥å£
    # ================================

    @router.get("/vectors")
    async def get_vector_data(request: IPCRequest) -> IPCResponse:
        """
        è·å–å‘é‡æ•°æ® - ç›¸ä¼¼æ˜Ÿäº‘å¯è§†åŒ–

        æŸ¥è¯¢å‚æ•°ï¼š
        {
            "limit": 200,
            "include_embeddings": false,  // æ˜¯å¦åŒ…å«å®Œæ•´å‘é‡
            "cluster_count": 10,          // èšç±»æ•°é‡
            "similarity_threshold": 0.7,  // ç›¸ä¼¼åº¦é˜ˆå€¼
            "content_types": ["text", "document"]  // å†…å®¹ç±»å‹
        }

        å“åº”æ ¼å¼ï¼š
        {
            "documents": [
                {
                    "document_id": "doc_123",
                    "content": "æ–‡æ¡£å†…å®¹...",
                    "title": "æ–‡æ¡£æ ‡é¢˜",
                    "content_type": "text",
                    "metadata": {...},
                    "embedding_preview": [0.1, 0.2, ...],  // å‰10ç»´
                    "score": 0.95,
                    "source_connector": "connector_id",
                    "timestamp": "2025-08-20T10:00:00Z"
                }
            ],
            "clusters": [
                {
                    "cluster_id": "cluster_1", 
                    "document_ids": ["doc_123", "doc_456"],
                    "centroid_preview": [0.15, 0.25, ...],
                    "cohesion": 0.85,
                    "topic": "é¡¹ç›®ç®¡ç†",
                    "keywords": ["é¡¹ç›®", "ç®¡ç†", "è®¡åˆ’"]
                }
            ],
            "similarity_matrix": {
                "doc_123": {"doc_456": 0.89, "doc_789": 0.76}
            },
            "vector_stats": {
                "total_documents": 1500,
                "embedding_dimension": 384,
                "avg_similarity": 0.45,
                "cluster_count": 10
            }
        }
        """
        try:
            data = request.data or {}
            
            # å‚æ•°è§£æ
            limit = data.get("limit", 200)
            include_embeddings = data.get("include_embeddings", False)
            cluster_count = data.get("cluster_count", 10)
            similarity_threshold = data.get("similarity_threshold", 0.7)
            content_types = data.get("content_types", [])

            logger.info(f"ğŸ’« è·å–å‘é‡æ•°æ®: limit={limit}, clusters={cluster_count}")

            # è·å–å‘é‡æœåŠ¡
            vector_service = await _get_vector_service()
            
            # è·å–å‘é‡æ–‡æ¡£
            documents = await _fetch_vector_documents(
                vector_service, limit, content_types, include_embeddings
            )
            
            # æ‰§è¡Œèšç±»
            clusters = await _perform_vector_clustering(
                vector_service, documents, cluster_count
            )
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä»…é™é«˜ç›¸ä¼¼åº¦å¯¹ï¼‰
            similarity_matrix = await _calculate_similarity_matrix(
                vector_service, documents, similarity_threshold
            )
            
            # å‘é‡ç»Ÿè®¡
            vector_stats = await _get_vector_stats(vector_service)

            logger.info(f"ğŸŒŸ å‘é‡æ•°æ®è·å–å®Œæˆ: {len(documents)} documents, {len(clusters)} clusters")

            return IPCResponse.success_response({
                "documents": documents,
                "clusters": clusters,
                "similarity_matrix": similarity_matrix,
                "vector_stats": vector_stats,
                "query_info": {
                    "limit": limit,
                    "include_embeddings": include_embeddings,
                    "cluster_count": cluster_count,
                    "similarity_threshold": similarity_threshold,
                    "content_types": content_types
                }
            })

        except Exception as e:
            logger.error(f"âŒ è·å–å‘é‡æ•°æ®å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "VECTOR_FETCH_ERROR", f"Failed to fetch vector data: {str(e)}"
            )

    @router.post("/vectors/search")
    async def vector_search(request: IPCRequest) -> IPCResponse:
        """
        å‘é‡è¯­ä¹‰æœç´¢
        
        è¯·æ±‚æ ¼å¼ï¼š
        {
            "query": "æœç´¢æ–‡æœ¬",
            "limit": 20,
            "similarity_threshold": 0.7
        }
        """
        try:
            data = request.data or {}
            
            query = data.get("query", "")
            if not query.strip():
                return IPCResponse.error_response("INVALID_REQUEST", "Query cannot be empty")
                
            limit = data.get("limit", 20) 
            similarity_threshold = data.get("similarity_threshold", 0.7)

            vector_service = await _get_vector_service()
            
            # æ‰§è¡Œå‘é‡æœç´¢
            search_results = await _perform_vector_search(
                vector_service, query, limit, similarity_threshold
            )

            return IPCResponse.success_response({
                "query": query,
                "results": search_results,
                "total_results": len(search_results),
                "similarity_threshold": similarity_threshold
            })

        except Exception as e:
            logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "VECTOR_SEARCH_ERROR", f"Vector search failed: {str(e)}"
            )

    # ================================
    # ç»Ÿä¸€æ•°æ®æ¥å£
    # ================================

    @router.get("/unified")
    async def get_unified_starry_data(request: IPCRequest) -> IPCResponse:
        """
        è·å–æ˜Ÿç©ºå®‡å®™ç»Ÿä¸€æ•°æ® - ä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ•°æ®ç±»å‹

        æŸ¥è¯¢å‚æ•°ï¼š
        {
            "include_entities": true,
            "include_graph": true,
            "include_vectors": true,
            "include_universal_index": true,
            "data_limits": {
                "entities": 100,
                "graph_nodes": 300,
                "vector_docs": 150,
                "index_entries": 200
            }
        }

        å“åº”æ ¼å¼ï¼š
        {
            "entities": {...},          // Entityæ•°æ®
            "graph": {...},             // Graphæ•°æ®  
            "vectors": {...},           // Vectoræ•°æ®
            "universal_index": {...},   // UniversalIndexæ•°æ®
            "data_summary": {
                "total_entities": 100,
                "total_nodes": 300,
                "total_vectors": 150,
                "total_index_entries": 200,
                "fetch_time_ms": 245
            }
        }
        """
        try:
            data = request.data or {}
            start_time = datetime.now()
            
            # å‚æ•°è§£æ
            include_entities = data.get("include_entities", True)
            include_graph = data.get("include_graph", True)
            include_vectors = data.get("include_vectors", True)
            include_universal_index = data.get("include_universal_index", True)
            
            data_limits = data.get("data_limits", {})
            
            logger.info("ğŸŒ  è·å–æ˜Ÿç©ºå®‡å®™ç»Ÿä¸€æ•°æ®")

            # å¹¶è¡Œè·å–å„ç§æ•°æ®
            tasks = []
            result_data = {}

            if include_entities:
                tasks.append(_fetch_entities_for_unified(data_limits.get("entities", 100)))
            if include_graph:
                tasks.append(_fetch_graph_for_unified(data_limits.get("graph_nodes", 300)))
            if include_vectors:
                tasks.append(_fetch_vectors_for_unified(data_limits.get("vector_docs", 150)))
            if include_universal_index:
                tasks.append(_fetch_index_for_unified(data_limits.get("index_entries", 200)))

            # å¹¶è¡Œæ‰§è¡Œ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            data_types = []
            if include_entities: data_types.append("entities")
            if include_graph: data_types.append("graph")
            if include_vectors: data_types.append("vectors")
            if include_universal_index: data_types.append("universal_index")
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ è·å–{data_types[i]}æ•°æ®å¤±è´¥: {str(result)}")
                    result_data[data_types[i]] = {"error": str(result)}
                else:
                    result_data[data_types[i]] = result

            # è®¡ç®—ç»Ÿè®¡
            fetch_time_ms = (datetime.now() - start_time).total_seconds() * 1000
            data_summary = {
                "fetch_time_ms": round(fetch_time_ms, 2),
                "included_data_types": data_types,
                "data_limits": data_limits
            }

            # æ·»åŠ å„æ•°æ®ç±»å‹çš„ç»Ÿè®¡
            for data_type, data_content in result_data.items():
                if "error" not in data_content:
                    if data_type == "entities":
                        data_summary["total_entities"] = len(data_content.get("entities", []))
                    elif data_type == "graph":
                        data_summary["total_nodes"] = len(data_content.get("nodes", []))
                        data_summary["total_edges"] = len(data_content.get("edges", []))
                    elif data_type == "vectors":
                        data_summary["total_vectors"] = len(data_content.get("documents", []))
                    elif data_type == "universal_index":
                        data_summary["total_index_entries"] = len(data_content.get("results", []))

            logger.info(f"âœ¨ æ˜Ÿç©ºå®‡å®™ç»Ÿä¸€æ•°æ®è·å–å®Œæˆ: {fetch_time_ms:.1f}ms")

            return IPCResponse.success_response({
                **result_data,
                "data_summary": data_summary
            })

        except Exception as e:
            logger.error(f"âŒ è·å–æ˜Ÿç©ºå®‡å®™ç»Ÿä¸€æ•°æ®å¤±è´¥: {str(e)}")
            return IPCResponse.error_response(
                "UNIFIED_FETCH_ERROR", f"Failed to fetch unified starry data: {str(e)}"
            )

    return router


# ================================
# è¾…åŠ©å‡½æ•°
# ================================

async def _fetch_entities_with_importance(
    db_service, limit: int, entity_types: List[str], importance_threshold: float
) -> List[Dict[str, Any]]:
    """è·å–å¸¦é‡è¦æ€§è¯„åˆ†çš„å®ä½“æ•°æ®"""
    try:
        # ä½¿ç”¨çœŸå®çš„é€šç”¨ç´¢å¼•æœåŠ¡è·å–æ•°æ®
        # å·²ä¿®å¤ï¼šä½¿ç”¨ServiceFacadeè·å–UniversalIndexService
        from services.storage.universal_index_service import SearchQuery, ContentType, IndexTier
        
        index_service = get_service(UniversalIndexService)
        
        # æ„å»ºæœç´¢æŸ¥è¯¢ - è·å–æ‰€æœ‰ç±»å‹çš„å†…å®¹ä½œä¸ºå®ä½“
        content_type_mapping = {
            "file": ContentType.FILE_PATH,
            "document": ContentType.DOCUMENT,
            "contact": ContentType.CONTACT,
            "text": ContentType.TEXT,
            "person": ContentType.CONTACT,
            "concept": ContentType.TEXT
        }
        
        # è½¬æ¢entity_typesåˆ°ContentType
        search_content_types = []
        if entity_types:
            for entity_type in entity_types:
                if entity_type in content_type_mapping:
                    search_content_types.append(content_type_mapping[entity_type])
        else:
            # é»˜è®¤åŒ…å«æ‰€æœ‰ä¸»è¦ç±»å‹
            search_content_types = [ContentType.FILE_PATH, ContentType.DOCUMENT, ContentType.CONTACT, ContentType.TEXT]
        
        # æ‰§è¡Œæœç´¢æŸ¥è¯¢
        query = SearchQuery(
            text="*",  # è·å–æ‰€æœ‰å†…å®¹
            content_types=search_content_types,
            index_tiers=[IndexTier.HOT, IndexTier.WARM],  # ä¼˜å…ˆé«˜æ€§èƒ½æ•°æ®
            limit=limit,
            include_metadata=True
        )
        
        search_results = index_service.search(query)
        
        # è½¬æ¢ä¸ºå®ä½“æ ¼å¼
        entities = []
        for result in search_results:
            # ä¿®å¤ï¼šè®¿é—®result.entryä¸­çš„å±æ€§
            entry = result.entry
            
            # è®¡ç®—é‡è¦æ€§è¯„åˆ† (åŸºäºä¼˜å…ˆçº§å’Œè®¿é—®é¢‘ç‡)
            importance = 1.0 - (entry.priority / 10.0)  # priority 1-10 è½¬æ¢ä¸º 0.9-0.0
            if hasattr(entry, 'last_accessed') and entry.last_accessed:
                # æœ€è¿‘è®¿é—®çš„å†…å®¹é‡è¦æ€§æ›´é«˜
                days_since_access = (datetime.now() - entry.last_accessed).days
                importance += max(0, 1.0 - days_since_access / 30.0) * 0.2
            
            if importance < importance_threshold:
                continue
                
            entity = {
                "entity_id": entry.id,
                "name": entry.display_name,
                "type": entry.content_type.value,
                "description": entry.searchable_text[:200] + "..." if len(entry.searchable_text) > 200 else entry.searchable_text,
                "properties": entry.structured_data or {},
                "tags": list(entry.tags) if entry.tags else [],
                "access_count": entry.metadata.get("access_count", 1) if entry.metadata else 1,
                "last_accessed": entry.last_accessed.isoformat() if entry.last_accessed else None,
                "created_at": entry.indexed_at.isoformat(),
                "updated_at": entry.last_modified.isoformat() if entry.last_modified else entry.indexed_at.isoformat(),
                "importance_score": round(importance, 3),
                "primary_key": entry.primary_key,
                "connector_id": entry.connector_id
            }
            entities.append(entity)
        
        logger.info(f"è·å–åˆ° {len(entities)} ä¸ªçœŸå®å®ä½“æ•°æ®")
        return entities
        
    except Exception as e:
        logger.error(f"è·å–çœŸå®å®ä½“æ•°æ®å¤±è´¥: {e}")
        # é™çº§è¿”å›åŸºç¡€æ•°æ®
        return []

async def _fetch_entity_relationships(
    db_service, entity_ids: List[str]
) -> List[Dict[str, Any]]:
    """è·å–å®ä½“å…³ç³»æ•°æ®"""
    try:
        # åŸºäºå®ä½“çš„connector_idå’Œå†…å®¹ç±»å‹æ„å»ºåŸºç¡€å…³ç³»
        relationships = []
        
        # åˆ†æentity_idsä¸­çš„æ¨¡å¼æ¥æ¨æ–­å…³ç³»
        connectors = {}
        content_types = {}
        
        for entity_id in entity_ids:
            parts = entity_id.split(":")
            if len(parts) >= 2:
                connector_id = parts[0]
                content_type = parts[1] if len(parts) > 1 else "unknown"
                
                if connector_id not in connectors:
                    connectors[connector_id] = []
                connectors[connector_id].append(entity_id)
                
                if content_type not in content_types:
                    content_types[content_type] = []
                content_types[content_type].append(entity_id)
        
        # åˆ›å»ºåŒè¿æ¥å™¨çš„å®ä½“å…³ç³»
        for connector_id, entities in connectors.items():
            if len(entities) > 1:
                for i, source in enumerate(entities):
                    for target in entities[i+1:]:
                        relationships.append({
                            "source_entity_id": source,
                            "target_entity_id": target,
                            "relationship_type": "same_connector",
                            "strength": 5,
                            "description": f"æ¥è‡ªç›¸åŒè¿æ¥å™¨: {connector_id}",
                            "properties": {
                                "connector_id": connector_id,
                                "relationship_category": "structural"
                            }
                        })
        
        # åˆ›å»ºåŒç±»å‹çš„å®ä½“å…³ç³»
        for content_type, entities in content_types.items():
            if len(entities) > 1:
                for i, source in enumerate(entities):
                    for target in entities[i+1:]:
                        relationships.append({
                            "source_entity_id": source,
                            "target_entity_id": target,
                            "relationship_type": "same_type",
                            "strength": 3,
                            "description": f"ç›¸åŒå†…å®¹ç±»å‹: {content_type}",
                            "properties": {
                                "content_type": content_type,
                                "relationship_category": "semantic"
                            }
                        })
        
        logger.info(f"ç”Ÿæˆäº† {len(relationships)} ä¸ªå®ä½“å…³ç³»")
        return relationships[:100]  # é™åˆ¶å…³ç³»æ•°é‡
        
    except Exception as e:
        logger.error(f"è·å–å®ä½“å…³ç³»å¤±è´¥: {e}")
        return []

async def _calculate_entity_stats(db_service) -> Dict[str, Any]:
    """è®¡ç®—å®ä½“ç»Ÿè®¡ä¿¡æ¯"""
    # TODO: å®ç°ç»Ÿè®¡è®¡ç®—
    return {"by_type": {}, "total_relationships": 0}

async def _fetch_single_entity(db_service, entity_id: str) -> Optional[Dict[str, Any]]:
    """è·å–å•ä¸ªå®ä½“"""
    # TODO: å®ç°å•å®ä½“æŸ¥è¯¢
    return None

async def _fetch_related_entities(db_service, entity_id: str) -> List[Dict[str, Any]]:
    """è·å–ç›¸å…³å®ä½“"""
    # TODO: å®ç°ç›¸å…³å®ä½“æŸ¥è¯¢
    return []

async def _extract_graph_visualization_data(
    graph_service, max_nodes: int, max_edges: int, 
    node_types: List[str], min_edge_weight: float
) -> Dict[str, Any]:
    """æå–å›¾å¯è§†åŒ–æ•°æ®"""
    try:
        # è·å–å›¾çš„åŸºæœ¬ä¿¡æ¯
        if hasattr(graph_service, 'get_graph_stats'):
            stats = await graph_service.get_graph_stats()
        else:
            stats = {"node_count": 0, "edge_count": 0}
        
        # å¦‚æœæ²¡æœ‰å›¾æ•°æ®ï¼Œç”ŸæˆåŸºç¡€ç»“æ„
        nodes = []
        edges = []
        
        # ä»å®ä½“æ•°æ®ç”ŸæˆèŠ‚ç‚¹
        # å·²ä¿®å¤ï¼šä½¿ç”¨ServiceFacadeè·å–UniversalIndexService
        from services.storage.universal_index_service import SearchQuery, ContentType, IndexTier
        
        index_service = get_service(UniversalIndexService)
        query = SearchQuery(
            text="*",
            content_types=[ContentType.FILE_PATH, ContentType.DOCUMENT, ContentType.TEXT],
            index_tiers=[IndexTier.HOT, IndexTier.WARM],
            limit=min(max_nodes, 50),
            include_metadata=True
        )
        
        search_results = index_service.search(query)
        
        for i, result in enumerate(search_results):
            # ä¿®å¤ï¼šè®¿é—®result.entryä¸­çš„å±æ€§
            entry = result.entry
            
            node = {
                "id": entry.id,
                "label": entry.display_name,
                "node_type": entry.content_type.value,
                "attributes": entry.structured_data or {},
                "weight": 1.0 - (entry.priority / 10.0),
                "importance": 1.0 - (entry.priority / 10.0),
                "centrality": min(1.0, (i + 1) / len(search_results))
            }
            nodes.append(node)
        
        # åŸºäºç›¸ä¼¼æ€§ç”Ÿæˆè¾¹
        for i, source_node in enumerate(nodes):
            for j, target_node in enumerate(nodes[i+1:], i+1):
                # ç®€å•çš„ç›¸ä¼¼æ€§è®¡ç®—
                weight = 0.5 if source_node["node_type"] == target_node["node_type"] else 0.3
                
                if weight >= min_edge_weight and len(edges) < max_edges:
                    edge = {
                        "source": source_node["id"],
                        "target": target_node["id"],
                        "edge_type": "similarity",
                        "weight": weight,
                        "attributes": {
                            "similarity_type": "content_type" if source_node["node_type"] == target_node["node_type"] else "structural"
                        }
                    }
                    edges.append(edge)
        
        graph_metrics = {
            "node_count": len(nodes),
            "edge_count": len(edges),
            "density": len(edges) / max(1, len(nodes) * (len(nodes) - 1) / 2),
            "average_degree": (len(edges) * 2) / max(1, len(nodes)),
            "is_connected": len(nodes) > 0
        }
        
        logger.info(f"ç”Ÿæˆäº†å›¾è°±æ•°æ®: {len(nodes)} èŠ‚ç‚¹, {len(edges)} è¾¹")
        
        return {
            "nodes": nodes,
            "edges": edges,
            "graph_metrics": graph_metrics
        }
        
    except Exception as e:
        logger.error(f"æå–å›¾å¯è§†åŒ–æ•°æ®å¤±è´¥: {e}")
        return {"nodes": [], "edges": [], "graph_metrics": {}}

async def _calculate_centrality_metrics(graph_service) -> Dict[str, Any]:
    """è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡"""
    # TODO: å®ç°ä¸­å¿ƒæ€§è®¡ç®—
    return {}

async def _get_graph_clusters(graph_service) -> Dict[str, List[str]]:
    """è·å–å›¾èšç±»"""
    # TODO: å®ç°å›¾èšç±»
    return {}

async def _get_vector_service():
    """è·å–å‘é‡æœåŠ¡å®ä¾‹"""
    # TODO: å®ç°å‘é‡æœåŠ¡è·å–
    pass

async def _fetch_vector_documents(
    vector_service, limit: int, content_types: List[str], include_embeddings: bool
) -> List[Dict[str, Any]]:
    """è·å–å‘é‡æ–‡æ¡£"""
    try:
        # ä½¿ç”¨é€šç”¨ç´¢å¼•æœåŠ¡è·å–æ–‡æ¡£æ•°æ®
        # å·²ä¿®å¤ï¼šä½¿ç”¨ServiceFacadeè·å–UniversalIndexService
        from services.storage.universal_index_service import SearchQuery, ContentType, IndexTier
        
        index_service = get_service(UniversalIndexService)
        
        # ä¼˜å…ˆè·å–æ–‡æ¡£å’Œæ–‡æœ¬ç±»å‹çš„å†…å®¹
        search_content_types = [ContentType.DOCUMENT, ContentType.TEXT, ContentType.FILE_PATH]
        
        query = SearchQuery(
            text="*",
            content_types=search_content_types,
            index_tiers=[IndexTier.WARM, IndexTier.COLD],  # å‘é‡æœç´¢ç”¨è¾ƒæ·±çš„å±‚çº§
            limit=limit,
            include_metadata=True
        )
        
        search_results = index_service.search(query)
        
        documents = []
        for result in search_results:
            # ä¿®å¤ï¼šè®¿é—®result.entryä¸­çš„å±æ€§
            entry = result.entry
            
            doc = {
                "document_id": entry.id,
                "content": entry.searchable_text,
                "title": entry.display_name,
                "content_type": entry.content_type.value,
                "metadata": entry.metadata or {},
                "embedding_preview": [0.1, 0.2, 0.3, 0.15, 0.25],  # æ¨¡æ‹Ÿå‰5ç»´
                "score": 1.0 - (entry.priority / 10.0),
                "source_connector": entry.connector_id,
                "timestamp": entry.indexed_at.isoformat()
            }
            
            if include_embeddings:
                # å¦‚æœéœ€è¦å®Œæ•´å‘é‡ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
                import random
                doc["full_embedding"] = [random.uniform(-1, 1) for _ in range(384)]
            
            documents.append(doc)
        
        logger.info(f"è·å–äº† {len(documents)} ä¸ªå‘é‡æ–‡æ¡£")
        return documents
        
    except Exception as e:
        logger.error(f"è·å–å‘é‡æ–‡æ¡£å¤±è´¥: {e}")
        return []

async def _perform_vector_clustering(
    vector_service, documents: List[Dict], cluster_count: int
) -> List[Dict[str, Any]]:
    """æ‰§è¡Œå‘é‡èšç±»"""
    # TODO: å®ç°å‘é‡èšç±»
    return []

async def _calculate_similarity_matrix(
    vector_service, documents: List[Dict], threshold: float
) -> Dict[str, Dict[str, float]]:
    """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
    # TODO: å®ç°ç›¸ä¼¼åº¦è®¡ç®—
    return {}

async def _get_vector_stats(vector_service) -> Dict[str, Any]:
    """è·å–å‘é‡ç»Ÿè®¡"""
    # TODO: å®ç°å‘é‡ç»Ÿè®¡
    return {}

async def _perform_vector_search(
    vector_service, query: str, limit: int, threshold: float
) -> List[Dict[str, Any]]:
    """æ‰§è¡Œå‘é‡æœç´¢"""
    # TODO: å®ç°å‘é‡æœç´¢
    return []

async def _fetch_entities_for_unified(limit: int) -> Dict[str, Any]:
    """ä¸ºç»Ÿä¸€æ¥å£è·å–å®ä½“æ•°æ®"""
    # TODO: å®ç°ç»Ÿä¸€å®ä½“è·å–
    return {"entities": [], "relationships": []}

async def _fetch_graph_for_unified(max_nodes: int) -> Dict[str, Any]:
    """ä¸ºç»Ÿä¸€æ¥å£è·å–å›¾æ•°æ®"""
    # TODO: å®ç°ç»Ÿä¸€å›¾è·å–
    return {"nodes": [], "edges": []}

async def _fetch_vectors_for_unified(limit: int) -> Dict[str, Any]:
    """ä¸ºç»Ÿä¸€æ¥å£è·å–å‘é‡æ•°æ®"""
    # TODO: å®ç°ç»Ÿä¸€å‘é‡è·å–
    return {"documents": [], "clusters": []}

async def _fetch_index_for_unified(limit: int) -> Dict[str, Any]:
    """ä¸ºç»Ÿä¸€æ¥å£è·å–ç´¢å¼•æ•°æ®"""
    # TODO: å®ç°ç»Ÿä¸€ç´¢å¼•è·å–
    return {"results": []}