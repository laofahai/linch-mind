#!/usr/bin/env python3
"""
æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨ - AIé©±åŠ¨çš„äº‹ä»¶å¤„ç†æµæ°´çº¿
æ›¿ä»£GenericEventStorageï¼Œå®ç°åŸºäºOllama AIçš„æ™ºèƒ½å†…å®¹è¿‡æ»¤å’Œå­˜å‚¨
"""

import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

from core.error_handling import handle_errors, ErrorSeverity, ErrorCategory
from core.service_facade import get_service
from services.ai.ollama_service import OllamaService, ContentEvaluation
from services.storage.faiss_vector_store import (
    get_faiss_vector_store, 
    VectorDocument,
    SearchResult
)
from services.storage.core.database import UnifiedDatabaseService
from models.database_models import EntityMetadata, ConnectorLog, EventCorrelation
from config.intelligent_storage import get_intelligent_storage_config
from services.event_correlation.ai_driven_correlator import get_ai_correlator

logger = logging.getLogger(__name__)


@dataclass
class ProcessingResult:
    """äº‹ä»¶å¤„ç†ç»“æœ"""
    accepted: bool
    value_score: float
    summary: str
    reasoning: str
    entities_created: int = 0
    vector_stored: bool = False
    database_stored: bool = False
    processing_time_ms: float = 0.0
    # AIå…³è”ä¿¡æ¯
    ai_semantic_tags: List[Dict[str, Any]] = None
    ai_correlations: List[Dict[str, Any]] = None
    ai_insights_available: bool = False


@dataclass
class ProcessingMetrics:
    """å¤„ç†æ€§èƒ½æŒ‡æ ‡"""
    total_events: int
    accepted_events: int
    rejected_events: int
    avg_processing_time_ms: float
    avg_value_score: float
    acceptance_rate: float
    entities_extracted: int
    storage_efficiency: float
    last_updated: datetime


class IntelligentEventProcessor:
    """AIé©±åŠ¨çš„æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨ - å¢å¼ºéŸ§æ€§ç‰ˆæœ¬
    
    ç‰¹æ€§:
    - æ™ºèƒ½é™çº§ç­–ç•¥ï¼šAIæœåŠ¡ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°è§„åˆ™å¤„ç†
    - éƒ¨åˆ†åŠŸèƒ½ä¿æŒï¼šå‘é‡å­˜å‚¨ã€å®ä½“æå–ç­‰åŠŸèƒ½ç‹¬ç«‹è¿è¡Œ
    - è‡ªé€‚åº”é˜ˆå€¼ï¼šæ ¹æ®æœåŠ¡çŠ¶æ€åŠ¨æ€è°ƒæ•´å¤„ç†é˜ˆå€¼
    - æ€§èƒ½ç›‘æ§ï¼šå®æ—¶è·Ÿè¸ªå¤„ç†æ•ˆæœå’ŒæœåŠ¡å¥åº·åº¦
    """

    def __init__(
        self,
        value_threshold: Optional[float] = None,
        entity_threshold: Optional[float] = None,
        max_content_length: Optional[int] = None,
        enable_vector_storage: Optional[bool] = None,
        enable_entity_extraction: Optional[bool] = None,
    ):
        # ä»ç»Ÿä¸€é…ç½®è·å–å‚æ•°
        config = get_intelligent_storage_config()
        
        self.value_threshold = value_threshold if value_threshold is not None else 0.3
        self.entity_threshold = entity_threshold if entity_threshold is not None else 0.8
        self.max_content_length = max_content_length if max_content_length is not None else 10000
        self.enable_vector_storage = enable_vector_storage if enable_vector_storage is not None else True
        self.enable_entity_extraction = enable_entity_extraction if enable_entity_extraction is not None else True
        
        # é™çº§ç­–ç•¥é…ç½®
        self.enable_fallback_processing = True
        self.fallback_value_threshold = 0.1
        self.fallback_accept_rate = 0.5
        self.enable_partial_processing = True
        
        # æœåŠ¡ä¾èµ– - æ‡’åŠ è½½
        self._ollama_service = None
        self._vector_store = None
        self._db_service = None
        self._ai_correlator = None
        
        # å¤„ç†æ¨¡å¼çŠ¶æ€
        self._processing_mode = "unknown"  # ai, hybrid, fallback
        self._last_mode_check = datetime.utcnow()
        self._mode_check_interval = 60  # 60ç§’æ£€æŸ¥ä¸€æ¬¡æ¨¡å¼
        
        # æ€§èƒ½ç›‘æ§
        self._processing_times: List[float] = []
        self._value_scores: List[float] = []
        self._max_history = 100
        
        self._metrics = ProcessingMetrics(
            total_events=0,
            accepted_events=0,
            rejected_events=0,
            avg_processing_time_ms=0.0,
            avg_value_score=0.0,
            acceptance_rate=0.0,
            entities_extracted=0,
            storage_efficiency=0.0,
            last_updated=datetime.utcnow(),
        )

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨ï¼ˆå¢å¼ºéŸ§æ€§ç‰ˆæœ¬ï¼‰"""
        try:
            # åˆå§‹åŒ–éŸ§æ€§OllamaæœåŠ¡ - å¿…é¡»ä¾èµ–æ¨¡å¼
            try:
                self._ollama_service = get_service(OllamaService)
                if not await self._ollama_service.initialize():
                    raise RuntimeError("OllamaæœåŠ¡åˆå§‹åŒ–å¤±è´¥")
                if self._ollama_service.is_available():
                    self._processing_mode = "ai"
                    logger.info("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨ï¼šAIæ¨¡å¼ - OllamaæœåŠ¡å¯ç”¨")
                else:
                    self._processing_mode = "waiting"
                    logger.warning("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨ï¼šç­‰å¾…æ¨¡å¼ - OllamaæœåŠ¡é‡è¿ä¸­")
                    # ä¸è®¾ç½®ä¸ºé™çº§æ¨¡å¼ï¼Œè€Œæ˜¯ç­‰å¾…é‡è¿
            except Exception as e:
                logger.error(f"OllamaæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                self._ollama_service = None
                self._processing_mode = "waiting"
                logger.error("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨ï¼šç­‰å¾…æ¨¡å¼ - éœ€è¦AIæœåŠ¡æ”¯æŒ")
            
            # å‘é‡å­˜å‚¨åˆå§‹åŒ–ï¼ˆç‹¬ç«‹äºAIæœåŠ¡ï¼‰
            if self.enable_vector_storage:
                try:
                    self._vector_store = await get_faiss_vector_store()
                    logger.info("å‘é‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥ï¼Œç¦ç”¨å‘é‡å­˜å‚¨: {e}")
                    self.enable_vector_storage = False
                    self._vector_store = None
            
            # æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œé¿å…å¾ªç¯ä¾èµ–ï¼‰
            self._db_service = None
            logger.info("æ•°æ®åº“æœåŠ¡å°†å»¶è¿ŸåŠ è½½")
            
            # è¾“å‡ºåˆå§‹åŒ–æ‘˜è¦
            mode_desc = {
                "ai": "å®Œæ•´AIå¤„ç†æ¨¡å¼",
                "waiting": "ç­‰å¾…AIæœåŠ¡æ¨¡å¼ï¼ˆæ™ºèƒ½åŠŸèƒ½æš‚åœï¼‰"
            }
            
            logger.info(f"æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å¼: {mode_desc.get(self._processing_mode, 'æœªçŸ¥')}")
            logger.info(f"é…ç½®: é˜ˆå€¼={self.value_threshold}, å‘é‡å­˜å‚¨={self.enable_vector_storage}, å®ä½“æå–={self.enable_entity_extraction}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _get_db_service(self):
        """æ‡’åŠ è½½æ•°æ®åº“æœåŠ¡ï¼Œé¿å…å¾ªç¯ä¾èµ–"""
        if self._db_service is None:
            try:
                self._db_service = get_service(UnifiedDatabaseService)
                logger.debug("æ•°æ®åº“æœåŠ¡å»¶è¿ŸåŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"æ•°æ®åº“æœåŠ¡å»¶è¿ŸåŠ è½½å¤±è´¥: {e}")
                return None
        return self._db_service
    
    def _get_ai_correlator(self):
        """æ‡’åŠ è½½AIå…³è”å™¨"""
        if self._ai_correlator is None:
            try:
                self._ai_correlator = get_ai_correlator()
                logger.debug("AIå…³è”å™¨å»¶è¿ŸåŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"AIå…³è”å™¨å»¶è¿ŸåŠ è½½å¤±è´¥: {e}")
                return None
        return self._ai_correlator

    # === æ ¸å¿ƒå¤„ç†æµæ°´çº¿ ===

    @handle_errors(
        severity=ErrorSeverity.HIGH,
        category=ErrorCategory.EVENT_PROCESSING,
        user_message="æ™ºèƒ½äº‹ä»¶å¤„ç†å¤±è´¥",
        recovery_suggestions="æ£€æŸ¥AIæœåŠ¡çŠ¶æ€"
    )
    async def process_connector_event(
        self,
        connector_id: str,
        event_type: str,
        event_data: Dict[str, Any],
        timestamp: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> ProcessingResult:
        """
        æ™ºèƒ½å¤„ç†è¿æ¥å™¨äº‹ä»¶ - å¿…é¡»ä¾èµ–AIæœåŠ¡çš„è®¾è®¡
        
        æ ¸å¿ƒåŸåˆ™ï¼š
        - AIè¯„ä¼°æ˜¯å¿…é¡»åŠŸèƒ½ï¼Œä¸æ˜¯å¯é€‰é¡¹
        - Ollamaä¸å¯ç”¨æ—¶é˜»å¡å¤„ç†ï¼Œç­‰å¾…é‡è¿
        - æ˜ç¡®æç¤ºç”¨æˆ·AIæœåŠ¡çŠ¶æ€
        - ä¿è¯æ™ºèƒ½åŠ©æ‰‹çš„"æ™ºèƒ½"å±æ€§
        
        æµç¨‹ï¼š
        1. å†…å®¹æå–å’Œé¢„å¤„ç†
        2. AIæœåŠ¡å¯ç”¨æ€§æ£€æŸ¥ (å¿…é¡»é€šè¿‡)
        3. AIä»·å€¼è¯„ä¼° (å¿…é¡»)
        4. è¯­ä¹‰æ‘˜è¦ç”Ÿæˆ (å¿…é¡») 
        5. å‘é‡åŒ–å­˜å‚¨ (å¿…é¡»)
        6. å®ä½“æå– (æ¡ä»¶ï¼šscore > 0.8)
        7. æ•°æ®åº“å­˜å‚¨
        """
        start_time = datetime.utcnow()
        
        try:
            # 1. å†…å®¹æå–å’Œé¢„å¤„ç†
            content = self._extract_content_from_event(event_data)
            if not content:
                logger.debug(f"äº‹ä»¶æ— æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡å¤„ç†: {connector_id}/{event_type}")
                return ProcessingResult(
                    accepted=False,
                    value_score=0.0,
                    summary="",
                    reasoning="æ— æœ‰æ•ˆå†…å®¹"
                )
            
            # å†…å®¹é•¿åº¦é™åˆ¶
            if len(content) > self.max_content_length:
                content = content[:self.max_content_length] + "..."
                logger.debug(f"å†…å®¹æˆªæ–­åˆ° {self.max_content_length} å­—ç¬¦")
            
            # 2. å¼ºåˆ¶AIæœåŠ¡å¯ç”¨æ€§æ£€æŸ¥
            if not await self._ensure_ai_service_available():
                logger.warning(f"AIæœåŠ¡ä¸å¯ç”¨ï¼Œé˜»å¡äº‹ä»¶å¤„ç†: {connector_id}/{event_type}")
                return ProcessingResult(
                    accepted=False,
                    value_score=0.0,
                    summary="",
                    reasoning="AIæœåŠ¡ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¿"
                )
            
            # 3. å¿…é¡»çš„AIè¯„ä¼°å’Œæ‘˜è¦
            evaluation = await self._process_content_with_ai_required(content)
            
            # 3.5. AIå…³è”åˆ†æ - æ–°å¢åŠŸèƒ½
            ai_semantic_tags = []
            ai_correlations = []
            ai_insights_available = False
            
            ai_correlator = self._get_ai_correlator()
            if ai_correlator:
                try:
                    # æ„å»ºå®Œæ•´äº‹ä»¶æ•°æ®ä¾›AIåˆ†æ
                    full_event_data = {
                        'connector_id': connector_id,
                        'event_type': event_type,
                        'event_data': event_data,
                        'timestamp': timestamp,
                        'metadata': metadata or {},
                        'content': content,
                        'ai_evaluation': {
                            'value_score': evaluation.value_score,
                            'summary': evaluation.summary,
                            'confidence': evaluation.confidence
                        }
                    }
                    
                    # AIå…³è”åˆ†æ
                    correlation_result = await ai_correlator.process_event(full_event_data)
                    
                    if correlation_result.get('processing_success'):
                        ai_semantic_tags = [tag.__dict__ for tag in correlation_result.get('semantic_tags', [])]
                        ai_correlations = [corr.__dict__ for corr in correlation_result.get('correlations', [])]
                        ai_insights_available = correlation_result.get('ai_insights', False)
                        
                        logger.debug(f"ğŸ”— AIå…³è”åˆ†æå®Œæˆ: {len(ai_semantic_tags)}ä¸ªæ ‡ç­¾, {len(ai_correlations)}ä¸ªå…³è”")
                    
                except Exception as e:
                    logger.warning(f"AIå…³è”åˆ†æå¤±è´¥ï¼Œç»§ç»­å¤„ç†: {e}")
            
            # 4. åŸºäºçœŸå®AIè¯„åˆ†çš„è¿‡æ»¤å†³ç­–
            if evaluation.value_score < self.value_threshold:
                logger.debug(f"AIè¯„ä¼°åå†…å®¹è¢«è¿‡æ»¤: score={evaluation.value_score:.3f}, threshold={self.value_threshold:.3f}")
                self._record_rejection(evaluation.value_score)
                return ProcessingResult(
                    accepted=False,
                    value_score=evaluation.value_score,
                    summary=evaluation.summary,
                    reasoning=f"AIè¯„ä¼°: score={evaluation.value_score:.3f} < {self.value_threshold:.3f}"
                )
            
            # 4. æ„å»ºå‘é‡æ–‡æ¡£
            doc_id = f"{connector_id}_{hash(content + timestamp) % 1000000}"
            
            vector_doc = None
            if self.enable_vector_storage and self._vector_store is not None and self._ollama_service is not None:
                # 4. å‘é‡åŒ–å­˜å‚¨ï¼ˆå•ç‹¬è°ƒç”¨ï¼‰
                embedding = await self._ollama_service.embed_text(evaluation.summary)
                
                vector_doc = VectorDocument(
                    id=doc_id,
                    summary=evaluation.summary,
                    embedding=np.array(embedding, dtype=np.float32),
                    metadata={
                        "connector_id": connector_id,
                        "event_type": event_type,
                        "timestamp": timestamp,
                        "content_type": evaluation.content_type,
                        "keywords": evaluation.keywords,
                        "original_metadata": metadata or {},
                    },
                    entity_id=None,
                    timestamp=datetime.fromisoformat(timestamp) if timestamp else datetime.utcnow(),
                    content_type=evaluation.content_type,
                    value_score=evaluation.value_score,
                )
                
                # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                vector_stored = await self._vector_store.add_document(vector_doc)
            else:
                vector_stored = False
            
            # 5. å®ä½“æå–ï¼ˆé«˜ä»·å€¼å†…å®¹ï¼‰
            entities_created = 0
            if (self.enable_entity_extraction and 
                evaluation.value_score > self.entity_threshold and 
                evaluation.entities):
                entities_created = await self._create_valuable_entities(
                    evaluation.entities, doc_id, connector_id, metadata or {}
                )
            
            # 6. æ•°æ®åº“å­˜å‚¨ï¼ˆå…ƒæ•°æ®ï¼‰
            database_stored = await self._store_event_metadata(
                doc_id, connector_id, event_type, evaluation, timestamp, metadata
            )
            
            # è®°å½•æˆåŠŸå¤„ç†
            processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
            self._record_success(evaluation.value_score, processing_time)
            
            logger.info(f"âœ… æ™ºèƒ½å¤„ç†å®Œæˆ: {doc_id}, ä»·å€¼={evaluation.value_score:.3f}, è€—æ—¶={processing_time:.1f}ms, æ¨¡å¼={self._processing_mode}, å®ä½“={entities_created}")
            
            return ProcessingResult(
                accepted=True,
                value_score=evaluation.value_score,
                summary=evaluation.summary,
                reasoning=f"æ™ºèƒ½å¤„ç†[{self._processing_mode}]: {evaluation.value_score:.3f}",
                entities_created=entities_created,
                vector_stored=vector_stored,
                database_stored=database_stored,
                processing_time_ms=processing_time,
                ai_semantic_tags=ai_semantic_tags,
                ai_correlations=ai_correlations,
                ai_insights_available=ai_insights_available,
            )
            
        except Exception as e:
            processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
            logger.error(f"æ™ºèƒ½äº‹ä»¶å¤„ç†å¤±è´¥: {e}")
            
            return ProcessingResult(
                accepted=False,
                value_score=0.0,
                summary="",
                reasoning=f"å¤„ç†å¼‚å¸¸: {str(e)}"
            )

    # === å†…å®¹æå–å’Œé¢„å¤„ç† ===

    def _extract_content_from_event(self, event_data: Dict[str, Any]) -> Optional[str]:
        """ä»äº‹ä»¶æ•°æ®ä¸­æå–æ–‡æœ¬å†…å®¹"""
        # å¸¸è§çš„å†…å®¹å­—æ®µå
        content_fields = ["content", "text", "data", "message", "body", "value"]
        
        for field in content_fields:
            if field in event_data:
                content = event_data[field]
                if isinstance(content, str) and content.strip():
                    return content.strip()
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å†…å®¹å­—æ®µï¼Œå°è¯•è½¬æ¢æ•´ä¸ªäº‹ä»¶æ•°æ®ä¸ºå­—ç¬¦ä¸²
        if isinstance(event_data, dict) and len(event_data) == 1:
            value = list(event_data.values())[0]
            if isinstance(value, str) and value.strip():
                return value.strip()
        
        return None

    # === AIæœåŠ¡å¿…é¡»ä¾èµ–æ£€æŸ¥ ===

    async def _ensure_ai_service_available(self) -> bool:
        """ç¡®ä¿AIæœåŠ¡å¯ç”¨ - å¿…é¡»ä¾èµ–æ¨¡å¼
        
        é‡‡ç”¨ä¸»åŠ¨ç­‰å¾…ç­–ç•¥ï¼š
        1. æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
        2. å¦‚æœä¸å¯ç”¨ï¼Œè§¦å‘é‡è¿å°è¯•
        3. çŸ­æ—¶é—´ç­‰å¾…é‡è¿ç»“æœ
        4. å¦‚æœä»ä¸å¯ç”¨ï¼Œæ˜ç¡®è¿”å›å¤±è´¥
        """
        try:
            # æ£€æŸ¥å½“å‰çŠ¶æ€
            if self._ollama_service and self._ollama_service.is_available():
                return True
            
            logger.info("AIæœåŠ¡ä¸å¯ç”¨ï¼Œå°è¯•é‡æ–°è¿æ¥...")
            
            # å°è¯•é‡æ–°åˆå§‹åŒ–æœåŠ¡
            if self._ollama_service is None:
                self._ollama_service = get_service(OllamaService)
                if not await self._ollama_service.initialize():
                    raise RuntimeError("OllamaæœåŠ¡åˆå§‹åŒ–å¤±è´¥")
            
            # ç­‰å¾…çŸ­æ—¶é—´è®©é‡è¿é€»è¾‘å·¥ä½œ
            import asyncio
            for attempt in range(3):  # æœ€å¤šç­‰å¾…3æ¬¡ï¼Œæ¯æ¬¡2ç§’
                await asyncio.sleep(2)
                if self._ollama_service.is_available():
                    logger.info("AIæœåŠ¡é‡è¿æˆåŠŸ")
                    return True
                logger.debug(f"AIæœåŠ¡é‡è¿å°è¯• {attempt + 1}/3 å¤±è´¥")
            
            logger.error("AIæœåŠ¡é‡è¿å¤±è´¥ï¼Œéœ€è¦ç”¨æˆ·å¹²é¢„")
            return False
            
        except Exception as e:
            logger.error(f"AIæœåŠ¡å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def _process_content_with_ai_required(self, content: str) -> ContentEvaluation:
        """å¿…é¡»ä½¿ç”¨AIçš„å†…å®¹å¤„ç† - æ— é™çº§æ¨¡å¼
        
        è¿™æ˜¯æ™ºèƒ½åŠ©æ‰‹çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸å…è®¸é™çº§å¤„ç†
        """
        try:
            if not self._ollama_service:
                raise RuntimeError("AIæœåŠ¡æœªåˆå§‹åŒ–")
            
            if not self._ollama_service.is_available():
                raise RuntimeError("AIæœåŠ¡ä¸å¯ç”¨")
            
            # è°ƒç”¨AIæœåŠ¡è¿›è¡Œç»¼åˆå¤„ç†
            evaluation = await self._ollama_service.process_content_comprehensive(content)
            
            # éªŒè¯AIè¿”å›ç»“æœçš„æœ‰æ•ˆæ€§
            if evaluation.value_score == 0.0 and evaluation.confidence == 0.0:
                raise RuntimeError("AIæœåŠ¡è¿”å›æ— æ•ˆç»“æœ")
            
            logger.debug(f"AIè¯„ä¼°å®Œæˆ: score={evaluation.value_score:.3f}, confidence={evaluation.confidence:.3f}")
            return evaluation
            
        except Exception as e:
            logger.error(f"å¿…é¡»çš„AIå¤„ç†å¤±è´¥: {e}")
            # ä¸æä¾›é™çº§æ–¹æ¡ˆï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
            raise RuntimeError(f"AIæœåŠ¡å¤„ç†å¤±è´¥ï¼Œæ™ºèƒ½åŠŸèƒ½ä¸å¯ç”¨: {str(e)}")

    # === åˆå¹¶AIè°ƒç”¨ä¼˜åŒ–ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ä¸Šé¢çš„å¿…é¡»æ¨¡å¼ï¼‰ ===

    async def _process_content_merged(self, content: str) -> ContentEvaluation:
        """
        åˆå¹¶AIè°ƒç”¨ - ä¸€æ¬¡æ€§å®Œæˆä»·å€¼è¯„ä¼°å’Œè¯­ä¹‰æ‘˜è¦
        æ ¹æ®Geminiå»ºè®®ä¼˜åŒ–ï¼šå°†ä¸¤ä¸ªAIä»»åŠ¡åˆå¹¶ä¸ºä¸€æ¬¡è°ƒç”¨
        """
        try:
            # æ£€æŸ¥ollamaæœåŠ¡æ˜¯å¦å¯ç”¨
            if self._ollama_service is None:
                logger.error("OllamaæœåŠ¡æœªåˆå§‹åŒ–")
                return ContentEvaluation(
                    value_score=0.0,
                    confidence=0.0,
                    content_type="text",
                    summary=content[:100],
                    keywords=[],
                    entities=[],
                    reasoning="OllamaæœåŠ¡æœªåˆå§‹åŒ–"
                )
            
            # æ„å»ºåˆå¹¶çš„prompt - è¦æ±‚JSONæ ¼å¼è¾“å‡º
            merged_prompt = f"""åˆ†æä»¥ä¸‹å†…å®¹ï¼Œä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä¸¤ä¸ªå­—æ®µï¼š

å†…å®¹ï¼š
\"\"\"
{content[:2000]}
\"\"\"

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "value_score": <0-1ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºå†…å®¹é‡è¦æ€§>,
    "summary": "<100å­—ä»¥å†…çš„ç®€æ´æ‘˜è¦>"
}}

è¯„åˆ†æ ‡å‡†ï¼š
- 0.0-0.2: åƒåœ¾å†…å®¹ï¼ˆå¹¿å‘Šã€ä¹±ç ã€é‡å¤æ–‡æœ¬ï¼‰
- 0.3-0.5: ä½ä»·å€¼å†…å®¹ï¼ˆä¸´æ—¶ä¿¡æ¯ã€ç®€å•å¤åˆ¶ï¼‰
- 0.6-0.8: æœ‰ä»·å€¼å†…å®¹ï¼ˆæœ‰ç”¨ä¿¡æ¯ã€å­¦ä¹ èµ„æ–™ï¼‰
- 0.9-1.0: é«˜ä»·å€¼å†…å®¹ï¼ˆé‡è¦æ–‡æ¡£ã€åˆ›ä½œå†…å®¹ï¼‰

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""

            # è°ƒç”¨LLM
            response = await self._ollama_service._call_llm(content, merged_prompt)
            
            # è§£æJSONå“åº”
            evaluation_data = self._parse_merged_response(response)
            
            # æ„å»ºContentEvaluationå¯¹è±¡
            return ContentEvaluation(
                value_score=evaluation_data.get("value_score", 0.0),
                confidence=0.8,  # åˆå¹¶è°ƒç”¨çš„ç½®ä¿¡åº¦
                content_type="text",
                summary=evaluation_data.get("summary", content[:100]),
                keywords=[],  # ç®€åŒ–ç‰ˆæœ¬æš‚ä¸æå–å…³é”®è¯
                entities=[],  # ç®€åŒ–ç‰ˆæœ¬æš‚ä¸æå–å®ä½“
                reasoning=f"åˆå¹¶AIè°ƒç”¨: score={evaluation_data.get('value_score', 0.0):.3f}"
            )
            
        except Exception as e:
            logger.error(f"åˆå¹¶AIè°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return ContentEvaluation(
                value_score=0.0,
                confidence=0.0,
                content_type="text",
                summary=content[:100],
                keywords=[],
                entities=[],
                reasoning=f"åˆå¹¶è°ƒç”¨å¤±è´¥: {str(e)}"
            )

    def _parse_merged_response(self, response: str) -> dict:
        """è§£æåˆå¹¶AIè°ƒç”¨çš„JSONå“åº”"""
        try:
            import json
            import re
            
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                return json.loads(response.strip())
            except json.JSONDecodeError:
                pass
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*?\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            
            # å°è¯•æå–æ•°å­—å’Œæ–‡æœ¬
            score_match = re.search(r'"?value_score"?\s*:\s*([0-9.]+)', response)
            summary_match = re.search(r'"?summary"?\s*:\s*"([^"]*)"', response)
            
            value_score = float(score_match.group(1)) if score_match else 0.0
            summary = summary_match.group(1) if summary_match else ""
            
            return {
                "value_score": max(0.0, min(1.0, value_score)),
                "summary": summary[:100] if summary else "è§£æå¤±è´¥"
            }
            
        except Exception as e:
            logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
            return {"value_score": 0.0, "summary": "è§£æå¤±è´¥"}

    # === å®ä½“åˆ›å»º ===

    async def _create_valuable_entities(
        self,
        entities: List[Dict[str, Any]],
        source_doc_id: str,
        connector_id: str,
        metadata: Dict[str, Any],
    ) -> int:
        """åˆ›å»ºé«˜ä»·å€¼å®ä½“"""
        try:
            created_count = 0
            
            for entity_data in entities:
                entity_id = f"entity_{hash(entity_data.get('name', '') + source_doc_id) % 1000000}"
                
                entity_properties = {
                    "source_document": source_doc_id,
                    "connector_id": connector_id,
                    "entity_type": entity_data.get("type", "unknown"),
                    "entity_name": entity_data.get("name", ""),
                    "context": entity_data.get("context", ""),
                    "extraction_confidence": 0.9,  # é«˜ä»·å€¼å®ä½“çš„ç½®ä¿¡åº¦
                    "created_by": "ai_extraction",
                    "metadata": metadata,
                }
                
                # å­˜å‚¨å®ä½“åˆ°æ•°æ®åº“
                db_service = self._get_db_service()
                if not db_service:
                    logger.error("æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å®ä½“åˆ›å»º")
                    return 0
                
                with db_service.get_session() as session:
                    entity_record = EntityMetadata(
                        entity_id=entity_id,
                        name=entity_data.get("name", "AIæå–çš„å®ä½“"),
                        type="ai_extracted_entity",
                        description=f"AIä»é«˜ä»·å€¼å†…å®¹ä¸­æå–çš„{entity_data.get('type', 'å®ä½“')}",
                        properties=entity_properties,
                        access_count=1,
                    )
                    session.add(entity_record)
                    session.commit()
                    created_count += 1
                    
                    logger.debug(f"åˆ›å»ºAIå®ä½“: {entity_id} - {entity_data.get('name', '')}")
            
            self._metrics.entities_extracted += created_count
            return created_count
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå®ä½“å¤±è´¥: {e}")
            return 0

    # === æ•°æ®åº“å­˜å‚¨ ===

    async def _store_event_metadata(
        self,
        doc_id: str,
        connector_id: str,
        event_type: str,
        evaluation: ContentEvaluation,
        timestamp: str,
        metadata: Optional[Dict[str, Any]],
    ) -> bool:
        """å­˜å‚¨äº‹ä»¶å…ƒæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            event_properties = {
                "connector_id": connector_id,
                "event_type": event_type,
                "timestamp": timestamp,
                "processed_at": datetime.utcnow().isoformat(),
                "ai_evaluation": {
                    "value_score": evaluation.value_score,
                    "content_type": evaluation.content_type,
                    "keywords": evaluation.keywords,
                    "confidence": evaluation.confidence,
                    "method": "merged_ai_call",
                },
                "summary": evaluation.summary,
                "metadata": metadata or {},
            }
            
            db_service = self._get_db_service()
            if not db_service:
                logger.error("æ•°æ®åº“æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡äº‹ä»¶å…ƒæ•°æ®å­˜å‚¨")
                return False
            
            with db_service.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(EntityMetadata).filter_by(entity_id=doc_id).first()
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    existing.properties = event_properties
                    existing.updated_at = datetime.utcnow()
                    existing.access_count += 1
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    entity_record = EntityMetadata(
                        entity_id=doc_id,
                        name=f"æ™ºèƒ½å¤„ç†_{connector_id}_{datetime.now().strftime('%H%M%S')}",
                        type="intelligent_event",
                        description=f"åˆå¹¶AIè°ƒç”¨å¤„ç†çš„{event_type}äº‹ä»¶ï¼Œä»·å€¼è¯„åˆ†: {evaluation.value_score:.3f}",
                        properties=event_properties,
                        access_count=1,
                    )
                    session.add(entity_record)
                
                # è®°å½•è¿æ¥å™¨æ—¥å¿—
                log_entry = ConnectorLog(
                    connector_id=connector_id,
                    level="INFO",
                    message=f"åˆå¹¶AIè°ƒç”¨: {event_type}",
                    details={
                        "event_type": event_type,
                        "value_score": evaluation.value_score,
                        "content_type": evaluation.content_type,
                        "summary_length": len(evaluation.summary),
                        "entities_count": len(evaluation.entities) if evaluation.entities else 0,
                        "confidence": evaluation.confidence,
                        "timestamp": timestamp,
                        "method": "merged_ai_call",
                    },
                )
                session.add(log_entry)
                
                session.commit()
                
            return True
            
        except Exception as e:
            logger.error(f"å­˜å‚¨äº‹ä»¶å…ƒæ•°æ®å¤±è´¥: {e}")
            return False

    # === æœç´¢å’ŒæŸ¥è¯¢æ¥å£ ===

    async def search_intelligent_content(
        self,
        query: str,
        k: int = 10,
        min_value_score: float = 0.0,
        content_types: Optional[List[str]] = None,
        connector_ids: Optional[List[str]] = None,
    ) -> List[SearchResult]:
        """æ™ºèƒ½å†…å®¹æœç´¢"""
        try:
            if not self.enable_vector_storage or not self._vector_store or not self._ollama_service:
                logger.warning("å‘é‡å­˜å‚¨æˆ–OllamaæœåŠ¡æœªå¯ç”¨")
                return []
            
            # æ„å»ºæœç´¢å‘é‡
            query_embedding = await self._ollama_service.embed_text(query)
            query_vector = np.array(query_embedding, dtype=np.float32)
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            filter_metadata = {}
            if min_value_score > 0:
                filter_metadata["value_score"] = {"min": min_value_score}
            if content_types:
                filter_metadata["content_type"] = content_types
            if connector_ids:
                filter_metadata["connector_id"] = connector_ids
            
            # æ‰§è¡Œå‘é‡æœç´¢
            results = await self._vector_store.search_similar(
                query_vector=query_vector,
                k=k,
                search_tiers=["hot", "warm"],  # æœç´¢çƒ­æ•°æ®å’Œæ¸©æ•°æ®
                filter_metadata=filter_metadata if filter_metadata else None,
            )
            
            logger.debug(f"æ™ºèƒ½å†…å®¹æœç´¢å®Œæˆ: {len(results)} ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½å†…å®¹æœç´¢å¤±è´¥: {e}")
            return []

    async def get_content_by_value_range(
        self,
        min_score: float,
        max_score: float = 1.0,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """æŒ‰ä»·å€¼è¯„åˆ†èŒƒå›´è·å–å†…å®¹"""
        try:
            filter_metadata = {
                "value_score": {"min": min_score, "max": max_score}
            }
            
            # ä½¿ç”¨ä¸€ä¸ªé€šç”¨æŸ¥è¯¢å‘é‡
            dummy_query = np.zeros(384, dtype=np.float32)
            
            results = await self._vector_store.search_similar(
                query_vector=dummy_query,
                k=limit,
                filter_metadata=filter_metadata,
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            content_list = []
            for result in results:
                content_list.append({
                    "id": result.id,
                    "summary": result.summary,
                    "value_score": result.value_score,
                    "content_type": result.content_type,
                    "timestamp": result.timestamp.isoformat() if result.timestamp else None,
                    "metadata": result.metadata,
                })
            
            return content_list
            
        except Exception as e:
            logger.error(f"æŒ‰ä»·å€¼èŒƒå›´è·å–å†…å®¹å¤±è´¥: {e}")
            return []

    # === ç»Ÿè®¡å’Œåˆ†æ ===

    async def get_content_statistics(self) -> Dict[str, Any]:
        """è·å–å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                "processing_metrics": await self.get_metrics(),
                "value_score_distribution": {},
                "content_type_distribution": {},
                "connector_distribution": {},
            }
            
            # è·å–ä¸åŒä»·å€¼åŒºé—´çš„å†…å®¹æ•°é‡
            value_ranges = [
                ("premium", 0.8, 1.0),
                ("high", 0.6, 0.8),
                ("medium", 0.3, 0.6),
                ("low", 0.0, 0.3),
            ]
            
            for label, min_score, max_score in value_ranges:
                content = await self.get_content_by_value_range(min_score, max_score, 1000)
                stats["value_score_distribution"][label] = len(content)
                
                # ç»Ÿè®¡å†…å®¹ç±»å‹åˆ†å¸ƒ
                for item in content:
                    content_type = item.get("content_type", "unknown")
                    stats["content_type_distribution"][content_type] = \
                        stats["content_type_distribution"].get(content_type, 0) + 1
                    
                    # ç»Ÿè®¡è¿æ¥å™¨åˆ†å¸ƒ
                    connector_id = item.get("metadata", {}).get("connector_id", "unknown")
                    stats["connector_distribution"][connector_id] = \
                        stats["connector_distribution"].get(connector_id, 0) + 1
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    # === æ€§èƒ½ç›‘æ§ ===

    def _record_success(self, value_score: float, processing_time: float):
        """è®°å½•æˆåŠŸå¤„ç†"""
        self._metrics.total_events += 1
        self._metrics.accepted_events += 1
        
        self._processing_times.append(processing_time)
        self._value_scores.append(value_score)
        
        # ä¿æŒå†å²è®°å½•é™åˆ¶
        if len(self._processing_times) > self._max_history:
            self._processing_times = self._processing_times[-self._max_history:]
        if len(self._value_scores) > self._max_history:
            self._value_scores = self._value_scores[-self._max_history:]

    def _record_rejection(self, value_score: float):
        """è®°å½•æ‹’ç»å¤„ç†"""
        self._metrics.total_events += 1
        self._metrics.rejected_events += 1
        self._value_scores.append(value_score)

    async def get_metrics(self) -> ProcessingMetrics:
        """è·å–å¤„ç†æ€§èƒ½æŒ‡æ ‡"""
        await self._update_metrics()
        return self._metrics

    async def _update_metrics(self):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        try:
            if self._processing_times:
                self._metrics.avg_processing_time_ms = np.mean(self._processing_times)
            
            if self._value_scores:
                self._metrics.avg_value_score = np.mean(self._value_scores)
            
            if self._metrics.total_events > 0:
                self._metrics.acceptance_rate = self._metrics.accepted_events / self._metrics.total_events
                
                # è®¡ç®—å­˜å‚¨æ•ˆç‡ (å‹ç¼©æ¯”)
                if self.enable_vector_storage and self._vector_store:
                    vector_metrics = await self._vector_store.get_metrics()
                    self._metrics.storage_efficiency = vector_metrics.compression_ratio
            
            self._metrics.last_updated = datetime.utcnow()
            
        except Exception as e:
            logger.error(f"æ›´æ–°æŒ‡æ ‡å¤±è´¥: {e}")

    # === ç»´æŠ¤å’Œæ¸…ç† ===

    async def cleanup_low_value_content(self, min_age_days: int = 30) -> int:
        """æ¸…ç†ä½ä»·å€¼å†…å®¹"""
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=min_age_days)
            
            # è·å–ä½ä»·å€¼å†…å®¹
            low_value_content = await self.get_content_by_value_range(0.0, self.value_threshold, 10000)
            
            cleanup_count = 0
            for item in low_value_content:
                if item.get("timestamp"):
                    item_date = datetime.fromisoformat(item["timestamp"])
                    if item_date < cutoff_date:
                        # ä»æ•°æ®åº“åˆ é™¤
                        db_service = self._get_db_service()
                        if not db_service:
                            continue
                        
                        with db_service.get_session() as session:
                            existing = session.query(EntityMetadata).filter_by(
                                entity_id=item["id"]
                            ).first()
                            if existing:
                                session.delete(existing)
                                session.commit()
                                cleanup_count += 1
            
            logger.info(f"æ¸…ç†ä½ä»·å€¼å†…å®¹: {cleanup_count} é¡¹")
            return cleanup_count
            
        except Exception as e:
            logger.error(f"æ¸…ç†ä½ä»·å€¼å†…å®¹å¤±è´¥: {e}")
            return 0
    
    async def cleanup(self):
        """æ¸…ç†å¤„ç†å™¨èµ„æº"""
        try:
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            metrics = await self.get_metrics()
            logger.info(f"æ™ºèƒ½å¤„ç†å™¨ç»Ÿè®¡ - æ€»è®¡: {metrics.total_events}, æ¥å—: {metrics.accepted_events}, æ¥å—ç‡: {metrics.acceptance_rate:.2%}")
            
            # æ¸…ç†ollamaæœåŠ¡
            if hasattr(self, '_ollama_service') and self._ollama_service:
                await self._ollama_service.close()
                
            logger.info("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨å·²æ¸…ç†")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨å¤±è´¥: {e}")


# === æœåŠ¡å•ä¾‹ç®¡ç† ===

_intelligent_processor: Optional[IntelligentEventProcessor] = None

async def get_intelligent_event_processor() -> IntelligentEventProcessor:
    """è·å–æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _intelligent_processor
    if _intelligent_processor is None:
        _intelligent_processor = IntelligentEventProcessor()
        if not await _intelligent_processor.initialize():
            raise RuntimeError("æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥")
    return _intelligent_processor

async def cleanup_intelligent_event_processor():
    """æ¸…ç†æ™ºèƒ½äº‹ä»¶å¤„ç†å™¨"""
    global _intelligent_processor
    if _intelligent_processor:
        # ä¿å­˜æœ€ç»ˆæŒ‡æ ‡
        try:
            metrics = await _intelligent_processor.get_metrics()
            logger.info(f"æ™ºèƒ½å¤„ç†å™¨ç»Ÿè®¡ - æ€»è®¡: {metrics.total_events}, æ¥å—: {metrics.accepted_events}, æ¥å—ç‡: {metrics.acceptance_rate:.2%}")
        except:
            pass
        
        _intelligent_processor = None