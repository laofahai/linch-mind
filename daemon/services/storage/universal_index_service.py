#!/usr/bin/env python3
"""
é€šç”¨ç´¢å¼•æœåŠ¡ - çœŸæ­£è¿æ¥å™¨æ— å…³çš„æœç´¢ç´¢å¼•æ¶æ„

é‡æ„ç›®æ ‡ï¼š
- æ›¿ä»£FastIndexStorageServiceçš„æ–‡ä»¶ç³»ç»Ÿç‰¹å®šå®ç°
- æ”¯æŒä»»æ„è¿æ¥å™¨çš„å¿«é€Ÿæœç´¢éœ€æ±‚ (æ–‡ä»¶ã€é‚®ä»¶ã€ç½‘é¡µã€è”ç³»äººç­‰)
- ä¿æŒEverythingçº§åˆ«çš„æœç´¢æ€§èƒ½ (< 5ms)
- åŸºäºå†…å®¹ç±»å‹çš„æ™ºèƒ½ç´¢å¼•ç­–ç•¥
"""

import asyncio
import logging
import sqlite3
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Union
from dataclasses import dataclass
from enum import Enum

from core.service_facade import get_service
from core.environment_manager import EnvironmentManager
from services.unified_database_service import UnifiedDatabaseService

logger = logging.getLogger(__name__)


class IndexTier(Enum):
    """ç´¢å¼•å±‚çº§ - æ ¹æ®æ€§èƒ½éœ€æ±‚åˆ†å±‚å¤„ç†"""
    HOT = "hot"      # æé€Ÿæœç´¢ï¼Œç±»ä¼¼Everything (< 5ms)
    WARM = "warm"    # æ ‡å‡†æœç´¢ï¼Œå¹³è¡¡æ€§èƒ½å’ŒåŠŸèƒ½ (< 50ms)  
    COLD = "cold"    # è¯­ä¹‰æœç´¢ï¼ŒåŠŸèƒ½ä¸°å¯Œä½†è¾ƒæ…¢ (< 500ms)


class ContentType(Enum):
    """é€šç”¨å†…å®¹ç±»å‹ - è¿æ¥å™¨æ— å…³"""
    FILE_PATH = "file_path"      # æ–‡ä»¶è·¯å¾„
    URL = "url"                  # ç½‘å€
    EMAIL = "email"              # é‚®ç®±åœ°å€
    PHONE = "phone"              # ç”µè¯å·ç 
    TEXT = "text"                # çº¯æ–‡æœ¬
    CONTACT = "contact"          # è”ç³»äºº
    DOCUMENT = "document"        # æ–‡æ¡£å†…å®¹
    IMAGE = "image"              # å›¾ç‰‡
    AUDIO = "audio"              # éŸ³é¢‘
    VIDEO = "video"              # è§†é¢‘
    CODE = "code"                # ä»£ç 
    OTHER = "other"              # å…¶ä»–ç±»å‹


@dataclass
class UniversalIndexEntry:
    """é€šç”¨ç´¢å¼•æ¡ç›® - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹"""
    # æ ¸å¿ƒå­—æ®µ (æ‰€æœ‰æ¡ç›®å¿…éœ€)
    id: str                      # å”¯ä¸€æ ‡è¯†ç¬¦
    connector_id: str            # è¿æ¥å™¨ID
    content_type: ContentType    # å†…å®¹ç±»å‹
    primary_key: str             # ä¸»é”® (å¦‚æ–‡ä»¶è·¯å¾„ã€URLç­‰)
    searchable_text: str         # ç»Ÿä¸€æœç´¢æ–‡æœ¬
    display_name: str            # æ˜¾ç¤ºåç§°
    
    # æ€§èƒ½ä¼˜åŒ–å­—æ®µ
    index_tier: IndexTier        # ç´¢å¼•å±‚çº§
    priority: int                # æœç´¢ä¼˜å…ˆçº§ (1-10, è¶Šå°è¶Šä¼˜å…ˆ)
    
    # æ—¶é—´å­—æ®µ
    indexed_at: datetime         # ç´¢å¼•æ—¶é—´
    last_modified: Optional[datetime] = None  # å†…å®¹ä¿®æ”¹æ—¶é—´
    last_accessed: Optional[datetime] = None  # æœ€åè®¿é—®æ—¶é—´
    
    # æ‰©å±•å­—æ®µ (è¿æ¥å™¨ç‰¹å®š)
    structured_data: Dict[str, Any] = None  # ç»“æ„åŒ–æ•°æ®
    metadata: Dict[str, Any] = None         # å…ƒæ•°æ®
    
    # æœç´¢ä¼˜åŒ–å­—æ®µ
    keywords: Set[str] = None               # å…³é”®è¯é›†åˆ
    tags: Set[str] = None                   # æ ‡ç­¾é›†åˆ
    
    def __post_init__(self):
        if self.structured_data is None:
            self.structured_data = {}
        if self.metadata is None:
            self.metadata = {}
        if self.keywords is None:
            self.keywords = set()
        if self.tags is None:
            self.tags = set()


@dataclass 
class SearchQuery:
    """é€šç”¨æœç´¢æŸ¥è¯¢"""
    text: str                           # æœç´¢æ–‡æœ¬
    content_types: List[ContentType] = None  # å†…å®¹ç±»å‹è¿‡æ»¤
    connector_ids: List[str] = None     # è¿æ¥å™¨è¿‡æ»¤
    index_tiers: List[IndexTier] = None # ç´¢å¼•å±‚çº§è¿‡æ»¤
    limit: int = 100                    # ç»“æœé™åˆ¶
    min_priority: int = 1               # æœ€å°ä¼˜å…ˆçº§
    max_priority: int = 10              # æœ€å¤§ä¼˜å…ˆçº§
    include_metadata: bool = False      # æ˜¯å¦åŒ…å«å…ƒæ•°æ®
    
    def __post_init__(self):
        if self.content_types is None:
            self.content_types = []
        if self.connector_ids is None:
            self.connector_ids = []
        if self.index_tiers is None:
            self.index_tiers = [IndexTier.HOT, IndexTier.WARM, IndexTier.COLD]


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    entry: UniversalIndexEntry
    score: float                        # æœç´¢ç›¸å…³æ€§è¯„åˆ†
    highlights: List[str] = None        # é«˜äº®ç‰‡æ®µ
    
    def __post_init__(self):
        if self.highlights is None:
            self.highlights = []


class IndexingStrategy:
    """ç´¢å¼•ç­–ç•¥ç®¡ç†å™¨"""
    
    def __init__(self):
        # æ ¹æ®å†…å®¹ç±»å‹å†³å®šç´¢å¼•å±‚çº§ - ä¸ä¾èµ–å…·ä½“è¿æ¥å™¨ID
        # å°†æ¥åº”ä»è¿æ¥å™¨å…ƒæ•°æ®åŠ¨æ€è·å–å…¶ç‰¹æ€§å’Œä¼˜å…ˆçº§
        self._tier_rules = {
            # åŸºäºå†…å®¹ç±»å‹çš„é€šç”¨è§„åˆ™
            
            # é»˜è®¤è§„åˆ™
            ("*", ContentType.FILE_PATH): IndexTier.HOT,
            ("*", ContentType.URL): IndexTier.WARM,
            ("*", ContentType.EMAIL): IndexTier.WARM,
            ("*", ContentType.PHONE): IndexTier.WARM,
            ("*", ContentType.TEXT): IndexTier.COLD,
        }
    
    def get_index_tier(self, connector_id: str, content_type: ContentType) -> IndexTier:
        """è·å–ç´¢å¼•å±‚çº§"""
        # ç²¾ç¡®åŒ¹é…
        key = (connector_id, content_type)
        if key in self._tier_rules:
            return self._tier_rules[key]
        
        # é€šé…ç¬¦åŒ¹é…
        wildcard_key = ("*", content_type)
        if wildcard_key in self._tier_rules:
            return self._tier_rules[wildcard_key]
        
        # é»˜è®¤ä¸ºå†·å±‚
        return IndexTier.COLD
    
    def get_priority(self, connector_id: str, content_type: ContentType) -> int:
        """è·å–æœç´¢ä¼˜å…ˆçº§"""
        tier = self.get_index_tier(connector_id, content_type)
        
        if tier == IndexTier.HOT:
            return 1  # æœ€é«˜ä¼˜å…ˆçº§
        elif tier == IndexTier.WARM:
            return 5  # ä¸­ç­‰ä¼˜å…ˆçº§
        else:
            return 8  # è¾ƒä½ä¼˜å…ˆçº§


class ContentTypeDetector:
    """å†…å®¹ç±»å‹æ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_content_type(
        connector_id: str,
        event_type: str, 
        event_data: Dict[str, Any]
    ) -> ContentType:
        """
        æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        
        Args:
            connector_id: è¿æ¥å™¨ID
            event_type: äº‹ä»¶ç±»å‹
            event_data: äº‹ä»¶æ•°æ®
            
        Returns:
            æ£€æµ‹åˆ°çš„å†…å®¹ç±»å‹
        """
        # 1. åŸºäºäº‹ä»¶ç±»å‹åˆ¤æ–­ï¼ˆä¸ä¾èµ–å…·ä½“è¿æ¥å™¨IDï¼‰
        if "file" in event_type.lower():
            return ContentType.FILE_PATH
        elif "url" in event_type.lower() or "link" in event_type.lower():
            return ContentType.URL
        elif "email" in event_type.lower():
            return ContentType.EMAIL
        elif "phone" in event_type.lower():
            return ContentType.PHONE
        elif "contact" in event_type.lower():
            return ContentType.CONTACT
        
        # 3. åŸºäºæ•°æ®å†…å®¹å¯å‘å¼åˆ¤æ–­
        content = ContentTypeDetector._extract_content(event_data)
        if content:
            content_lower = content.lower()
            
            # URLæ£€æµ‹
            if content.startswith(("http://", "https://", "ftp://", "file://")):
                return ContentType.URL
            
            # æ–‡ä»¶è·¯å¾„æ£€æµ‹
            if ("/" in content or "\\" in content) and any(
                content.endswith(ext) for ext in [
                    ".txt", ".doc", ".pdf", ".jpg", ".png", ".mp4", ".mp3"
                ]
            ):
                return ContentType.FILE_PATH
            
            # é‚®ç®±æ£€æµ‹
            if "@" in content and "." in content.split("@")[-1]:
                return ContentType.EMAIL
            
            # ç”µè¯å·ç æ£€æµ‹ (ç®€å•åŒ¹é…)
            if len(content.replace("-", "").replace(" ", "").replace("(", "").replace(")", "")) >= 10:
                digits = sum(c.isdigit() for c in content)
                if digits >= 7:  # è‡³å°‘7ä¸ªæ•°å­—
                    return ContentType.PHONE
        
        # 4. é»˜è®¤ä¸ºæ–‡æœ¬
        return ContentType.TEXT
    
    @staticmethod
    def _extract_content(event_data: Dict[str, Any]) -> Optional[str]:
        """ä»äº‹ä»¶æ•°æ®ä¸­æå–å†…å®¹ç”¨äºæ£€æµ‹"""
        # å¸¸è§å†…å®¹å­—æ®µ
        for field in ["path", "url", "content", "text", "data", "value", "name"]:
            if field in event_data:
                value = event_data[field]
                if isinstance(value, str) and value.strip():
                    return value.strip()
        return None


class UniversalIndexService:
    """
    é€šç”¨ç´¢å¼•æœåŠ¡ - çœŸæ­£çš„è¿æ¥å™¨æ— å…³æ¶æ„
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ”¯æŒä»»æ„è¿æ¥å™¨çš„å¿«é€Ÿæœç´¢éœ€æ±‚
    2. åŸºäºå†…å®¹ç±»å‹çš„æ™ºèƒ½ç´¢å¼•åˆ†å±‚
    3. ä¿æŒEverythingçº§åˆ«æœç´¢æ€§èƒ½
    4. å®Œå…¨æ›¿ä»£FastIndexStorageService
    """
    
    def __init__(self):
        self._db_service = None
        self._universal_db_path = None
        self._universal_db_connection = None
        self._db_lock = threading.RLock()
        self._initialized = False
        
        # ç­–ç•¥ç»„ä»¶
        self._indexing_strategy = IndexingStrategy()
        self._content_detector = ContentTypeDetector()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'total_entries': 0,
            'entries_by_tier': {tier.value: 0 for tier in IndexTier},
            'entries_by_type': {ct.value: 0 for ct in ContentType},
            'entries_by_connector': {},
            'last_update': None,
            'search_requests': 0,
            'avg_search_time_ms': 0.0
        }
        
        logger.info("ğŸŒŸ UniversalIndexService åˆå§‹åŒ–")
    
    @property
    def db_service(self):
        """æ‡’åŠ è½½æ•°æ®åº“æœåŠ¡"""
        if self._db_service is None:
            try:
                self._db_service = get_service(UnifiedDatabaseService)
            except Exception as e:
                logger.warning(f"Database service not available: {str(e)}")
                return None
        return self._db_service
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–é€šç”¨ç´¢å¼•æœåŠ¡"""
        if self._initialized:
            return True
            
        try:
            # è·å–æ•°æ®åº“è·¯å¾„
            if self.db_service and hasattr(self.db_service, 'database_url'):
                db_dir = Path(self.db_service.database_url).parent
                self._universal_db_path = db_dir / "universal_index.db"
            else:
                # å›é€€è·¯å¾„
                env_manager = EnvironmentManager()
                self._universal_db_path = env_manager.current_config.data_dir / "universal_index.db"
            
            # åˆ›å»ºé€šç”¨ç´¢å¼•æ•°æ®åº“
            self._create_universal_database()
            self._initialized = True
            
            logger.info(f"ğŸš€ é€šç”¨ç´¢å¼•æœåŠ¡åˆå§‹åŒ–å®Œæˆ: {self._universal_db_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é€šç”¨ç´¢å¼•æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _create_universal_database(self):
        """åˆ›å»ºé€šç”¨ç´¢å¼•æ•°æ®åº“"""
        with self._db_lock:
            try:
                self._universal_db_connection = sqlite3.connect(
                    str(self._universal_db_path),
                    check_same_thread=False,
                    timeout=30.0
                )
                
                # å¯ç”¨æ€§èƒ½ä¼˜åŒ–
                self._universal_db_connection.execute("PRAGMA journal_mode=WAL")
                self._universal_db_connection.execute("PRAGMA synchronous=NORMAL")
                self._universal_db_connection.execute("PRAGMA cache_size=20000")
                self._universal_db_connection.execute("PRAGMA temp_store=MEMORY")
                self._universal_db_connection.execute("PRAGMA optimize")
                
                # åˆ›å»ºé€šç”¨ç´¢å¼•è¡¨ - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹
                self._universal_db_connection.execute("""
                    CREATE TABLE IF NOT EXISTS universal_index (
                        id TEXT PRIMARY KEY,
                        connector_id TEXT NOT NULL,
                        content_type TEXT NOT NULL,
                        primary_key TEXT NOT NULL,
                        primary_key_lower TEXT NOT NULL,  -- å°å†™ä¸»é”®ç”¨äºå¿«é€Ÿæœç´¢
                        searchable_text TEXT NOT NULL,
                        searchable_text_lower TEXT NOT NULL,  -- å°å†™æœç´¢æ–‡æœ¬
                        display_name TEXT NOT NULL,
                        display_name_lower TEXT NOT NULL,  -- å°å†™æ˜¾ç¤ºåç§°
                        
                        -- æ€§èƒ½åˆ†å±‚
                        index_tier TEXT NOT NULL,
                        priority INTEGER NOT NULL DEFAULT 5,
                        
                        -- æ—¶é—´å­—æ®µ
                        indexed_at INTEGER NOT NULL,
                        last_modified INTEGER,
                        last_accessed INTEGER,
                        
                        -- ç»“æ„åŒ–æ•°æ® (JSON)
                        structured_data TEXT,  -- JSONæ ¼å¼
                        metadata TEXT,         -- JSONæ ¼å¼
                        
                        -- æœç´¢ä¼˜åŒ–
                        keywords TEXT,         -- é€—å·åˆ†éš”çš„å…³é”®è¯
                        tags TEXT,             -- é€—å·åˆ†éš”çš„æ ‡ç­¾
                        
                        UNIQUE(connector_id, primary_key)
                    )
                """)
                
                # åˆ›å»ºé«˜æ€§èƒ½ç´¢å¼• - é’ˆå¯¹ä¸åŒæœç´¢åœºæ™¯ä¼˜åŒ–
                indices = [
                    # ä¸»æœç´¢ç´¢å¼• (æŒ‰å±‚çº§ä¼˜åŒ–)
                    "CREATE INDEX IF NOT EXISTS idx_hot_search ON universal_index(index_tier, priority, searchable_text_lower) WHERE index_tier = 'hot'",
                    "CREATE INDEX IF NOT EXISTS idx_warm_search ON universal_index(index_tier, priority, searchable_text_lower) WHERE index_tier = 'warm'", 
                    "CREATE INDEX IF NOT EXISTS idx_cold_search ON universal_index(index_tier, searchable_text_lower) WHERE index_tier = 'cold'",
                    
                    # æ˜¾ç¤ºåç§°æœç´¢
                    "CREATE INDEX IF NOT EXISTS idx_display_name ON universal_index(display_name_lower)",
                    "CREATE INDEX IF NOT EXISTS idx_primary_key ON universal_index(primary_key_lower)",
                    
                    # åˆ†ç±»æœç´¢
                    "CREATE INDEX IF NOT EXISTS idx_connector_type ON universal_index(connector_id, content_type)",
                    "CREATE INDEX IF NOT EXISTS idx_content_type ON universal_index(content_type, priority)",
                    "CREATE INDEX IF NOT EXISTS idx_connector_id ON universal_index(connector_id, priority)",
                    
                    # æ—¶é—´èŒƒå›´æœç´¢
                    "CREATE INDEX IF NOT EXISTS idx_last_modified ON universal_index(last_modified)",
                    "CREATE INDEX IF NOT EXISTS idx_indexed_at ON universal_index(indexed_at)",
                    
                    # å¤åˆç´¢å¼• (é«˜é¢‘æŸ¥è¯¢ä¼˜åŒ–)
                    "CREATE INDEX IF NOT EXISTS idx_connector_search ON universal_index(connector_id, searchable_text_lower, priority)",
                    "CREATE INDEX IF NOT EXISTS idx_type_search ON universal_index(content_type, searchable_text_lower, priority)",
                ]
                
                for idx_sql in indices:
                    self._universal_db_connection.execute(idx_sql)
                
                # åˆ›å»ºç»Ÿè®¡è¡¨
                self._universal_db_connection.execute("""
                    CREATE TABLE IF NOT EXISTS index_stats (
                        key TEXT PRIMARY KEY,
                        value TEXT NOT NULL,
                        updated_at INTEGER NOT NULL
                    )
                """)
                
                # åˆ›å»ºæœç´¢æ—¥å¿—è¡¨ (æ€§èƒ½ç›‘æ§)
                self._universal_db_connection.execute("""
                    CREATE TABLE IF NOT EXISTS search_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        query_text TEXT NOT NULL,
                        content_types TEXT,  -- JSON array
                        connector_ids TEXT,  -- JSON array
                        result_count INTEGER NOT NULL,
                        search_time_ms REAL NOT NULL,
                        timestamp INTEGER NOT NULL
                    )
                """)
                
                self._universal_db_connection.commit()
                logger.info("âœ… é€šç”¨ç´¢å¼•æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºé€šç”¨ç´¢å¼•æ•°æ®åº“å¤±è´¥: {e}")
                raise
    
    async def index_content_batch(self, entries: List[Dict[str, Any]]) -> bool:
        """
        æ‰¹é‡ç´¢å¼•å†…å®¹ - é€šç”¨æ¥å£æ”¯æŒæ‰€æœ‰è¿æ¥å™¨
        
        Args:
            entries: é€šç”¨ç´¢å¼•æ¡ç›®åˆ—è¡¨ï¼Œæ ¼å¼ï¼š
            [
                {
                    "connector_id": "filesystem",
                    "event_type": "file_indexed", 
                    "event_data": {...},
                    "timestamp": "2025-08-15T19:00:00Z",
                    "metadata": {...}
                },
                ...
            ]
            
        Returns:
            bool: ç´¢å¼•æ˜¯å¦æˆåŠŸ
        """
        if not self._initialized:
            if not self.initialize():
                return False
        
        if not entries:
            return True
            
        try:
            with self._db_lock:
                cursor = self._universal_db_connection.cursor()
                
                # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
                insert_data = []
                current_time = int(datetime.utcnow().timestamp())
                
                for entry_data in entries:
                    try:
                        # è½¬æ¢ä¸ºé€šç”¨ç´¢å¼•æ¡ç›®
                        index_entry = self._convert_to_index_entry(entry_data, current_time)
                        if index_entry:
                            insert_data.append(self._prepare_db_row(index_entry))
                    except Exception as e:
                        logger.warning(f"å¤„ç†ç´¢å¼•æ¡ç›®å¤±è´¥: {e}")
                        continue
                
                if not insert_data:
                    return True
                
                # ä½¿ç”¨ REPLACE è¿›è¡Œæ‰¹é‡æ’å…¥ï¼ˆæ›´æ–°å·²å­˜åœ¨çš„æ¡ç›®ï¼‰
                cursor.executemany("""
                    REPLACE INTO universal_index (
                        id, connector_id, content_type, primary_key, primary_key_lower,
                        searchable_text, searchable_text_lower, display_name, display_name_lower,
                        index_tier, priority, indexed_at, last_modified, last_accessed,
                        structured_data, metadata, keywords, tags
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, insert_data)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._update_stats_batch(insert_data, current_time)
                
                self._universal_db_connection.commit()
                
                logger.info(f"âœ… æ‰¹é‡ç´¢å¼•å†…å®¹: {len(insert_data)} æ¡ç›®")
                return True
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç´¢å¼•å†…å®¹å¤±è´¥: {e}")
            if self._universal_db_connection:
                self._universal_db_connection.rollback()
            return False
    
    def _convert_to_index_entry(self, entry_data: Dict[str, Any], current_time: int) -> Optional[UniversalIndexEntry]:
        """å°†é€šç”¨äº‹ä»¶æ•°æ®è½¬æ¢ä¸ºç´¢å¼•æ¡ç›®"""
        try:
            connector_id = entry_data.get("connector_id", "")
            event_type = entry_data.get("event_type", "")
            event_data = entry_data.get("event_data", {})
            metadata = entry_data.get("metadata", {})
            
            if not connector_id or not event_data:
                return None
            
            # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
            content_type = self._content_detector.detect_content_type(
                connector_id, event_type, event_data
            )
            
            # æå–ä¸»é”®å’Œæ˜¾ç¤ºåç§°
            primary_key = self._extract_primary_key(content_type, event_data)
            if not primary_key:
                return None
            
            display_name = self._extract_display_name(content_type, event_data, primary_key)
            searchable_text = self._extract_searchable_text(content_type, event_data, display_name)
            
            # ç¡®å®šç´¢å¼•ç­–ç•¥
            index_tier = self._indexing_strategy.get_index_tier(connector_id, content_type)
            priority = self._indexing_strategy.get_priority(connector_id, content_type)
            
            # ç”Ÿæˆå”¯ä¸€ID
            entry_id = f"{connector_id}:{content_type.value}:{hash(primary_key) % 1000000}"
            
            # å¤„ç†æ—¶é—´æˆ³
            last_modified = self._extract_timestamp(event_data.get("last_modified") or event_data.get("modified_time"))
            
            # æå–å…³é”®è¯å’Œæ ‡ç­¾
            keywords = self._extract_keywords(searchable_text, event_data)
            tags = self._extract_tags(content_type, event_data, metadata)
            
            return UniversalIndexEntry(
                id=entry_id,
                connector_id=connector_id,
                content_type=content_type,
                primary_key=primary_key,
                searchable_text=searchable_text,
                display_name=display_name,
                index_tier=index_tier,
                priority=priority,
                indexed_at=datetime.fromtimestamp(current_time),
                last_modified=last_modified,
                structured_data=event_data,
                metadata=metadata,
                keywords=keywords,
                tags=tags
            )
            
        except Exception as e:
            logger.error(f"è½¬æ¢ç´¢å¼•æ¡ç›®å¤±è´¥: {e}")
            return None
    
    def _extract_primary_key(self, content_type: ContentType, event_data: Dict[str, Any]) -> Optional[str]:
        """æå–ä¸»é”®"""
        if content_type == ContentType.FILE_PATH:
            return event_data.get("path")
        elif content_type == ContentType.URL:
            return event_data.get("url") or event_data.get("content")
        elif content_type == ContentType.EMAIL:
            return event_data.get("email") or event_data.get("content")
        elif content_type == ContentType.PHONE:
            return event_data.get("phone") or event_data.get("content")
        else:
            # é€šç”¨æƒ…å†µï¼šå°è¯•å¤šä¸ªå­—æ®µ
            for field in ["path", "url", "content", "text", "data", "value", "id"]:
                if field in event_data and event_data[field]:
                    return str(event_data[field])
        return None
    
    def _extract_display_name(self, content_type: ContentType, event_data: Dict[str, Any], primary_key: str) -> str:
        """æå–æ˜¾ç¤ºåç§°"""
        # ä¼˜å…ˆä½¿ç”¨æ˜ç¡®çš„åç§°å­—æ®µ
        if "name" in event_data and event_data["name"]:
            return str(event_data["name"])
        
        # æ ¹æ®å†…å®¹ç±»å‹æ™ºèƒ½æå–
        if content_type == ContentType.FILE_PATH:
            return Path(primary_key).name if primary_key else "Unknown File"
        elif content_type == ContentType.URL:
            # æå–åŸŸåä½œä¸ºæ˜¾ç¤ºåç§°
            try:
                from urllib.parse import urlparse
                parsed = urlparse(primary_key)
                return parsed.netloc or primary_key
            except:
                return primary_key
        else:
            # æˆªæ–­é•¿æ–‡æœ¬
            display = primary_key if primary_key else "Unknown"
            return display[:100] + "..." if len(display) > 100 else display
    
    def _extract_searchable_text(self, content_type: ContentType, event_data: Dict[str, Any], display_name: str) -> str:
        """æå–å¯æœç´¢æ–‡æœ¬"""
        text_parts = [display_name]
        
        # æ·»åŠ å†…å®¹å­—æ®µ
        for field in ["content", "text", "description", "title", "body"]:
            if field in event_data and event_data[field]:
                text_parts.append(str(event_data[field]))
        
        # å¯¹äºæ–‡ä»¶è·¯å¾„ï¼Œæ·»åŠ è·¯å¾„ç»„ä»¶
        if content_type == ContentType.FILE_PATH:
            path = event_data.get("path", "")
            if path:
                # æ·»åŠ ç›®å½•å
                text_parts.extend(Path(path).parts)
        
        return " ".join(text_parts)
    
    def _extract_timestamp(self, timestamp_data: Any) -> Optional[datetime]:
        """æå–æ—¶é—´æˆ³"""
        if not timestamp_data:
            return None
            
        try:
            if isinstance(timestamp_data, (int, float)):
                return datetime.fromtimestamp(timestamp_data)
            elif isinstance(timestamp_data, str):
                return datetime.fromisoformat(timestamp_data.replace('Z', '+00:00'))
            else:
                return None
        except:
            return None
    
    def _extract_keywords(self, searchable_text: str, event_data: Dict[str, Any]) -> Set[str]:
        """æå–å…³é”®è¯"""
        keywords = set()
        
        # ä»æœç´¢æ–‡æœ¬ä¸­æå–
        if searchable_text:
            # ç®€å•åˆ†è¯ (å¯ä»¥åç»­ä½¿ç”¨æ›´å¤æ‚çš„NLP)
            words = searchable_text.lower().split()
            keywords.update(word.strip(".,!?;:\"'()[]{}") for word in words if len(word) >= 3)
        
        # ä»æ‰©å±•åæå– (æ–‡ä»¶ç³»ç»Ÿ)
        if "extension" in event_data and event_data["extension"]:
            ext = str(event_data["extension"]).lower().lstrip(".")
            keywords.add(ext)
        
        return keywords
    
    def _extract_tags(self, content_type: ContentType, event_data: Dict[str, Any], metadata: Dict[str, Any]) -> Set[str]:
        """æå–æ ‡ç­¾"""
        tags = set()
        
        # å†…å®¹ç±»å‹æ ‡ç­¾
        tags.add(content_type.value)
        
        # ä»å…ƒæ•°æ®æå–
        if metadata:
            for key, value in metadata.items():
                if isinstance(value, str) and len(value) < 50:
                    tags.add(f"{key}:{value}")
        
        # æ–‡ä»¶ç‰¹å®šæ ‡ç­¾
        if content_type == ContentType.FILE_PATH:
            if event_data.get("is_directory"):
                tags.add("directory")
            else:
                tags.add("file")
                
            # æ–‡ä»¶å¤§å°æ ‡ç­¾
            size = event_data.get("size", 0)
            if isinstance(size, (int, float)):
                if size < 1024:
                    tags.add("size:tiny")
                elif size < 1024 * 1024:
                    tags.add("size:small") 
                elif size < 1024 * 1024 * 10:
                    tags.add("size:medium")
                else:
                    tags.add("size:large")
        
        return tags
    
    def _prepare_db_row(self, entry: UniversalIndexEntry) -> tuple:
        """å‡†å¤‡æ•°æ®åº“è¡Œæ•°æ®"""
        import json
        
        return (
            entry.id,
            entry.connector_id,
            entry.content_type.value,
            entry.primary_key,
            entry.primary_key.lower(),
            entry.searchable_text,
            entry.searchable_text.lower(),
            entry.display_name,
            entry.display_name.lower(),
            entry.index_tier.value,
            entry.priority,
            int(entry.indexed_at.timestamp()),
            int(entry.last_modified.timestamp()) if entry.last_modified else None,
            int(entry.last_accessed.timestamp()) if entry.last_accessed else None,
            json.dumps(entry.structured_data) if entry.structured_data else "{}",
            json.dumps(entry.metadata) if entry.metadata else "{}",
            ",".join(entry.keywords) if entry.keywords else "",
            ",".join(entry.tags) if entry.tags else ""
        )
    
    def _update_stats_batch(self, insert_data: List[tuple], timestamp: int):
        """æ‰¹é‡æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # æ›´æ–°åŸºç¡€ç»Ÿè®¡
            total_count = len(insert_data)
            self._stats['total_entries'] += total_count
            self._stats['last_update'] = datetime.fromtimestamp(timestamp)
            
            # æ›´æ–°åˆ†å±‚ç»Ÿè®¡
            for row in insert_data:
                tier = row[9]  # index_tier
                content_type = row[2]  # content_type
                connector_id = row[1]  # connector_id
                
                self._stats['entries_by_tier'][tier] = self._stats['entries_by_tier'].get(tier, 0) + 1
                self._stats['entries_by_type'][content_type] = self._stats['entries_by_type'].get(content_type, 0) + 1
                self._stats['entries_by_connector'][connector_id] = self._stats['entries_by_connector'].get(connector_id, 0) + 1
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    def search(self, query: SearchQuery) -> List[SearchResult]:
        """
        é€šç”¨æœç´¢æ¥å£ - æ”¯æŒæ‰€æœ‰è¿æ¥å™¨ç±»å‹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._initialized:
            if not self.initialize():
                return []
        
        start_time = time.time()
        
        try:
            with self._db_lock:
                cursor = self._universal_db_connection.cursor()
                
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                conditions = []
                params = []
                
                # æ–‡æœ¬æœç´¢
                if query.text.strip():
                    query_lower = query.text.lower().replace('*', '%').replace('?', '_')
                    if '%' in query_lower or '_' in query_lower:
                        conditions.append("(searchable_text_lower LIKE ? OR display_name_lower LIKE ? OR primary_key_lower LIKE ?)")
                        params.extend([query_lower, query_lower, query_lower])
                    else:
                        conditions.append("(searchable_text_lower LIKE ? OR display_name_lower LIKE ? OR primary_key_lower LIKE ?)")
                        query_pattern = f"%{query_lower}%"
                        params.extend([query_pattern, query_pattern, query_pattern])
                
                # å†…å®¹ç±»å‹è¿‡æ»¤
                if query.content_types:
                    type_placeholders = ",".join("?" * len(query.content_types))
                    conditions.append(f"content_type IN ({type_placeholders})")
                    params.extend([ct.value for ct in query.content_types])
                
                # è¿æ¥å™¨è¿‡æ»¤
                if query.connector_ids:
                    connector_placeholders = ",".join("?" * len(query.connector_ids))
                    conditions.append(f"connector_id IN ({connector_placeholders})")
                    params.extend(query.connector_ids)
                
                # ç´¢å¼•å±‚çº§è¿‡æ»¤
                if query.index_tiers:
                    tier_placeholders = ",".join("?" * len(query.index_tiers))
                    conditions.append(f"index_tier IN ({tier_placeholders})")
                    params.extend([tier.value for tier in query.index_tiers])
                
                # ä¼˜å…ˆçº§è¿‡æ»¤
                conditions.append("priority BETWEEN ? AND ?")
                params.extend([query.min_priority, query.max_priority])
                
                # æ„å»ºå®Œæ•´æŸ¥è¯¢
                where_clause = " AND ".join(conditions) if conditions else "1=1"
                
                # æ ¹æ®ç´¢å¼•å±‚çº§ä¼˜åŒ–æ’åºç­–ç•¥
                if IndexTier.HOT in query.index_tiers:
                    # çƒ­å±‚ä¼˜å…ˆï¼Œæé€Ÿå“åº”
                    order_by = "index_tier ASC, priority ASC, display_name_lower ASC"
                else:
                    # æ ‡å‡†æ’åº
                    order_by = "priority ASC, index_tier ASC, last_modified DESC"
                
                if query.include_metadata:
                    select_fields = "*"
                else:
                    select_fields = "id, connector_id, content_type, primary_key, searchable_text, display_name, index_tier, priority, indexed_at, last_modified"
                
                sql = f"""
                    SELECT {select_fields}
                    FROM universal_index 
                    WHERE {where_clause}
                    ORDER BY {order_by}
                    LIMIT ?
                """
                params.append(query.limit)
                
                cursor.execute(sql, params)
                results = cursor.fetchall()
                
                # è½¬æ¢ç»“æœæ ¼å¼
                search_results = []
                for row in results:
                    try:
                        result = self._convert_db_row_to_result(row, query.include_metadata)
                        if result:
                            search_results.append(result)
                    except Exception as e:
                        logger.warning(f"è½¬æ¢æœç´¢ç»“æœå¤±è´¥: {e}")
                        continue
                
                # è®°å½•æœç´¢æ€§èƒ½
                search_time_ms = (time.time() - start_time) * 1000
                self._record_search_performance(query, len(search_results), search_time_ms)
                
                logger.debug(f"ğŸ” é€šç”¨æœç´¢å®Œæˆ: æŸ¥è¯¢='{query.text}', ç»“æœ={len(search_results)}, è€—æ—¶={search_time_ms:.1f}ms")
                return search_results
                
        except Exception as e:
            logger.error(f"âŒ é€šç”¨æœç´¢å¤±è´¥: {e}")
            return []
    
    def _convert_db_row_to_result(self, row: tuple, include_metadata: bool) -> Optional[SearchResult]:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºæœç´¢ç»“æœ"""
        try:
            import json
            
            if include_metadata:
                # å®Œæ•´æ•°æ®
                (id, connector_id, content_type, primary_key, _, searchable_text, _, 
                 display_name, _, index_tier, priority, indexed_at, last_modified, 
                 last_accessed, structured_data, metadata, keywords, tags) = row
                
                structured_data_dict = json.loads(structured_data) if structured_data else {}
                metadata_dict = json.loads(metadata) if metadata else {}
                keywords_set = set(keywords.split(",")) if keywords else set()
                tags_set = set(tags.split(",")) if tags else set()
            else:
                # åŸºç¡€æ•°æ®
                (id, connector_id, content_type, primary_key, searchable_text,
                 display_name, index_tier, priority, indexed_at, last_modified) = row
                
                structured_data_dict = {}
                metadata_dict = {}
                keywords_set = set()
                tags_set = set()
                last_accessed = None
            
            entry = UniversalIndexEntry(
                id=id,
                connector_id=connector_id,
                content_type=ContentType(content_type),
                primary_key=primary_key,
                searchable_text=searchable_text,
                display_name=display_name,
                index_tier=IndexTier(index_tier),
                priority=priority,
                indexed_at=datetime.fromtimestamp(indexed_at),
                last_modified=datetime.fromtimestamp(last_modified) if last_modified else None,
                last_accessed=datetime.fromtimestamp(last_accessed) if last_accessed else None,
                structured_data=structured_data_dict,
                metadata=metadata_dict,
                keywords=keywords_set,
                tags=tags_set
            )
            
            # è®¡ç®—ç®€å•çš„ç›¸å…³æ€§è¯„åˆ† (å¯ä»¥åç»­ä¼˜åŒ–)
            score = 10.0 - priority  # ä¼˜å…ˆçº§è¶Šä½ï¼Œåˆ†æ•°è¶Šé«˜
            
            return SearchResult(entry=entry, score=score)
            
        except Exception as e:
            logger.error(f"è½¬æ¢æœç´¢ç»“æœå¤±è´¥: {e}")
            return None
    
    def _record_search_performance(self, query: SearchQuery, result_count: int, search_time_ms: float):
        """è®°å½•æœç´¢æ€§èƒ½"""
        try:
            import json
            
            self._stats['search_requests'] += 1
            
            # æ›´æ–°å¹³å‡æœç´¢æ—¶é—´
            current_avg = self._stats['avg_search_time_ms']
            request_count = self._stats['search_requests']
            self._stats['avg_search_time_ms'] = (current_avg * (request_count - 1) + search_time_ms) / request_count
            
            # è®°å½•è¯¦ç»†æ—¥å¿— (é‡‡æ ·è®°å½•ï¼Œé¿å…æ—¥å¿—è¿‡å¤š)
            if self._stats['search_requests'] % 100 == 0:  # æ¯100æ¬¡è®°å½•ä¸€æ¬¡
                with self._db_lock:
                    cursor = self._universal_db_connection.cursor()
                    cursor.execute("""
                        INSERT INTO search_log (
                            query_text, content_types, connector_ids, 
                            result_count, search_time_ms, timestamp
                        ) VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        query.text,
                        json.dumps([ct.value for ct in query.content_types]),
                        json.dumps(query.connector_ids),
                        result_count,
                        search_time_ms,
                        int(time.time())
                    ))
                    self._universal_db_connection.commit()
            
        except Exception as e:
            logger.warning(f"è®°å½•æœç´¢æ€§èƒ½å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        if not self._initialized:
            return self._stats.copy()
        
        try:
            with self._db_lock:
                cursor = self._universal_db_connection.cursor()
                
                # è·å–å®æ—¶æ€»æ•°
                total_count = cursor.execute("SELECT COUNT(*) FROM universal_index").fetchone()[0]
                
                # æ›´æ–°ç»Ÿè®¡
                updated_stats = self._stats.copy()
                updated_stats['total_entries'] = total_count
                updated_stats['database_size'] = self._universal_db_path.stat().st_size if self._universal_db_path.exists() else 0
                
                return updated_stats
                
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return self._stats.copy()
    
    def clear_index(self, connector_id: Optional[str] = None) -> bool:
        """æ¸…ç©ºç´¢å¼• (å¯æŒ‡å®šè¿æ¥å™¨)"""
        if not self._initialized:
            return True
            
        try:
            with self._db_lock:
                cursor = self._universal_db_connection.cursor()
                
                if connector_id:
                    cursor.execute("DELETE FROM universal_index WHERE connector_id = ?", (connector_id,))
                    logger.info(f"ğŸ§¹ æ¸…ç©ºè¿æ¥å™¨ç´¢å¼•: {connector_id}")
                else:
                    cursor.execute("DELETE FROM universal_index")
                    cursor.execute("DELETE FROM index_stats") 
                    cursor.execute("DELETE FROM search_log")
                    logger.info("ğŸ§¹ æ¸…ç©ºå…¨éƒ¨ç´¢å¼•")
                
                self._universal_db_connection.commit()
                
                # é‡ç½®ç»Ÿè®¡
                if not connector_id:
                    self._stats = {
                        'total_entries': 0,
                        'entries_by_tier': {tier.value: 0 for tier in IndexTier},
                        'entries_by_type': {ct.value: 0 for ct in ContentType},
                        'entries_by_connector': {},
                        'last_update': None,
                        'search_requests': 0,
                        'avg_search_time_ms': 0.0
                    }
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def close(self):
        """å…³é—­æœåŠ¡"""
        if self._universal_db_connection:
            self._universal_db_connection.close()
            self._universal_db_connection = None
        self._initialized = False
        logger.info("ğŸ“¦ é€šç”¨ç´¢å¼•æœåŠ¡å·²å…³é—­")


# å…¨å±€å®ä¾‹ç®¡ç†
_universal_index_service: Optional[UniversalIndexService] = None


def get_universal_index_service() -> UniversalIndexService:
    """è·å–é€šç”¨ç´¢å¼•æœåŠ¡å•ä¾‹"""
    global _universal_index_service
    
    if _universal_index_service is None:
        _universal_index_service = UniversalIndexService()
        _universal_index_service.initialize()
    
    return _universal_index_service


def cleanup_universal_index_service():
    """æ¸…ç†é€šç”¨ç´¢å¼•æœåŠ¡"""
    global _universal_index_service
    
    if _universal_index_service:
        try:
            _universal_index_service.close()
        except Exception as e:
            logger.error(f"æ¸…ç†é€šç”¨ç´¢å¼•æœåŠ¡å¤±è´¥: {e}")
        finally:
            _universal_index_service = None