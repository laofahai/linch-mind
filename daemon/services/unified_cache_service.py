#!/usr/bin/env python3
"""
ç»Ÿä¸€ç¼“å­˜æœåŠ¡ - æ¶ˆé™¤6ä¸ªé‡å¤ç¼“å­˜å®ç°
æ•´åˆå†…å­˜ç¼“å­˜ã€ç£ç›˜ç¼“å­˜ã€åˆ†å±‚ç¼“å­˜ä¸ºç»Ÿä¸€æ¥å£
"""

import asyncio
import json
import logging
import pickle
import time
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable
from datetime import datetime, timedelta
import hashlib

logger = logging.getLogger(__name__)


class CacheType(Enum):
    """ç¼“å­˜ç±»å‹æšä¸¾"""
    MEMORY = "memory"           # å†…å­˜ç¼“å­˜
    DISK = "disk"              # ç£ç›˜ç¼“å­˜  
    HYBRID = "hybrid"          # æ··åˆç¼“å­˜(å†…å­˜+ç£ç›˜)
    HOT = "hot"                # çƒ­ç¼“å­˜(æœ€è¿‘è®¿é—®)
    WARM = "warm"              # æ¸©ç¼“å­˜(è¾ƒå°‘è®¿é—®)
    PERSISTENT = "persistent"   # æŒä¹…åŒ–ç¼“å­˜


class SerializationFormat(Enum):
    """åºåˆ—åŒ–æ ¼å¼æšä¸¾"""
    JSON = "json"
    PICKLE = "pickle"
    RAW = "raw"                # åŸå§‹æ•°æ®(ä¸åºåˆ—åŒ–)


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    created_at: float = field(default_factory=time.time)
    accessed_at: float = field(default_factory=time.time)
    ttl: Optional[float] = None          # ç”Ÿå­˜æ—¶é—´(ç§’)
    access_count: int = 0                # è®¿é—®æ¬¡æ•°
    size_bytes: int = 0                  # æ•°æ®å¤§å°
    cache_type: CacheType = CacheType.MEMORY
    format: SerializationFormat = SerializationFormat.RAW


@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    total_entries: int = 0
    memory_entries: int = 0
    disk_entries: int = 0
    total_hits: int = 0
    total_misses: int = 0
    total_evictions: int = 0
    memory_usage_bytes: int = 0
    disk_usage_bytes: int = 0
    hit_rate: float = 0.0
    last_cleanup: Optional[datetime] = None


class UnifiedCacheService:
    """ç»Ÿä¸€ç¼“å­˜æœåŠ¡ - æ¶ˆé™¤6ä¸ªé‡å¤ç¼“å­˜å®ç°"""
    
    def __init__(
        self,
        cache_dir: Optional[Path] = None,
        memory_limit: int = 1000,      # å†…å­˜ç¼“å­˜æ¡ç›®é™åˆ¶
        hot_cache_size: int = 100,     # çƒ­ç¼“å­˜å¤§å°
        warm_cache_size: int = 500,    # æ¸©ç¼“å­˜å¤§å°
        default_ttl: int = 3600,       # é»˜è®¤TTL(ç§’)
        cleanup_interval: int = 300    # æ¸…ç†é—´éš”(ç§’)
    ):
        # ç¼“å­˜å­˜å‚¨
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.hot_cache: Dict[str, CacheEntry] = {}    # æœ€è¿‘è®¿é—®
        self.warm_cache: Dict[str, CacheEntry] = {}   # è¾ƒå°‘è®¿é—®
        
        # é…ç½®å‚æ•°
        self.memory_limit = memory_limit
        self.hot_cache_size = hot_cache_size
        self.warm_cache_size = warm_cache_size
        self.default_ttl = default_ttl
        self.cleanup_interval = cleanup_interval
        
        # ç£ç›˜ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        if self.cache_dir:
            self.cache_dir = Path(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = CacheStats()
        
        # è®¿é—®é¢‘ç‡è·Ÿè¸ª
        self._access_order: List[str] = []  # LRUé¡ºåº
        self._last_cleanup = time.time()
        
        # åºåˆ—åŒ–å™¨æ˜ å°„
        self._serializers: Dict[SerializationFormat, Callable] = {
            SerializationFormat.JSON: self._json_serialize,
            SerializationFormat.PICKLE: self._pickle_serialize,
            SerializationFormat.RAW: self._raw_serialize,
        }
        
        self._deserializers: Dict[SerializationFormat, Callable] = {
            SerializationFormat.JSON: self._json_deserialize,
            SerializationFormat.PICKLE: self._pickle_deserialize,
            SerializationFormat.RAW: self._raw_deserialize,
        }
        
        logger.info(f"ğŸ’¾ UnifiedCacheService åˆå§‹åŒ– (å†…å­˜é™åˆ¶: {memory_limit}, çƒ­ç¼“å­˜: {hot_cache_size})")

    async def get(
        self, 
        key: str, 
        cache_type: CacheType = CacheType.MEMORY,
        default: Any = None
    ) -> Any:
        """ç»Ÿä¸€ç¼“å­˜è·å–æ¥å£"""
        try:
            # ç”Ÿæˆæ ‡å‡†åŒ–é”®
            cache_key = self._normalize_key(key)
            
            # é¦–å…ˆæ£€æŸ¥çƒ­ç¼“å­˜
            if cache_key in self.hot_cache:
                entry = self.hot_cache[cache_key]
                if not self._is_expired(entry):
                    self._update_access(entry)
                    self.stats.total_hits += 1
                    logger.debug(f"ç¼“å­˜å‘½ä¸­ (çƒ­): {cache_key}")
                    return entry.value
                else:
                    # è¿‡æœŸé¡¹ç›®æ¸…ç†
                    del self.hot_cache[cache_key]
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if cache_key in self.memory_cache:
                entry = self.memory_cache[cache_key]
                if not self._is_expired(entry):
                    self._update_access(entry)
                    # æå‡åˆ°çƒ­ç¼“å­˜
                    self._promote_to_hot(cache_key, entry)
                    self.stats.total_hits += 1
                    logger.debug(f"ç¼“å­˜å‘½ä¸­ (å†…å­˜): {cache_key}")
                    return entry.value
                else:
                    del self.memory_cache[cache_key]
            
            # æ£€æŸ¥æ¸©ç¼“å­˜
            if cache_key in self.warm_cache:
                entry = self.warm_cache[cache_key]
                if not self._is_expired(entry):
                    self._update_access(entry)
                    # æå‡åˆ°å†…å­˜ç¼“å­˜
                    self._promote_to_memory(cache_key, entry)
                    self.stats.total_hits += 1
                    logger.debug(f"ç¼“å­˜å‘½ä¸­ (æ¸©): {cache_key}")
                    return entry.value
                else:
                    del self.warm_cache[cache_key]
            
            # æ£€æŸ¥ç£ç›˜ç¼“å­˜
            if cache_type in [CacheType.DISK, CacheType.HYBRID, CacheType.PERSISTENT]:
                disk_value = await self._load_from_disk(cache_key)
                if disk_value is not None:
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    await self.set(cache_key, disk_value, CacheType.MEMORY, ttl=self.default_ttl)
                    self.stats.total_hits += 1
                    logger.debug(f"ç¼“å­˜å‘½ä¸­ (ç£ç›˜): {cache_key}")
                    return disk_value
            
            # ç¼“å­˜æœªå‘½ä¸­
            self.stats.total_misses += 1
            logger.debug(f"ç¼“å­˜æœªå‘½ä¸­: {cache_key}")
            return default
            
        except Exception as e:
            logger.error(f"ç¼“å­˜è·å–å¤±è´¥ [{key}]: {e}")
            return default

    async def set(
        self,
        key: str,
        value: Any,
        cache_type: CacheType = CacheType.MEMORY,
        ttl: Optional[int] = None,
        format: SerializationFormat = SerializationFormat.RAW
    ) -> bool:
        """ç»Ÿä¸€ç¼“å­˜è®¾ç½®æ¥å£"""
        try:
            cache_key = self._normalize_key(key)
            
            if ttl is None:
                ttl = self.default_ttl
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=cache_key,
                value=value,
                ttl=ttl,
                cache_type=cache_type,
                format=format,
                size_bytes=self._estimate_size(value)
            )
            
            # æ ¹æ®ç¼“å­˜ç±»å‹å­˜å‚¨
            if cache_type == CacheType.HOT:
                await self._set_hot_cache(cache_key, entry)
                
            elif cache_type == CacheType.WARM:
                await self._set_warm_cache(cache_key, entry)
                
            elif cache_type == CacheType.DISK:
                await self._save_to_disk(cache_key, entry)
                
            elif cache_type == CacheType.PERSISTENT:
                await self._save_to_disk(cache_key, entry)
                await self._set_memory_cache(cache_key, entry)
                
            elif cache_type == CacheType.HYBRID:
                await self._set_memory_cache(cache_key, entry)
                await self._save_to_disk(cache_key, entry)
                
            else:  # CacheType.MEMORY
                await self._set_memory_cache(cache_key, entry)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats.total_entries = (
                len(self.memory_cache) + 
                len(self.hot_cache) + 
                len(self.warm_cache)
            )
            
            # å®šæœŸæ¸…ç†
            await self._maybe_cleanup()
            
            logger.debug(f"ç¼“å­˜è®¾ç½®æˆåŠŸ [{cache_type.value}]: {cache_key}")
            return True
            
        except Exception as e:
            logger.error(f"ç¼“å­˜è®¾ç½®å¤±è´¥ [{key}]: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜é¡¹"""
        try:
            cache_key = self._normalize_key(key)
            deleted = False
            
            # ä»æ‰€æœ‰ç¼“å­˜ä¸­åˆ é™¤
            if cache_key in self.hot_cache:
                del self.hot_cache[cache_key]
                deleted = True
            
            if cache_key in self.memory_cache:
                del self.memory_cache[cache_key]
                deleted = True
            
            if cache_key in self.warm_cache:
                del self.warm_cache[cache_key]
                deleted = True
            
            # ä»è®¿é—®è®°å½•ä¸­åˆ é™¤
            if cache_key in self._access_order:
                self._access_order.remove(cache_key)
            
            # åˆ é™¤ç£ç›˜æ–‡ä»¶
            if self.cache_dir:
                disk_file = self.cache_dir / f"{cache_key}.cache"
                if disk_file.exists():
                    disk_file.unlink()
                    deleted = True
            
            if deleted:
                logger.debug(f"ç¼“å­˜åˆ é™¤æˆåŠŸ: {cache_key}")
            
            return deleted
            
        except Exception as e:
            logger.error(f"ç¼“å­˜åˆ é™¤å¤±è´¥ [{key}]: {e}")
            return False

    async def clear(self, cache_type: Optional[CacheType] = None):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            if cache_type is None:
                # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
                self.memory_cache.clear()
                self.hot_cache.clear()
                self.warm_cache.clear()
                self._access_order.clear()
                
                # æ¸…ç©ºç£ç›˜ç¼“å­˜
                if self.cache_dir and self.cache_dir.exists():
                    for cache_file in self.cache_dir.glob("*.cache"):
                        cache_file.unlink()
                
            elif cache_type == CacheType.HOT:
                self.hot_cache.clear()
            elif cache_type == CacheType.WARM:
                self.warm_cache.clear()
            elif cache_type == CacheType.MEMORY:
                self.memory_cache.clear()
            elif cache_type == CacheType.DISK:
                if self.cache_dir and self.cache_dir.exists():
                    for cache_file in self.cache_dir.glob("*.cache"):
                        cache_file.unlink()
            
            # é‡ç½®ç»Ÿè®¡
            self.stats = CacheStats()
            
            logger.info(f"ç¼“å­˜å·²æ¸…ç©º [{cache_type.value if cache_type else 'all'}]")
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")

    async def invalidate_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼å¤±æ•ˆç¼“å­˜"""
        try:
            import re
            pattern_re = re.compile(pattern)
            
            keys_to_delete = []
            
            # æŸ¥æ‰¾åŒ¹é…çš„é”®
            for cache_dict in [self.hot_cache, self.memory_cache, self.warm_cache]:
                for key in cache_dict.keys():
                    if pattern_re.search(key):
                        keys_to_delete.append(key)
            
            # åˆ é™¤åŒ¹é…çš„é”®
            for key in keys_to_delete:
                await self.delete(key)
            
            logger.info(f"æ¨¡å¼å¤±æ•ˆå®Œæˆ [{pattern}]: {len(keys_to_delete)} é¡¹")
            
        except Exception as e:
            logger.error(f"æ¨¡å¼å¤±æ•ˆå¤±è´¥ [{pattern}]: {e}")

    # === ä¾¿æ·æ–¹æ³• - æ›¿ä»£åŸæœ‰å„ç§ç¼“å­˜API ===
    
    async def get_or_set(
        self,
        key: str,
        factory: Callable,
        cache_type: CacheType = CacheType.MEMORY,
        ttl: Optional[int] = None
    ) -> Any:
        """è·å–ç¼“å­˜æˆ–è®¾ç½®æ–°å€¼"""
        value = await self.get(key, cache_type)
        if value is None:
            # ç”Ÿæˆæ–°å€¼
            if asyncio.iscoroutinefunction(factory):
                value = await factory()
            else:
                value = factory()
            
            # ç¼“å­˜æ–°å€¼
            await self.set(key, value, cache_type, ttl)
        
        return value
    
    async def mget(self, keys: List[str], cache_type: CacheType = CacheType.MEMORY) -> Dict[str, Any]:
        """æ‰¹é‡è·å–ç¼“å­˜"""
        results = {}
        for key in keys:
            results[key] = await self.get(key, cache_type)
        return results
    
    async def mset(self, items: Dict[str, Any], cache_type: CacheType = CacheType.MEMORY, ttl: Optional[int] = None) -> bool:
        """æ‰¹é‡è®¾ç½®ç¼“å­˜"""
        try:
            for key, value in items.items():
                await self.set(key, value, cache_type, ttl)
            return True
        except Exception as e:
            logger.error(f"æ‰¹é‡è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False

    # === å†…éƒ¨æ–¹æ³• ===
    
    def _normalize_key(self, key: str) -> str:
        """æ ‡å‡†åŒ–ç¼“å­˜é”®"""
        # ä½¿ç”¨MD5å“ˆå¸Œç¡®ä¿é”®çš„ä¸€è‡´æ€§
        return hashlib.md5(key.encode('utf-8')).hexdigest()
    
    def _is_expired(self, entry: CacheEntry) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ¡ç›®æ˜¯å¦è¿‡æœŸ"""
        if entry.ttl is None:
            return False
        return (time.time() - entry.created_at) > entry.ttl
    
    def _update_access(self, entry: CacheEntry):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        entry.accessed_at = time.time()
        entry.access_count += 1
        
        # æ›´æ–°LRUé¡ºåº
        if entry.key in self._access_order:
            self._access_order.remove(entry.key)
        self._access_order.append(entry.key)
    
    def _promote_to_hot(self, key: str, entry: CacheEntry):
        """æå‡åˆ°çƒ­ç¼“å­˜"""
        if len(self.hot_cache) >= self.hot_cache_size:
            # ç§»é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
            self._evict_from_hot()
        
        self.hot_cache[key] = entry
        
        # ä»å…¶ä»–ç¼“å­˜ä¸­ç§»é™¤
        self.memory_cache.pop(key, None)
        self.warm_cache.pop(key, None)
    
    def _promote_to_memory(self, key: str, entry: CacheEntry):
        """æå‡åˆ°å†…å­˜ç¼“å­˜"""
        if len(self.memory_cache) >= self.memory_limit:
            self._evict_from_memory()
        
        self.memory_cache[key] = entry
        self.warm_cache.pop(key, None)
    
    async def _set_hot_cache(self, key: str, entry: CacheEntry):
        """è®¾ç½®çƒ­ç¼“å­˜"""
        if len(self.hot_cache) >= self.hot_cache_size:
            self._evict_from_hot()
        
        self.hot_cache[key] = entry
        self.stats.memory_entries += 1
    
    async def _set_memory_cache(self, key: str, entry: CacheEntry):
        """è®¾ç½®å†…å­˜ç¼“å­˜"""
        if len(self.memory_cache) >= self.memory_limit:
            self._evict_from_memory()
        
        self.memory_cache[key] = entry
        self.stats.memory_entries += 1
    
    async def _set_warm_cache(self, key: str, entry: CacheEntry):
        """è®¾ç½®æ¸©ç¼“å­˜"""
        if len(self.warm_cache) >= self.warm_cache_size:
            self._evict_from_warm()
        
        self.warm_cache[key] = entry
    
    def _evict_from_hot(self):
        """ä»çƒ­ç¼“å­˜ä¸­æ·˜æ±°"""
        if not self.hot_cache:
            return
        
        # æ‰¾åˆ°æœ€å°‘ä½¿ç”¨çš„é¡¹
        lru_key = min(
            self.hot_cache.keys(),
            key=lambda k: self.hot_cache[k].accessed_at
        )
        
        # ç§»åŠ¨åˆ°å†…å­˜ç¼“å­˜
        entry = self.hot_cache.pop(lru_key)
        if len(self.memory_cache) < self.memory_limit:
            self.memory_cache[lru_key] = entry
        else:
            self.stats.total_evictions += 1
    
    def _evict_from_memory(self):
        """ä»å†…å­˜ç¼“å­˜ä¸­æ·˜æ±°"""
        if not self.memory_cache:
            return
        
        lru_key = min(
            self.memory_cache.keys(),
            key=lambda k: self.memory_cache[k].accessed_at
        )
        
        # ç§»åŠ¨åˆ°æ¸©ç¼“å­˜
        entry = self.memory_cache.pop(lru_key)
        if len(self.warm_cache) < self.warm_cache_size:
            self.warm_cache[lru_key] = entry
        else:
            self.stats.total_evictions += 1
    
    def _evict_from_warm(self):
        """ä»æ¸©ç¼“å­˜ä¸­æ·˜æ±°"""
        if not self.warm_cache:
            return
        
        lru_key = min(
            self.warm_cache.keys(),
            key=lambda k: self.warm_cache[k].accessed_at
        )
        
        self.warm_cache.pop(lru_key)
        self.stats.total_evictions += 1
    
    async def _save_to_disk(self, key: str, entry: CacheEntry):
        """ä¿å­˜åˆ°ç£ç›˜"""
        if not self.cache_dir:
            return
        
        try:
            disk_file = self.cache_dir / f"{key}.cache"
            serialized_data = self._serializers[entry.format](entry.value)
            
            # ä¿å­˜å…ƒæ•°æ®å’Œæ•°æ®
            cache_data = {
                "value": serialized_data,
                "created_at": entry.created_at,
                "ttl": entry.ttl,
                "format": entry.format.value
            }
            
            with open(disk_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            self.stats.disk_entries += 1
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç£ç›˜ç¼“å­˜å¤±è´¥ [{key}]: {e}")
    
    async def _load_from_disk(self, key: str) -> Optional[Any]:
        """ä»ç£ç›˜åŠ è½½"""
        if not self.cache_dir:
            return None
        
        try:
            disk_file = self.cache_dir / f"{key}.cache"
            if not disk_file.exists():
                return None
            
            with open(disk_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if cache_data.get("ttl"):
                if (time.time() - cache_data["created_at"]) > cache_data["ttl"]:
                    disk_file.unlink()
                    return None
            
            # ååºåˆ—åŒ–æ•°æ®
            format_enum = SerializationFormat(cache_data.get("format", "raw"))
            value = self._deserializers[format_enum](cache_data["value"])
            
            return value
            
        except Exception as e:
            logger.error(f"åŠ è½½ç£ç›˜ç¼“å­˜å¤±è´¥ [{key}]: {e}")
            return None
    
    def _estimate_size(self, value: Any) -> int:
        """ä¼°ç®—æ•°æ®å¤§å°"""
        try:
            return len(pickle.dumps(value))
        except:
            return 0
    
    # === åºåˆ—åŒ–å™¨ ===
    
    def _json_serialize(self, value: Any) -> str:
        """JSONåºåˆ—åŒ–"""
        return json.dumps(value, ensure_ascii=False)
    
    def _json_deserialize(self, data: str) -> Any:
        """JSONååºåˆ—åŒ–"""
        return json.loads(data)
    
    def _pickle_serialize(self, value: Any) -> bytes:
        """Pickleåºåˆ—åŒ–"""
        return pickle.dumps(value)
    
    def _pickle_deserialize(self, data: bytes) -> Any:
        """Pickleååºåˆ—åŒ–"""
        return pickle.loads(data)
    
    def _raw_serialize(self, value: Any) -> Any:
        """åŸå§‹æ•°æ®(ä¸åºåˆ—åŒ–)"""
        return value
    
    def _raw_deserialize(self, data: Any) -> Any:
        """åŸå§‹æ•°æ®(ä¸ååºåˆ—åŒ–)"""
        return data
    
    async def _maybe_cleanup(self):
        """å®šæœŸæ¸…ç†è¿‡æœŸé¡¹"""
        now = time.time()
        if (now - self._last_cleanup) > self.cleanup_interval:
            await self._cleanup_expired()
            self._last_cleanup = now
    
    async def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸé¡¹"""
        try:
            expired_keys = []
            
            # æ£€æŸ¥æ‰€æœ‰ç¼“å­˜ä¸­çš„è¿‡æœŸé¡¹
            for cache_dict in [self.hot_cache, self.memory_cache, self.warm_cache]:
                for key, entry in cache_dict.items():
                    if self._is_expired(entry):
                        expired_keys.append(key)
            
            # åˆ é™¤è¿‡æœŸé¡¹
            for key in expired_keys:
                await self.delete(key)
            
            if expired_keys:
                logger.debug(f"æ¸…ç†è¿‡æœŸç¼“å­˜é¡¹: {len(expired_keys)} ä¸ª")
            
            self.stats.last_cleanup = datetime.utcnow()
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")
    
    def get_stats(self) -> CacheStats:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats.memory_entries = len(self.memory_cache) + len(self.hot_cache)
        self.stats.total_entries = self.stats.memory_entries + len(self.warm_cache)
        
        # è®¡ç®—å‘½ä¸­ç‡
        total_requests = self.stats.total_hits + self.stats.total_misses
        if total_requests > 0:
            self.stats.hit_rate = self.stats.total_hits / total_requests
        
        # è®¡ç®—å†…å­˜ä½¿ç”¨
        memory_usage = 0
        for cache_dict in [self.hot_cache, self.memory_cache, self.warm_cache]:
            for entry in cache_dict.values():
                memory_usage += entry.size_bytes
        self.stats.memory_usage_bytes = memory_usage
        
        return self.stats


# å…¨å±€å®ä¾‹ç®¡ç†
_unified_cache_service: Optional[UnifiedCacheService] = None


def get_unified_cache_service(
    cache_dir: Optional[Path] = None,
    **kwargs
) -> UnifiedCacheService:
    """è·å–ç»Ÿä¸€ç¼“å­˜æœåŠ¡å®ä¾‹"""
    global _unified_cache_service
    
    if _unified_cache_service is None:
        from core.environment_manager import get_environment_manager
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç¼“å­˜ç›®å½•ï¼Œä½¿ç”¨ç¯å¢ƒç®¡ç†å™¨çš„ç¼“å­˜ç›®å½•
        if cache_dir is None:
            env_manager = get_environment_manager()
            cache_dir = env_manager.get_cache_dir()
        
        _unified_cache_service = UnifiedCacheService(
            cache_dir=cache_dir,
            **kwargs
        )
    
    return _unified_cache_service


async def cleanup_unified_cache_service():
    """æ¸…ç†ç»Ÿä¸€ç¼“å­˜æœåŠ¡"""
    global _unified_cache_service
    
    if _unified_cache_service:
        try:
            await _unified_cache_service.clear()
            logger.info("ğŸ’¾ UnifiedCacheService å·²æ¸…ç†")
        except Exception as e:
            logger.error(f"æ¸…ç†UnifiedCacheServiceå¤±è´¥: {e}")
        finally:
            _unified_cache_service = None