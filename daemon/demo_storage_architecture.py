#!/usr/bin/env python3
"""
ä¸‰å±‚å­˜å‚¨æ¶æ„æ¼”ç¤ºè„šæœ¬
å±•ç¤ºSQLite + NetworkX + FAISSçš„ååŒå·¥ä½œ
"""

import asyncio
import logging
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from services.storage.data_migration import get_migration_service
from services.storage.storage_orchestrator import get_storage_orchestrator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def demo_basic_operations():
    """æ¼”ç¤ºåŸºç¡€æ“ä½œ"""
    logger.info("ğŸš€ å¼€å§‹ä¸‰å±‚å­˜å‚¨æ¶æ„æ¼”ç¤º")

    try:
        # è·å–å­˜å‚¨ç¼–æ’å™¨
        storage = await get_storage_orchestrator()

        logger.info("ğŸ“Š è·å–ç³»ç»ŸæŒ‡æ ‡...")
        metrics = await storage.get_metrics()
        logger.info(f"ç³»ç»ŸçŠ¶æ€: {metrics.health_status}")
        logger.info(f"æ€»å®ä½“æ•°: {metrics.total_entities}")
        logger.info(f"æ€»å…³ç³»æ•°: {metrics.total_relationships}")
        logger.info(f"æ€»å‘é‡æ•°: {metrics.total_vectors}")

        # æ¼”ç¤ºåˆ›å»ºå®ä½“
        logger.info("ğŸ“ åˆ›å»ºç¤ºä¾‹å®ä½“...")

        entities = [
            {
                "id": "demo_ai_001",
                "name": "äººå·¥æ™ºèƒ½æ¦‚è¿°",
                "type": "concept",
                "content": "äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰æŠ€æœ¯ã€‚",
            },
            {
                "id": "demo_ml_001",
                "name": "æœºå™¨å­¦ä¹ åŸºç¡€",
                "type": "concept",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
            },
            {
                "id": "demo_dl_001",
                "name": "æ·±åº¦å­¦ä¹ åŸç†",
                "type": "concept",
                "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
            },
        ]

        for entity in entities:
            success = await storage.create_entity(
                entity_id=entity["id"],
                name=entity["name"],
                entity_type=entity["type"],
                content=entity["content"],
                auto_embed=True,
            )

            if success:
                logger.info(f"âœ… åˆ›å»ºå®ä½“: {entity['name']}")
            else:
                logger.error(f"âŒ åˆ›å»ºå®ä½“å¤±è´¥: {entity['name']}")

        # æ¼”ç¤ºåˆ›å»ºå…³ç³»
        logger.info("ğŸ”— åˆ›å»ºå®ä½“å…³ç³»...")

        relationships = [
            ("demo_ai_001", "demo_ml_001", "includes", 0.9),
            ("demo_ml_001", "demo_dl_001", "includes", 0.8),
            ("demo_ai_001", "demo_dl_001", "includes", 0.7),
        ]

        for source, target, rel_type, strength in relationships:
            success = await storage.create_relationship(
                source_entity=source,
                target_entity=target,
                relationship_type=rel_type,
                strength=strength,
                confidence=0.9,
            )

            if success:
                logger.info(f"âœ… åˆ›å»ºå…³ç³»: {source} -> {target}")
            else:
                logger.error(f"âŒ åˆ›å»ºå…³ç³»å¤±è´¥: {source} -> {target}")

        # æ¼”ç¤ºè¯­ä¹‰æœç´¢
        logger.info("ğŸ” æ¼”ç¤ºè¯­ä¹‰æœç´¢...")

        search_queries = ["ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "æœºå™¨å­¦ä¹ ç®—æ³•", "ç¥ç»ç½‘ç»œæ·±åº¦å­¦ä¹ "]

        for query in search_queries:
            logger.info(f"æœç´¢æŸ¥è¯¢: '{query}'")
            results = await storage.semantic_search(query=query, k=3)

            for i, result in enumerate(results, 1):
                logger.info(
                    f"  {i}. {result.metadata.get('entity_name', 'Unknown')} "
                    f"(ç›¸ä¼¼åº¦: {result.score:.3f})"
                )

        # æ¼”ç¤ºå›¾æœç´¢
        logger.info("ğŸ“Š æ¼”ç¤ºå›¾ç»“æ„æœç´¢...")

        neighbors = await storage.graph_search(
            entity_id="demo_ai_001", max_depth=2, max_results=10
        )

        logger.info(f"AIå®ä½“çš„é‚»å±…èŠ‚ç‚¹ (2åº¦å†…): {neighbors}")

        # æ¼”ç¤ºæ™ºèƒ½æ¨è
        logger.info("ğŸ¯ æ¼”ç¤ºæ™ºèƒ½æ¨è...")

        recommendations = await storage.get_smart_recommendations(
            max_recommendations=5, algorithm="hybrid"
        )

        logger.info("æ¨èç»“æœ:")
        for i, rec in enumerate(recommendations, 1):
            logger.info(
                f"  {i}. {rec.entity_name} (åˆ†æ•°: {rec.score:.3f}, "
                f"æ¥æº: {rec.source}, åŸå› : {rec.reason})"
            )

        # æœ€ç»ˆç³»ç»ŸæŒ‡æ ‡
        logger.info("ğŸ“ˆ æ›´æ–°åçš„ç³»ç»ŸæŒ‡æ ‡...")
        final_metrics = await storage.get_metrics()
        logger.info(f"æ€»å®ä½“æ•°: {final_metrics.total_entities}")
        logger.info(f"æ€»å…³ç³»æ•°: {final_metrics.total_relationships}")
        logger.info(f"æ€»å‘é‡æ•°: {final_metrics.total_vectors}")
        logger.info(f"å­˜å‚¨ä½¿ç”¨é‡: {final_metrics.storage_usage_mb:.2f} MB")

        logger.info("âœ… ä¸‰å±‚å­˜å‚¨æ¶æ„æ¼”ç¤ºå®Œæˆï¼")

    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback

        logger.error(traceback.format_exc())


async def demo_data_migration():
    """æ¼”ç¤ºæ•°æ®è¿ç§»åŠŸèƒ½"""
    logger.info("ğŸ”„ æ¼”ç¤ºæ•°æ®è¿ç§»åŠŸèƒ½...")

    try:
        migration_service = await get_migration_service()

        # æ£€æŸ¥è¿ç§»çŠ¶æ€
        logger.info("æ£€æŸ¥å½“å‰è¿ç§»çŠ¶æ€...")
        status = await migration_service.check_migration_status()

        logger.info(
            f"SQLiteå®ä½“æ•°: {status.get('sqlite_stats', {}).get('entities', 0)}"
        )
        logger.info(f"å›¾æ•°æ®åº“èŠ‚ç‚¹æ•°: {status.get('graph_stats', {}).get('nodes', 0)}")
        logger.info(
            f"å‘é‡æ•°æ®åº“æ–‡æ¡£æ•°: {status.get('vector_stats', {}).get('documents', 0)}"
        )
        logger.info(f"éœ€è¦è¿ç§»: {status.get('needs_migration', False)}")

        if status.get("needs_migration", False):
            logger.info("æ‰§è¡Œæ•°æ®è¿ç§»...")
            stats = await migration_service.migrate_all_data(
                force_rebuild=False, batch_size=50, auto_embed=True
            )

            logger.info("è¿ç§»ç»“æœ:")
            logger.info(
                f"  å®ä½“: {stats.migrated_entities}/{stats.total_entities} "
                f"({stats.entity_success_rate:.1f}%)"
            )
            logger.info(
                f"  å…³ç³»: {stats.migrated_relationships}/{stats.total_relationships} "
                f"({stats.relationship_success_rate:.1f}%)"
            )
            logger.info(f"  å‘é‡: {stats.migrated_embeddings}")
            logger.info(f"  è€—æ—¶: {stats.duration_seconds:.2f} ç§’")

    except Exception as e:
        logger.error(f"âŒ æ•°æ®è¿ç§»æ¼”ç¤ºå¤±è´¥: {e}")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    logger.info("ğŸ¬ Linch Mind ä¸‰å±‚å­˜å‚¨æ¶æ„æ¼”ç¤ºå¼€å§‹")
    logger.info("æ¶æ„: SQLite (å…³ç³»å‹) + NetworkX (å›¾æ•°æ®åº“) + FAISS (å‘é‡æ•°æ®åº“)")

    try:
        # åŸºç¡€æ“ä½œæ¼”ç¤º
        await demo_basic_operations()

        # æ•°æ®è¿ç§»æ¼”ç¤º
        await demo_data_migration()

        logger.info("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼ä¸‰å±‚å­˜å‚¨æ¶æ„å·¥ä½œæ­£å¸¸")

    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
